From: svc_sdk <svc_sdk@arm.com>
Date: Tue Jul 29 00:00:00 2025 
Subject: llvm-changes-for-model-converter-29-07-2025

diff --git a/llvm/tools/spirv-tools/CMakeLists.txt b/llvm/tools/spirv-tools/CMakeLists.txt
index c2c0f3e3c2e4..5db7aec99759 100644
--- a/llvm/tools/spirv-tools/CMakeLists.txt
+++ b/llvm/tools/spirv-tools/CMakeLists.txt
@@ -5,10 +5,6 @@ if (NOT LLVM_INCLUDE_SPIRV_TOOLS_TESTS)
   return()
 endif ()
 
-if (NOT "SPIRV" IN_LIST LLVM_TARGETS_TO_BUILD)
-  message(FATAL_ERROR "Building SPIRV-Tools tests is unsupported without the SPIR-V target")
-endif ()
-
 # SPIRV_DIS, SPIRV_VAL, SPIRV_AS and SPIRV_LINK variables can be used to provide paths to existing
 # spirv-dis, spirv-val, spirv-as, and spirv-link binaries, respectively. Otherwise, build them from
 # SPIRV-Tools source.
diff --git a/mlir/include/mlir/Conversion/Passes.h b/mlir/include/mlir/Conversion/Passes.h
index d93fbefab74a..f7bb3cbe2e6d 100644
--- a/mlir/include/mlir/Conversion/Passes.h
+++ b/mlir/include/mlir/Conversion/Passes.h
@@ -72,6 +72,8 @@
 #include "mlir/Conversion/TosaToLinalg/TosaToLinalg.h"
 #include "mlir/Conversion/TosaToMLProgram/TosaToMLProgram.h"
 #include "mlir/Conversion/TosaToSCF/TosaToSCF.h"
+#include "mlir/Conversion/TosaToSPIRV/TosaToSPIRV.h"
+#include "mlir/Conversion/TosaToSPIRV/ConvertTosaConstants.h"
 #include "mlir/Conversion/TosaToTensor/TosaToTensor.h"
 #include "mlir/Conversion/UBToLLVM/UBToLLVM.h"
 #include "mlir/Conversion/UBToSPIRV/UBToSPIRV.h"
diff --git a/mlir/include/mlir/Conversion/Passes.td b/mlir/include/mlir/Conversion/Passes.td
index 76e751243a12..2239d065fcd6 100644
--- a/mlir/include/mlir/Conversion/Passes.td
+++ b/mlir/include/mlir/Conversion/Passes.td
@@ -1262,6 +1262,38 @@ def TosaToSCFPass : Pass<"tosa-to-scf"> {
   }];
 }
 
+//===----------------------------------------------------------------------===//
+// TosaToSPIRV
+//===----------------------------------------------------------------------===//
+
+def TosaToSPIRV : Pass<"tosa-to-spirv"> {
+  let summary = "Lower TOSA to the SPIR-V dialect";
+  let dependentDialects = [
+    "spirv::SPIRVDialect",
+  ];
+  let description = [{
+    Pass that converts TOSA operations to the equivalent operations using the
+    operations in the SPIR-V dialect.
+  }];
+
+  let constructor = "tosa::createTosaToSPIRV()";
+}
+
+//===----------------------------------------------------------------------===//
+// ConvertTosaConstantsPass
+//===----------------------------------------------------------------------===//
+
+def ConvertTosaConstantsPass : Pass<"convert-tosa-constants-pass", "mlir::func::FuncOp"> {
+  let summary = "Convert TOSA constants to SPIR-V graph constants";
+
+  let description = [{
+    Pass that converts TOSA constants to SPIR-V graph constants in the
+    SPIR-V dialect.
+  }];
+
+  let constructor = "tosa::createConvertTosaConstantsPass()";
+}
+
 //===----------------------------------------------------------------------===//
 // TosaToTensor
 //===----------------------------------------------------------------------===//
diff --git a/mlir/include/mlir/Conversion/TosaToSPIRV/ConvertTosaConstants.h b/mlir/include/mlir/Conversion/TosaToSPIRV/ConvertTosaConstants.h
new file mode 100644
index 000000000000..c3dd512a90eb
--- /dev/null
+++ b/mlir/include/mlir/Conversion/TosaToSPIRV/ConvertTosaConstants.h
@@ -0,0 +1,34 @@
+//===-- TosaToSPIRV.h - TOSA to SPIR-V patterns -----------------*- C++ -*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// Pass to Convert Tosa Constants to SPIR-V graph constants.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef MLIR_CONVERSION_TOSATOSPIRV_CONVERTTOSACONSTANTS_H
+#define MLIR_CONVERSION_TOSATOSPIRV_CONVERTTOSACONSTANTS_H
+
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Pass/Pass.h"
+
+namespace mlir {
+#define GEN_PASS_DEF_CONVERTTOSACONSTANTSPASS
+#include "mlir/Conversion/Passes.h.inc"
+
+namespace tosa {
+
+std::optional<uint32_t> getGraphId(Operation *op, const std::string &attrName);
+
+std::optional<uint32_t> getGraphIdForConst(Operation *op);
+
+std::unique_ptr<Pass> createConvertTosaConstantsPass();
+
+} // namespace tosa
+} // namespace mlir
+
+#endif // MLIR_CONVERSION_TOSATOSPIRV_CONVERTTOSACONSTANTS_H
diff --git a/mlir/include/mlir/Conversion/TosaToSPIRV/TosaToSPIRV.h b/mlir/include/mlir/Conversion/TosaToSPIRV/TosaToSPIRV.h
new file mode 100644
index 000000000000..20fb9ef11a42
--- /dev/null
+++ b/mlir/include/mlir/Conversion/TosaToSPIRV/TosaToSPIRV.h
@@ -0,0 +1,36 @@
+//===-- TosaToSPIRV.h - TOSA to SPIR-V patterns -----------------*- C++ -*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// Provides patterns to convert Tosa dialect to SPIR-V dialect.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef MLIR_CONVERSION_TOSATOSPIRV_TOSATOSPIRV_H
+#define MLIR_CONVERSION_TOSATOSPIRV_TOSATOSPIRV_H
+
+#include "mlir/Dialect/SPIRV/Transforms/SPIRVConversion.h"
+#include "mlir/Pass/Pass.h"
+
+namespace mlir {
+
+#define GEN_PASS_DECL_TOSATOSPIRV
+#include "mlir/Conversion/Passes.h.inc"
+
+namespace tosa {
+
+std::unique_ptr<Pass> createTosaToSPIRV(bool analysis = false);
+
+void populateTosaToSPIRVConversionPatterns(SPIRVTypeConverter &typeConverter,
+                                           RewritePatternSet &patterns);
+void populateTosaToSPIRVOpsConversionPatterns(SPIRVTypeConverter &typeConverter,
+                                              RewritePatternSet &patterns);
+
+} // namespace tosa
+} // namespace mlir
+
+#endif // MLIR_CONVERSION_TOSATOSPIRV_TOSATOSPIRV_H
diff --git a/mlir/include/mlir/Dialect/SPIRV/IR/SPIRVBase.td b/mlir/include/mlir/Dialect/SPIRV/IR/SPIRVBase.td
index 90383265002a..40a90eee55d7 100644
--- a/mlir/include/mlir/Dialect/SPIRV/IR/SPIRVBase.td
+++ b/mlir/include/mlir/Dialect/SPIRV/IR/SPIRVBase.td
@@ -424,6 +424,7 @@ def SPV_NV_ray_tracing_motion_blur       : I32EnumAttrCase<"SPV_NV_ray_tracing_m
 def SPV_NVX_multiview_per_view_attributes : I32EnumAttrCase<"SPV_NVX_multiview_per_view_attributes", 5015>;
 
 def SPV_ARM_tensors                      : I32EnumAttrCase<"SPV_ARM_tensors", 6000>;
+def SPV_ARM_graph                        : I32EnumAttrCase<"SPV_ARM_graph", 6001>;
 
 def SPIRV_ExtensionAttr :
     SPIRV_I32EnumAttr<"Extension", "supported SPIR-V extensions", "ext", [
@@ -448,7 +449,7 @@ def SPIRV_ExtensionAttr :
       SPV_EXT_shader_atomic_float_add, SPV_EXT_shader_atomic_float_min_max,
       SPV_EXT_shader_image_int64, SPV_EXT_shader_atomic_float16_add,
       SPV_EXT_mesh_shader, SPV_EXT_replicated_composites,
-      SPV_ARM_tensors,
+      SPV_ARM_tensors, SPV_ARM_graph,
       SPV_AMD_gpu_shader_half_float_fetch, SPV_AMD_shader_ballot,
       SPV_AMD_shader_explicit_vertex_parameter, SPV_AMD_shader_fragment_mask,
       SPV_AMD_shader_image_load_store_lod, SPV_AMD_texture_gather_bias_lod,
@@ -1339,6 +1340,12 @@ def SPIRV_C_StorageTensorArrayNonUniformIndexingEXT     : I32EnumAttrCase<"Stora
     Extension<[SPV_ARM_tensors]>
   ];
 }
+def SPIRV_C_GraphARM                                    : I32EnumAttrCase<"GraphARM", 4191> {
+  list<I32EnumAttrCase> implies = [SPIRV_C_TensorsARM, SPIRV_C_Shader, SPIRV_C_VulkanMemoryModel];
+  list<Availability> availability = [
+    Extension<[SPV_ARM_graph]>
+  ];
+}
 def SPIRV_C_WorkgroupMemoryExplicitLayout8BitAccessKHR  : I32EnumAttrCase<"WorkgroupMemoryExplicitLayout8BitAccessKHR", 4429> {
   list<I32EnumAttrCase> implies = [SPIRV_C_WorkgroupMemoryExplicitLayoutKHR];
   list<Availability> availability = [
@@ -1552,7 +1559,7 @@ def SPIRV_CapabilityAttr :
       SPIRV_C_GeometryPointSize, SPIRV_C_ImageCubeArray, SPIRV_C_ImageRect,
       SPIRV_C_GeometryStreams, SPIRV_C_MultiViewport,
       SPIRV_C_TensorsARM, SPIRV_C_StorageTensorArrayDynamicIndexingEXT,
-      SPIRV_C_StorageTensorArrayNonUniformIndexingEXT,
+      SPIRV_C_StorageTensorArrayNonUniformIndexingEXT, SPIRV_C_GraphARM,
       SPIRV_C_WorkgroupMemoryExplicitLayout8BitAccessKHR, SPIRV_C_VariablePointers,
       SPIRV_C_RayTraversalPrimitiveCullingKHR, SPIRV_C_SampleMaskOverrideCoverageNV,
       SPIRV_C_GeometryShaderPassthroughNV, SPIRV_C_PerViewAttributesNV,
@@ -4217,6 +4224,7 @@ def SPIRV_IsTensorArmType : CPred<"::llvm::isa<::mlir::spirv::TensorArmType>($_s
 def SPIRV_Void : TypeAlias<NoneType, "void">;
 def SPIRV_Bool : TypeAlias<I1, "bool">;
 def SPIRV_Integer : AnyIntOfWidths<[8, 16, 32, 64]>;
+def SPIRV_Int8 : TypeAlias<I8, "Int8">;
 def SPIRV_Int16 : TypeAlias<I16, "Int16">;
 def SPIRV_Int32 : TypeAlias<I32, "Int32">;
 def SPIRV_Float32 : TypeAlias<F32, "Float32">;
@@ -4252,6 +4260,7 @@ def SPIRV_AnyTensorArm : DialectType<SPIRV_Dialect, SPIRV_IsTensorArmType,
 
 def SPIRV_Numerical : AnyTypeOf<[SPIRV_Integer, SPIRV_AnyFloat]>;
 def SPIRV_Scalar : AnyTypeOf<[SPIRV_Numerical, SPIRV_Bool]>;
+
 def SPIRV_Aggregate : AnyTypeOf<[SPIRV_AnyArray, SPIRV_AnyRTArray, SPIRV_AnyStruct]>;
 def SPIRV_Composite :
     AnyTypeOf<[SPIRV_Vector, SPIRV_AnyArray, SPIRV_AnyRTArray, SPIRV_AnyStruct,
@@ -4559,6 +4568,13 @@ def SPIRV_OC_OpGroupNonUniformLogicalAnd      : I32EnumAttrCase<"OpGroupNonUnifo
 def SPIRV_OC_OpGroupNonUniformLogicalOr       : I32EnumAttrCase<"OpGroupNonUniformLogicalOr", 363>;
 def SPIRV_OC_OpGroupNonUniformLogicalXor      : I32EnumAttrCase<"OpGroupNonUniformLogicalXor", 364>;
 def SPIRV_OC_OpTypeTensorARM                  : I32EnumAttrCase<"OpTypeTensorARM", 4163>;
+def SPIRV_OC_OpGraphConstantARM               : I32EnumAttrCase<"OpGraphConstantARM", 4181>;
+def SPIRV_OC_OpGraphEntryPointARM             : I32EnumAttrCase<"OpGraphEntryPointARM", 4182>;
+def SPIRV_OC_OpGraphARM                       : I32EnumAttrCase<"OpGraphARM", 4183>;
+def SPIRV_OC_OpGraphInputARM                  : I32EnumAttrCase<"OpGraphInputARM", 4184>;
+def SPIRV_OC_OpGraphSetOutputARM              : I32EnumAttrCase<"OpGraphSetOutputARM", 4185>;
+def SPIRV_OC_OpGraphEndARM                    : I32EnumAttrCase<"OpGraphEndARM", 4186>;
+def SPIRV_OC_OpTypeGraphARM                   : I32EnumAttrCase<"OpTypeGraphARM", 4190>;
 def SPIRV_OC_OpSubgroupBallotKHR              : I32EnumAttrCase<"OpSubgroupBallotKHR", 4421>;
 def SPIRV_OC_OpGroupNonUniformRotateKHR       : I32EnumAttrCase<"OpGroupNonUniformRotateKHR", 4431>;
 def SPIRV_OC_OpSDot                           : I32EnumAttrCase<"OpSDot", 4450>;
@@ -4677,6 +4693,9 @@ def SPIRV_OpcodeAttr :
       SPIRV_OC_OpGroupNonUniformLogicalAnd, SPIRV_OC_OpGroupNonUniformLogicalOr,
       SPIRV_OC_OpGroupNonUniformLogicalXor,
       SPIRV_OC_OpTypeTensorARM,
+      SPIRV_OC_OpGraphEntryPointARM, SPIRV_OC_OpGraphARM,
+      SPIRV_OC_OpGraphInputARM, SPIRV_OC_OpGraphSetOutputARM, SPIRV_OC_OpGraphEndARM,
+      SPIRV_OC_OpTypeGraphARM, SPIRV_OC_OpGraphConstantARM,
       SPIRV_OC_OpSubgroupBallotKHR,
       SPIRV_OC_OpGroupNonUniformRotateKHR, SPIRV_OC_OpSDot, SPIRV_OC_OpUDot,
       SPIRV_OC_OpSUDot, SPIRV_OC_OpSDotAccSat, SPIRV_OC_OpUDotAccSat,
@@ -4849,6 +4868,11 @@ class SPIRV_NvVendorOp<string mnemonic, list<Trait> traits = []> :
   SPIRV_VendorOp<mnemonic, "NV", traits> {
 }
 
+class SPIRV_ArmVendorOp<string mnemonic, list<Trait> traits = []> :
+  SPIRV_VendorOp<mnemonic, "ARM", traits> {
+}
+
+
 def SPIRV_FPFMM_None         : I32BitEnumAttrCaseNone<"None">;
 def SPIRV_FPFMM_NotNaN       : I32BitEnumAttrCaseBit<"NotNaN", 0>;
 def SPIRV_FPFMM_NotInf       : I32BitEnumAttrCaseBit<"NotInf", 1>;
diff --git a/mlir/include/mlir/Dialect/SPIRV/IR/SPIRVGraphOps.td b/mlir/include/mlir/Dialect/SPIRV/IR/SPIRVGraphOps.td
new file mode 100644
index 000000000000..b23728a6eb6f
--- /dev/null
+++ b/mlir/include/mlir/Dialect/SPIRV/IR/SPIRVGraphOps.td
@@ -0,0 +1,202 @@
+//===- SPIRVGraphOps.td - Graph extended insts spec file -----*- tablegen -*-=//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This is the op definition spec of Graph extension ops.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef MLIR_DIALECT_SPIRV_IR_GRAPH_OPS
+#define MLIR_DIALECT_SPIRV_IR_GRAPH_OPS
+
+include "mlir/Dialect/SPIRV/IR/SPIRVBase.td"
+include "mlir/Dialect/SPIRV/IR/SPIRVTosaTypes.td"
+include "mlir/Interfaces/CallInterfaces.td"
+include "mlir/Interfaces/SideEffectInterfaces.td"
+include "mlir/Interfaces/FunctionInterfaces.td"
+
+//===----------------------------------------------------------------------===//
+// SPIR-V Graph opcode specification.
+//===----------------------------------------------------------------------===//
+
+// Base class for all Graph ops.
+class SPIRV_GraphARMOp<string mnemonic, list<Trait> traits = []> :
+  SPIRV_ArmVendorOp<mnemonic, traits> {
+
+  let availability = [
+    MinVersion<SPIRV_V_1_0>,
+    MaxVersion<SPIRV_V_1_6>,
+    Extension<[SPV_ARM_graph, SPV_ARM_tensors, SPV_KHR_vulkan_memory_model]>,
+    Capability<[SPIRV_C_GraphARM]>
+  ];
+}
+
+def SPIRV_GraphConstantARMOp : SPIRV_GraphARMOp<"GraphConstant", [Pure]> {
+  let summary = "Declare a graph constant.";
+
+  let description = [{
+    Declare a graph constant.
+    Result Type must be an OpTypeTensorARM.
+    GraphConstantID must be a 32-bit integer literal.
+  }];
+
+  let arguments = (ins
+    I32Attr: $graph_constant_id
+  );
+
+  let results = (outs
+    SPIRV_TosaAny_TensorArmUpTo6D:$output
+  );
+
+  let hasVerifier = 0;
+
+  let autogenSerialization = 0;
+
+  let assemblyFormat = [{
+    attr-dict `:` type($output)
+  }];
+}
+
+// -----
+
+def SPIRV_GraphARMOp : SPIRV_GraphARMOp<"Graph", [
+    AutomaticAllocationScope, DeclareOpInterfaceMethods<CallableOpInterface>,
+    FunctionOpInterface, InModuleScope, IsolatedFromAbove
+  ]> {
+
+  let summary = "Declare or define a SPIR-V graph";
+
+  let description = [{
+    This op declares or defines a SPIR-V graph using one region, which
+    contains one or more blocks.
+
+    Different from the SPIR-V binary format, this op is not allowed to
+    implicitly capture global values, and all external references must use
+    function arguments or symbol references. This op itself defines a symbol
+    that is unique in the enclosing module op.
+
+    This op itself takes no operands and generates no results. Its region
+    can take zero or more arguments and return zero or more values.
+
+    ```
+    spv-graph-arm-op ::= `spirv.ARM.Graph` function-signature
+                        region
+    ```
+  }];
+
+  let arguments = (ins
+    TypeAttrOf<GraphType>:$function_type,
+    OptionalAttr<DictArrayAttr>:$arg_attrs,
+    OptionalAttr<DictArrayAttr>:$res_attrs,
+    OptionalAttr<BoolAttr>:$entry_point,
+    StrAttr:$sym_name
+  );
+
+  let results = (outs);
+
+  let regions = (region AnyRegion:$body);
+
+  let hasVerifier = 0;
+
+  let builders = [
+    OpBuilder<(ins "StringRef":$name, "GraphType":$type,
+      CArg<"ArrayRef<NamedAttribute>", "{}">:$attrs,  CArg<"bool", "false">:$entry_point)>];
+
+  let hasOpcode = 0;
+
+  let autogenSerialization = 0;
+
+  let extraClassDeclaration = [{
+    /// Hook for FunctionOpInterface, called after verifying that the 'type'
+    /// attribute is present and checks if it holds a function type. Ensures
+    /// getType, getNumArguments, and getNumResults can be called safely
+    LogicalResult verifyType();
+
+    /// Hook for FunctionOpInterface, called after verifying the function
+    /// type and the presence of the (potentially empty) function body.
+    /// Ensures SPIR-V specific semantics.
+    LogicalResult verifyBody();
+  }];
+}
+
+// Check that an op can only be used within the scope of a spirv.ARM.Graph op.
+def InGraphScope : PredOpTrait<
+  "op must appear in a spirv.ARM.Graph op's block",
+  CPred<"isNestedInGraphARMOpInterface($_op.getParentOp())">>;
+
+// -----
+
+def SPIRV_GraphEntryPointARMOp : SPIRV_GraphARMOp<"GraphEntryPoint", [InModuleScope]> {
+  let summary = [{
+    Declare a graph entry point and its interface.
+  }];
+
+  let description = [{
+    Graph Entry Point must be the Result <id> of an OpGraphARM instruction.
+
+    Name is a name string for the graphentry point. A module cannot have two
+    OpGraphEntryPointARM instructions with the same Name string.
+
+    Interface is a list of symbol references to `spirv.GlobalVariable`
+    operations. These declare the set of global variables from a
+    module that form the interface of this entry point. The set of
+    Interface symbols must be equal to or a superset of the
+    `spirv.GlobalVariable`s referenced by the entry point’s static call
+    tree, within the interface’s storage classes.
+
+    ```
+    entry-point-op ::= ssa-id `=` `spirv.ARM.GraphEntryPoint`
+                       symbol-reference (`, ` symbol-reference)*
+    ```
+  }];
+
+  let arguments = (ins
+    FlatSymbolRefAttr:$fn,
+    SymbolRefArrayAttr:$interface
+  );
+
+  let results = (outs);
+
+  let autogenSerialization = 0;
+
+  let builders = [
+    OpBuilder<(ins "spirv::GraphARMOp":$graph, "ArrayRef<Attribute>":$interfaceVars)>];
+}
+
+// -----
+
+def SPIRV_GraphOutputsARMOp : SPIRV_GraphARMOp<"GraphOutputs", [InGraphScope, Pure,
+                                               Terminator]> {
+
+  let summary = "Define graph outputs.";
+
+  let description = [{
+    Values are the graph outputs values and must match the GraphOutputs Type
+    operand of the OpTypeGraphARM type of the OpGraphARM body this
+    instruction is in.
+
+    This instruction must be the last instruction in a block.
+
+    ```
+    graph-output-op ::= `spirv.ARM.GraphOutputs` ssa-use `:` type-list-no-parens
+    ```
+  }];
+
+  let arguments = (ins
+    Variadic<SPIRV_AnyTensorArm>:$value
+  );
+
+  let results = (outs);
+
+  let autogenSerialization = 0;
+
+  let hasOpcode = 0;
+
+  let assemblyFormat = "$value attr-dict `:` type($value)";
+}
+
+#endif // MLIR_DIALECT_SPIRV_IR_GRAPH_OPS
diff --git a/mlir/include/mlir/Dialect/SPIRV/IR/SPIRVOps.td b/mlir/include/mlir/Dialect/SPIRV/IR/SPIRVOps.td
index 0fa1bb9d5bd0..3ef9699154cd 100644
--- a/mlir/include/mlir/Dialect/SPIRV/IR/SPIRVOps.td
+++ b/mlir/include/mlir/Dialect/SPIRV/IR/SPIRVOps.td
@@ -32,6 +32,7 @@ include "mlir/Dialect/SPIRV/IR/SPIRVControlFlowOps.td"
 include "mlir/Dialect/SPIRV/IR/SPIRVCooperativeMatrixOps.td"
 include "mlir/Dialect/SPIRV/IR/SPIRVIntelExtOps.td"
 include "mlir/Dialect/SPIRV/IR/SPIRVGLOps.td"
+include "mlir/Dialect/SPIRV/IR/SPIRVGraphOps.td"
 include "mlir/Dialect/SPIRV/IR/SPIRVGroupOps.td"
 include "mlir/Dialect/SPIRV/IR/SPIRVImageOps.td"
 include "mlir/Dialect/SPIRV/IR/SPIRVIntegerDotProductOps.td"
@@ -44,6 +45,7 @@ include "mlir/Dialect/SPIRV/IR/SPIRVNonUniformOps.td"
 include "mlir/Dialect/SPIRV/IR/SPIRVPrimitiveOps.td"
 include "mlir/Dialect/SPIRV/IR/SPIRVCLOps.td"
 include "mlir/Dialect/SPIRV/IR/SPIRVStructureOps.td"
+include "mlir/Dialect/SPIRV/IR/SPIRVTosaOps.td"
 include "mlir/Interfaces/SideEffectInterfaces.td"
 
 #endif // MLIR_DIALECT_SPIRV_IR_OPS
diff --git a/mlir/include/mlir/Dialect/SPIRV/IR/SPIRVTosaOps.td b/mlir/include/mlir/Dialect/SPIRV/IR/SPIRVTosaOps.td
new file mode 100644
index 000000000000..5bab8aa6e41a
--- /dev/null
+++ b/mlir/include/mlir/Dialect/SPIRV/IR/SPIRVTosaOps.td
@@ -0,0 +1,1806 @@
+//===- SPIRVTosaOps.td - TOSA extended insts spec file -----*- tablegen -*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This is the op definition spec of TOSA extension ops.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef MLIR_DIALECT_SPIRV_IR_TOSA_OPS
+#define MLIR_DIALECT_SPIRV_IR_TOSA_OPS
+
+include "mlir/Dialect/SPIRV/IR/SPIRVBase.td"
+include "mlir/Dialect/SPIRV/IR/SPIRVGraphOps.td"
+include "mlir/Dialect/SPIRV/IR/SPIRVTosaTypes.td"
+include "mlir/Interfaces/SideEffectInterfaces.td"
+
+//===----------------------------------------------------------------------===//
+// SPIR-V TOSA opcode specification.
+//===----------------------------------------------------------------------===//
+
+// Base class for all TOSA ops.
+class SPIRV_TosaOp<string mnemonic, int opcode, list<Trait> traits = []> :
+  SPIRV_ExtInstOp<mnemonic, "Tosa", "TOSA.001000.1", opcode, !listconcat(traits, [InGraphScope])> {
+
+  let availability = [
+    MinVersion<SPIRV_V_1_5>,
+    MaxVersion<SPIRV_V_1_6>,
+    Extension<[SPV_ARM_graph]>,
+    Capability<[SPIRV_C_GraphARM]>
+  ];
+}
+
+
+def SPIRV_TosaArgMaxOp : SPIRV_TosaOp<"ArgMax", 0, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_Int32: $axis,
+    SPIRV_Int32: $nan_mode,
+    SPIRV_TosaNumerical_TensorArm1DTo6D: $input
+  );
+
+
+  let results = (outs
+    SPIRV_TosaInteger_TensorArmUpTo5D: $output
+  );
+
+  let hasVerifier = 1;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaAvgPool2DOp : SPIRV_TosaOp<"AvgPool2D", 1, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_Int32_1DTensorArmOfLength2: $kernel,
+    SPIRV_Int32_1DTensorArmOfLength2: $stride,
+    SPIRV_Int32_1DTensorArmOfLength4: $pad,
+    SPIRV_Int32: $acc_type,
+    SPIRV_TosaNumerical_TensorArm4D: $input,
+    SPIRV_TosaNumerical_1DTensorArmOfLength1: $input_zp,
+    SPIRV_TosaNumerical_1DTensorArmOfLength1: $output_zp
+  );
+
+
+  let results = (outs
+    SPIRV_TosaNumerical_TensorArm4D: $output
+  );
+
+  let hasVerifier = 1;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaConv2DOp : SPIRV_TosaOp<"Conv2D", 2, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_Int32_1DTensorArmOfLength4: $pad,
+    SPIRV_Int32_1DTensorArmOfLength2: $stride,
+    SPIRV_Int32_1DTensorArmOfLength2: $dilation,
+    SPIRV_Int32: $acc_type,
+    SPIRV_Bool: $local_bound,
+    SPIRV_TosaNumerical_TensorArm4D: $input,
+    SPIRV_TosaNumerical_TensorArm4D: $weight,
+    SPIRV_TosaNumerical_TensorArm1D: $bias,
+    SPIRV_TosaNumerical_1DTensorArmOfLength1: $input_zp,
+    SPIRV_TosaNumerical_1DTensorArmOfLength1: $weight_zp
+  );
+
+
+  let results = (outs
+    SPIRV_TosaNumerical_TensorArm4D: $output
+  );
+
+  let hasVerifier = 1;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaConv3DOp : SPIRV_TosaOp<"Conv3D", 3, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_Int32_1DTensorArmOfLength6: $pad,
+    SPIRV_Int32_1DTensorArmOfLength3: $stride,
+    SPIRV_Int32_1DTensorArmOfLength3: $dilation,
+    SPIRV_Int32: $acc_type,
+    SPIRV_Bool: $local_bound,
+    SPIRV_TosaNumerical_TensorArm5D: $input,
+    SPIRV_TosaNumerical_TensorArm5D: $weight,
+    SPIRV_TosaNumerical_TensorArm1D: $bias,
+    SPIRV_TosaNumerical_1DTensorArmOfLength1: $input_zp,
+    SPIRV_TosaNumerical_1DTensorArmOfLength1: $weight_zp
+  );
+
+
+  let results = (outs
+    SPIRV_TosaNumerical_TensorArm5D: $output
+  );
+
+  let hasVerifier = 1;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaDepthwiseConv2DOp : SPIRV_TosaOp<"DepthwiseConv2D", 4, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_Int32_1DTensorArmOfLength4: $pad,
+    SPIRV_Int32_1DTensorArmOfLength2: $stride,
+    SPIRV_Int32_1DTensorArmOfLength2: $dilation,
+    SPIRV_Int32: $acc_type,
+    SPIRV_Bool: $local_bound,
+    SPIRV_TosaNumerical_TensorArm4D: $input,
+    SPIRV_TosaNumerical_TensorArm4D: $weight,
+    SPIRV_TosaNumerical_TensorArm1D: $bias,
+    SPIRV_TosaNumerical_1DTensorArmOfLength1: $input_zp,
+    SPIRV_TosaNumerical_1DTensorArmOfLength1: $weight_zp
+  );
+
+
+  let results = (outs
+    SPIRV_TosaNumerical_TensorArm4D: $output
+  );
+
+  let hasVerifier = 1;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaFFT2DOp : SPIRV_TosaOp<"FFT2D", 5, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_Bool: $inverse,
+    SPIRV_Bool: $local_bound,
+    SPIRV_TosaFloat_TensorArm3D: $input_real,
+    SPIRV_TosaFloat_TensorArm3D: $input_imag
+  );
+
+
+  let results = (outs
+    SPIRV_Struct_2_TosaFloat_TensorArm3D: $output
+  );
+
+  let hasVerifier = 1;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaMatMulOp : SPIRV_TosaOp<"MatMul", 6, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaNumerical_TensorArm3D: $A,
+    SPIRV_TosaNumerical_TensorArm3D: $B,
+    SPIRV_TosaNumerical_1DTensorArmOfLength1: $A_zp,
+    SPIRV_TosaNumerical_1DTensorArmOfLength1: $B_zp
+  );
+
+
+  let results = (outs
+    SPIRV_TosaNumerical_TensorArm3D: $output
+  );
+
+  let hasVerifier = 1;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaMaxPool2DOp : SPIRV_TosaOp<"MaxPool2D", 7, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_Int32_1DTensorArmOfLength2: $kernel,
+    SPIRV_Int32_1DTensorArmOfLength2: $stride,
+    SPIRV_Int32_1DTensorArmOfLength4: $pad,
+    SPIRV_Int32: $nan_mode,
+    SPIRV_TosaNumerical_TensorArm4D: $input
+  );
+
+
+  let results = (outs
+    SPIRV_TosaNumerical_TensorArm4D: $output
+  );
+
+  let hasVerifier = 1;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaRFFT2DOp : SPIRV_TosaOp<"RFFT2D", 8, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_Bool: $local_bound,
+    SPIRV_TosaFloat_TensorArm3D: $input_real
+  );
+
+
+  let results = (outs
+    SPIRV_Struct_2_TosaFloat_TensorArm3D: $output
+  );
+
+  let hasVerifier = 1;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaTransposeConv2DOp : SPIRV_TosaOp<"TransposeConv2D", 9, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_Int32_1DTensorArmOfLength4: $out_pad,
+    SPIRV_Int32_1DTensorArmOfLength2: $stride,
+    SPIRV_Int32: $acc_type,
+    SPIRV_Bool: $local_bound,
+    SPIRV_TosaNumerical_TensorArm4D: $input,
+    SPIRV_TosaNumerical_TensorArm4D: $weight,
+    SPIRV_TosaNumerical_TensorArm1D: $bias,
+    SPIRV_TosaNumerical_1DTensorArmOfLength1: $input_zp,
+    SPIRV_TosaNumerical_1DTensorArmOfLength1: $weight_zp
+  );
+
+
+  let results = (outs
+    SPIRV_TosaNumerical_TensorArm4D: $output
+  );
+
+  let hasVerifier = 1;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaClampOp : SPIRV_TosaOp<"Clamp", 10, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaNumerical: $min_val,
+    SPIRV_TosaNumerical: $max_val,
+    SPIRV_Int32: $nan_mode,
+    SPIRV_TosaNumerical_TensorArmUpTo6D: $input
+  );
+
+
+  let results = (outs
+    SPIRV_TosaNumerical_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 1;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaErfOp : SPIRV_TosaOp<"Erf", 11, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaFloat_TensorArmUpTo6D: $input
+  );
+
+
+  let results = (outs
+    SPIRV_TosaFloat_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaSigmoidOp : SPIRV_TosaOp<"Sigmoid", 12, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaFloat_TensorArmUpTo6D: $input
+  );
+
+
+  let results = (outs
+    SPIRV_TosaFloat_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaTanhOp : SPIRV_TosaOp<"Tanh", 13, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaFloat_TensorArmUpTo6D: $input
+  );
+
+
+  let results = (outs
+    SPIRV_TosaFloat_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaAddOp : SPIRV_TosaOp<"Add", 14, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaNumerical_TensorArmUpTo6D: $input1,
+    SPIRV_TosaNumerical_TensorArmUpTo6D: $input2
+  );
+
+
+  let results = (outs
+    SPIRV_TosaNumerical_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaArithmeticRightShiftOp : SPIRV_TosaOp<"ArithmeticRightShift", 15, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_Bool: $round,
+    SPIRV_TosaInteger_TensorArmUpTo6D: $input1,
+    SPIRV_TosaInteger_TensorArmUpTo6D: $input2
+  );
+
+
+  let results = (outs
+    SPIRV_TosaInteger_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaBitwiseAndOp : SPIRV_TosaOp<"BitwiseAnd", 16, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaInteger_TensorArmUpTo6D: $input1,
+    SPIRV_TosaInteger_TensorArmUpTo6D: $input2
+  );
+
+
+  let results = (outs
+    SPIRV_TosaInteger_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaBitwiseOrOp : SPIRV_TosaOp<"BitwiseOr", 17, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaInteger_TensorArmUpTo6D: $input1,
+    SPIRV_TosaInteger_TensorArmUpTo6D: $input2
+  );
+
+
+  let results = (outs
+    SPIRV_TosaInteger_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaBitwiseXorOp : SPIRV_TosaOp<"BitwiseXor", 18, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaInteger_TensorArmUpTo6D: $input1,
+    SPIRV_TosaInteger_TensorArmUpTo6D: $input2
+  );
+
+
+  let results = (outs
+    SPIRV_TosaInteger_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaIntDivOp : SPIRV_TosaOp<"IntDiv", 19, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaInteger_TensorArmUpTo6D: $input1,
+    SPIRV_TosaInteger_TensorArmUpTo6D: $input2
+  );
+
+
+  let results = (outs
+    SPIRV_TosaInteger_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaLogicalAndOp : SPIRV_TosaOp<"LogicalAnd", 20, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_Bool_TensorArmUpTo6D: $input1,
+    SPIRV_Bool_TensorArmUpTo6D: $input2
+  );
+
+
+  let results = (outs
+    SPIRV_Bool_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaLogicalLeftShiftOp : SPIRV_TosaOp<"LogicalLeftShift", 21, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaInteger_TensorArmUpTo6D: $input1,
+    SPIRV_TosaInteger_TensorArmUpTo6D: $input2
+  );
+
+
+  let results = (outs
+    SPIRV_TosaInteger_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaLogicalRightShiftOp : SPIRV_TosaOp<"LogicalRightShift", 22, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaInteger_TensorArmUpTo6D: $input1,
+    SPIRV_TosaInteger_TensorArmUpTo6D: $input2
+  );
+
+
+  let results = (outs
+    SPIRV_TosaInteger_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaLogicalOrOp : SPIRV_TosaOp<"LogicalOr", 23, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_Bool_TensorArmUpTo6D: $input1,
+    SPIRV_Bool_TensorArmUpTo6D: $input2
+  );
+
+
+  let results = (outs
+    SPIRV_Bool_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaLogicalXorOp : SPIRV_TosaOp<"LogicalXor", 24, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_Bool_TensorArmUpTo6D: $input1,
+    SPIRV_Bool_TensorArmUpTo6D: $input2
+  );
+
+
+  let results = (outs
+    SPIRV_Bool_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaMaximumOp : SPIRV_TosaOp<"Maximum", 25, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_Int32: $nan_mode,
+    SPIRV_TosaNumerical_TensorArmUpTo6D: $input1,
+    SPIRV_TosaNumerical_TensorArmUpTo6D: $input2
+  );
+
+
+  let results = (outs
+    SPIRV_TosaNumerical_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaMinimumOp : SPIRV_TosaOp<"Minimum", 26, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_Int32: $nan_mode,
+    SPIRV_TosaNumerical_TensorArmUpTo6D: $input1,
+    SPIRV_TosaNumerical_TensorArmUpTo6D: $input2
+  );
+
+
+  let results = (outs
+    SPIRV_TosaNumerical_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaMulOp : SPIRV_TosaOp<"Mul", 27, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaNumerical_TensorArmUpTo6D: $input1,
+    SPIRV_TosaNumerical_TensorArmUpTo6D: $input2,
+    SPIRV_Int8_1DTensorArmOfLength1: $shift
+  );
+
+
+  let results = (outs
+    SPIRV_TosaNumerical_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 1;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaPowOp : SPIRV_TosaOp<"Pow", 28, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaFloat_TensorArmUpTo6D: $input1,
+    SPIRV_TosaFloat_TensorArmUpTo6D: $input2
+  );
+
+
+  let results = (outs
+    SPIRV_TosaFloat_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaSubOp : SPIRV_TosaOp<"Sub", 29, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaNumerical_TensorArmUpTo6D: $input1,
+    SPIRV_TosaNumerical_TensorArmUpTo6D: $input2
+  );
+
+
+  let results = (outs
+    SPIRV_TosaNumerical_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaTableOp : SPIRV_TosaOp<"Table", 30, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaInteger_TensorArmUpTo6D: $input1,
+    SPIRV_TosaInteger_TensorArm1D: $table
+  );
+
+
+  let results = (outs
+    SPIRV_TosaInteger_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaAbsOp : SPIRV_TosaOp<"Abs", 31, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaNumerical_TensorArmUpTo6D: $input1
+  );
+
+
+  let results = (outs
+    SPIRV_TosaNumerical_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaBitwiseNotOp : SPIRV_TosaOp<"BitwiseNot", 32, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaInteger_TensorArmUpTo6D: $input1
+  );
+
+
+  let results = (outs
+    SPIRV_TosaInteger_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaCeilOp : SPIRV_TosaOp<"Ceil", 33, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaFloat_TensorArmUpTo6D: $input1
+  );
+
+
+  let results = (outs
+    SPIRV_TosaFloat_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaClzOp : SPIRV_TosaOp<"Clz", 34, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaInteger_TensorArmUpTo6D: $input1
+  );
+
+
+  let results = (outs
+    SPIRV_TosaInteger_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaCosOp : SPIRV_TosaOp<"Cos", 35, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaFloat_TensorArmUpTo6D: $input1
+  );
+
+
+  let results = (outs
+    SPIRV_TosaFloat_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaExpOp : SPIRV_TosaOp<"Exp", 36, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaFloat_TensorArmUpTo6D: $input1
+  );
+
+
+  let results = (outs
+    SPIRV_TosaFloat_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaFloorOp : SPIRV_TosaOp<"Floor", 37, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaFloat_TensorArmUpTo6D: $input1
+  );
+
+
+  let results = (outs
+    SPIRV_TosaFloat_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaLogOp : SPIRV_TosaOp<"Log", 38, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaFloat_TensorArmUpTo6D: $input1
+  );
+
+
+  let results = (outs
+    SPIRV_TosaFloat_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaLogicalNotOp : SPIRV_TosaOp<"LogicalNot", 39, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_Bool_TensorArmUpTo6D: $input1
+  );
+
+
+  let results = (outs
+    SPIRV_Bool_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaNegateOp : SPIRV_TosaOp<"Negate", 40, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaNumerical_TensorArmUpTo6D: $input1,
+    SPIRV_TosaNumerical_1DTensorArmOfLength1: $input1_zp,
+    SPIRV_TosaNumerical_1DTensorArmOfLength1: $output_zp
+  );
+
+
+  let results = (outs
+    SPIRV_TosaNumerical_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaReciprocalOp : SPIRV_TosaOp<"Reciprocal", 41, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaFloat_TensorArmUpTo6D: $input1
+  );
+
+
+  let results = (outs
+    SPIRV_TosaFloat_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaRsqrtOp : SPIRV_TosaOp<"Rsqrt", 42, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaFloat_TensorArmUpTo6D: $input1
+  );
+
+
+  let results = (outs
+    SPIRV_TosaFloat_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaSinOp : SPIRV_TosaOp<"Sin", 43, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaFloat_TensorArmUpTo6D: $input1
+  );
+
+
+  let results = (outs
+    SPIRV_TosaFloat_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaSelectOp : SPIRV_TosaOp<"Select", 44, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_Bool_TensorArmUpTo6D: $input1,
+    SPIRV_TosaAny_TensorArmUpTo6D: $input2,
+    SPIRV_TosaAny_TensorArmUpTo6D: $input3
+  );
+
+
+  let results = (outs
+    SPIRV_TosaAny_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 1;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaEqualOp : SPIRV_TosaOp<"Equal", 45, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaNumerical_TensorArmUpTo6D: $input1,
+    SPIRV_TosaNumerical_TensorArmUpTo6D: $input2
+  );
+
+
+  let results = (outs
+    SPIRV_Bool_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaGreaterOp : SPIRV_TosaOp<"Greater", 46, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaNumerical_TensorArmUpTo6D: $input1,
+    SPIRV_TosaNumerical_TensorArmUpTo6D: $input2
+  );
+
+
+  let results = (outs
+    SPIRV_Bool_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaGreaterEqualOp : SPIRV_TosaOp<"GreaterEqual", 47, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaNumerical_TensorArmUpTo6D: $input1,
+    SPIRV_TosaNumerical_TensorArmUpTo6D: $input2
+  );
+
+
+  let results = (outs
+    SPIRV_Bool_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaReduceAllOp : SPIRV_TosaOp<"ReduceAll", 48, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_Int32: $axis,
+    SPIRV_Bool_TensorArm1DTo6D: $input
+  );
+
+
+  let results = (outs
+    SPIRV_Bool_TensorArm1DTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaReduceAnyOp : SPIRV_TosaOp<"ReduceAny", 49, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_Int32: $axis,
+    SPIRV_Bool_TensorArm1DTo6D: $input
+  );
+
+
+  let results = (outs
+    SPIRV_Bool_TensorArm1DTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaReduceMaxOp : SPIRV_TosaOp<"ReduceMax", 50, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_Int32: $axis,
+    SPIRV_Int32: $nan_mode,
+    SPIRV_TosaNumerical_TensorArm1DTo6D: $input
+  );
+
+
+  let results = (outs
+    SPIRV_TosaNumerical_TensorArm1DTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaReduceMinOp : SPIRV_TosaOp<"ReduceMin", 51, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_Int32: $axis,
+    SPIRV_Int32: $nan_mode,
+    SPIRV_TosaNumerical_TensorArm1DTo6D: $input
+  );
+
+
+  let results = (outs
+    SPIRV_TosaNumerical_TensorArm1DTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaReduceProductOp : SPIRV_TosaOp<"ReduceProduct", 52, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_Int32: $axis,
+    SPIRV_TosaFloat_TensorArm1DTo6D: $input
+  );
+
+
+  let results = (outs
+    SPIRV_TosaFloat_TensorArm1DTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaReduceSumOp : SPIRV_TosaOp<"ReduceSum", 53, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_Int32: $axis,
+    SPIRV_TosaNumerical_TensorArm1DTo6D: $input
+  );
+
+
+  let results = (outs
+    SPIRV_TosaNumerical_TensorArm1DTo6D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaConcatOp : SPIRV_TosaOp<"Concat", 54, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_Int32: $axis,
+    Variadic<SPIRV_TosaAny_TensorArm1DTo6D>: $input1
+  );
+
+
+  let results = (outs
+    SPIRV_TosaAny_TensorArm1DTo6D: $output
+  );
+
+  let hasVerifier = 1;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaPadOp : SPIRV_TosaOp<"Pad", 55, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaAny_TensorArm1DTo6D: $input1,
+    SPIRV_Int32_1DTensorArmOfEvenLength2To12: $padding,
+    SPIRV_TosaAny_1DTensorArmOfLength1: $pad_const
+  );
+
+
+  let results = (outs
+    SPIRV_TosaAny_TensorArm1DTo6D: $output
+  );
+
+  let hasVerifier = 1;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaReshapeOp : SPIRV_TosaOp<"Reshape", 56, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaAny_TensorArmUpTo6D: $input1,
+    SPIRV_Int32_1DTensorArmOfLength1To6: $shape
+  );
+
+
+  let results = (outs
+    SPIRV_TosaAny_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 1;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaReverseOp : SPIRV_TosaOp<"Reverse", 57, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_Int32: $axis,
+    SPIRV_TosaAny_TensorArm1DTo6D: $input1
+  );
+
+
+  let results = (outs
+    SPIRV_TosaAny_TensorArm1DTo6D: $output
+  );
+
+  let hasVerifier = 1;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaSliceOp : SPIRV_TosaOp<"Slice", 58, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaAny_TensorArm1DTo6D: $input1,
+    SPIRV_Int32_1DTensorArmOfLength1To6: $start,
+    SPIRV_Int32_1DTensorArmOfLength1To6: $size
+  );
+
+
+  let results = (outs
+    SPIRV_TosaAny_TensorArm1DTo6D: $output
+  );
+
+  let hasVerifier = 1;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaTileOp : SPIRV_TosaOp<"Tile", 59, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaAny_TensorArm1DTo6D: $input1,
+    SPIRV_Int32_1DTensorArmOfLength1To6: $multiples
+  );
+
+
+  let results = (outs
+    SPIRV_TosaAny_TensorArm1DTo6D: $output
+  );
+
+  let hasVerifier = 1;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaTransposeOp : SPIRV_TosaOp<"Transpose", 60, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_Int32_1DTensorArmOfLength1To6: $perms,
+    SPIRV_TosaAny_TensorArm1DTo6D: $input1
+  );
+
+
+  let results = (outs
+    SPIRV_TosaAny_TensorArm1DTo6D: $output
+  );
+
+  let hasVerifier = 1;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaGatherOp : SPIRV_TosaOp<"Gather", 61, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaNumerical_TensorArm3D: $values,
+    SPIRV_Int32_TensorArm2D: $indices
+  );
+
+
+  let results = (outs
+    SPIRV_TosaNumerical_TensorArm3D: $output
+  );
+
+  let hasVerifier = 1;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaScatterOp : SPIRV_TosaOp<"Scatter", 62, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaNumerical_TensorArm3D: $values_in,
+    SPIRV_Int32_TensorArm2D: $indices,
+    SPIRV_TosaNumerical_TensorArm3D: $input
+  );
+
+
+  let results = (outs
+    SPIRV_TosaNumerical_TensorArm3D: $values_out
+  );
+
+  let hasVerifier = 1;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaResizeOp : SPIRV_TosaOp<"Resize", 63, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_Int32: $mode,
+    SPIRV_TosaNumerical_TensorArm4D: $input,
+    SPIRV_Int32_1DTensorArmOfLength4: $scale,
+    SPIRV_Int32_1DTensorArmOfLength2: $offset,
+    SPIRV_Int32_1DTensorArmOfLength2: $border
+  );
+
+
+  let results = (outs
+    SPIRV_TosaNumerical_TensorArm4D: $output
+  );
+
+  let hasVerifier = 0;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaCastOp : SPIRV_TosaOp<"Cast", 64, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_TosaAny_TensorArmUpTo6D: $input
+  );
+
+
+  let results = (outs
+    SPIRV_TosaAny_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 1;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+def SPIRV_TosaRescaleOp : SPIRV_TosaOp<"Rescale", 65, [Pure]> {
+  let summary = "FIXME: write summary";
+
+  let description = [{
+     FIXME: write description
+  }];
+
+
+  let arguments = (ins
+    SPIRV_Bool: $scale32,
+    SPIRV_Int32: $rounding_mode,
+    SPIRV_Bool: $per_channel,
+    SPIRV_Bool: $input_unsigned,
+    SPIRV_Bool: $output_unsigned,
+    SPIRV_TosaInteger_TensorArmUpTo6D: $input,
+    SPIRV_Int16OrInt32_TensorArm1D: $multiplier,
+    SPIRV_Int8_TensorArm1D: $shift,
+    SPIRV_TosaInteger_1DTensorArmOfLength1: $input_zp,
+    SPIRV_TosaInteger_1DTensorArmOfLength1: $output_zp
+  );
+
+
+  let results = (outs
+    SPIRV_TosaInteger_TensorArmUpTo6D: $output
+  );
+
+  let hasVerifier = 1;
+
+  let assemblyFormat = [{
+    operands attr-dict `:` `(` type(operands) `)` `->` type(results)
+  }];
+}
+
+
+#endif // MLIR_DIALECT_SPIRV_IR_TOSA_OPS
diff --git a/mlir/include/mlir/Dialect/SPIRV/IR/SPIRVTosaTypes.td b/mlir/include/mlir/Dialect/SPIRV/IR/SPIRVTosaTypes.td
new file mode 100644
index 000000000000..c9e23b60470b
--- /dev/null
+++ b/mlir/include/mlir/Dialect/SPIRV/IR/SPIRVTosaTypes.td
@@ -0,0 +1,97 @@
+//===- SPIRVTosaTypes.td - Tosa Types insts spec file ----*- tablegen -*-=//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This specifies Tosa types used by the Graph Extension and Tosa Ops.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef MLIR_DIALECT_SPIRV_IR_TOSA_TYPES
+#define MLIR_DIALECT_SPIRV_IR_TOSA_TYPES
+
+include "mlir/Dialect/SPIRV/IR/SPIRVBase.td"
+
+def SPIRV_TosaInteger : AnyIntOfWidths<[8, 16, 32, 64]>;
+def SPIRV_TosaFloat : AnyTypeOf<[BF16, SPIRV_Float16or32]>;
+def SPIRV_TosaNumerical : AnyTypeOf<[SPIRV_TosaInteger, SPIRV_TosaFloat]>;
+def SPIRV_TosaAny : AnyTypeOf<[SPIRV_TosaNumerical, SPIRV_Bool]>;
+
+// TensorARM type
+
+class RankedTensorArmOf<list<Type> allowedTypes, list<Pred> preds = [],
+                     string summary = "ranked tensorArm">
+  : ShapedContainerType<
+      allowedTypes, And<!listconcat([SPIRV_IsTensorArmType], preds)>,
+      summary, "::mlir::spirv::TensorArmType">;
+
+class TensorArmRankOf<list<Type> allowedTypes, list<int> ranks>
+  : RankedTensorArmOf<allowedTypes,
+      [HasAnyRankOfPred<ranks>],
+      !interleave(!foreach(rank, ranks, rank # "D"), "/") # " tensorArm">;
+
+def SPIRV_Int8_TensorArm1D : TensorArmRankOf<[SPIRV_Int8], [1]>;
+def SPIRV_Int16OrInt32_TensorArm1D : TensorArmRankOf<[SPIRV_Int16, SPIRV_Int32], [1]>;
+def SPIRV_Int32_TensorArm2D : TensorArmRankOf<[SPIRV_Int32], [2]>;
+def SPIRV_TosaFloat_TensorArm3D: TensorArmRankOf<[SPIRV_TosaFloat], [3]>;
+def SPIRV_TosaInteger_TensorArm1D : TensorArmRankOf<[SPIRV_TosaInteger], [1]>;
+def SPIRV_TosaNumerical_TensorArm1D : TensorArmRankOf<[SPIRV_TosaNumerical], [1]>;
+def SPIRV_TosaNumerical_TensorArm3D : TensorArmRankOf<[SPIRV_TosaNumerical], [3]>;
+def SPIRV_TosaNumerical_TensorArm4D : TensorArmRankOf<[SPIRV_TosaNumerical], [4]>;
+def SPIRV_TosaNumerical_TensorArm5D : TensorArmRankOf<[SPIRV_TosaNumerical], [5]>;
+
+def SPIRV_TosaAny_TensorArmUpTo6D : TensorArmRankOf<[SPIRV_TosaAny], [0, 1, 2, 3, 4, 5, 6]>;
+def SPIRV_TosaAny_TensorArm1DTo6D : TensorArmRankOf<[SPIRV_TosaAny], [1, 2, 3, 4, 5, 6]>;
+def SPIRV_TosaNumerical_TensorArmUpTo6D : TensorArmRankOf<[SPIRV_TosaNumerical], [0, 1, 2, 3, 4, 5, 6]>;
+def SPIRV_TosaNumerical_TensorArm1DTo6D : TensorArmRankOf<[SPIRV_TosaNumerical], [1, 2, 3, 4, 5, 6]>;
+def SPIRV_TosaInteger_TensorArmUpTo5D : TensorArmRankOf<[SPIRV_TosaInteger], [0, 1, 2, 3, 4, 5]>;
+def SPIRV_TosaInteger_TensorArmUpTo6D : TensorArmRankOf<[SPIRV_TosaInteger], [0, 1, 2, 3, 4, 5, 6]>;
+def SPIRV_TosaFloat_TensorArmUpTo6D : TensorArmRankOf<[SPIRV_TosaFloat], [0, 1, 2, 3, 4, 5, 6]>;
+def SPIRV_TosaFloat_TensorArm1DTo6D : TensorArmRankOf<[SPIRV_TosaFloat], [1, 2, 3, 4, 5, 6]>;
+def SPIRV_Bool_TensorArmUpTo6D : TensorArmRankOf<[SPIRV_Bool], [0, 1, 2, 3, 4, 5, 6]>;
+def SPIRV_Bool_TensorArm1DTo6D : TensorArmRankOf<[SPIRV_Bool], [1, 2, 3, 4, 5, 6]>;
+
+class Is1DTensorArmOfLength<list<int> allowedLengths> :
+  And<[HasAnyRankOfPred<[1]>,
+       Or<!foreach(allowedlength, allowedLengths,
+                   CPred<[{::llvm::cast<::mlir::spirv::TensorArmType>($_self).getShape()[0] == }]
+                         # allowedlength>)>]>;
+
+class SPIRV_1DTensorArmOfLengthAndType<list<int> allowedLengths, list<Type> allowedTypes> :
+  ContainerType<AnyTypeOf<allowedTypes>, Is1DTensorArmOfLength<allowedLengths>,
+    "::llvm::cast<::mlir::spirv::TensorArmType>($_self).getElementType()",
+    "rank 1 tensorArm of length " # !interleave(allowedLengths, "/"),
+    "::mlir::spirv::TensorArmType">;
+
+def SPIRV_Int32_1DTensorArmOfLength2 : SPIRV_1DTensorArmOfLengthAndType<[2], [SPIRV_Int32]>;
+def SPIRV_Int32_1DTensorArmOfLength3 : SPIRV_1DTensorArmOfLengthAndType<[3], [SPIRV_Int32]>;
+def SPIRV_Int32_1DTensorArmOfLength4 : SPIRV_1DTensorArmOfLengthAndType<[4], [SPIRV_Int32]>;
+def SPIRV_Int32_1DTensorArmOfLength6 : SPIRV_1DTensorArmOfLengthAndType<[6], [SPIRV_Int32]>;
+
+def SPIRV_Int32_1DTensorArmOfLength1To6 : SPIRV_1DTensorArmOfLengthAndType<[1,2,3,4,5,6], [SPIRV_Int32]>;
+def SPIRV_Int32_1DTensorArmOfEvenLength2To12 : SPIRV_1DTensorArmOfLengthAndType<[2,4,6,8,10,12], [SPIRV_Int32]>;
+
+def SPIRV_Int8_1DTensorArmOfLength1 : SPIRV_1DTensorArmOfLengthAndType<[1], [SPIRV_Int8]>;
+def SPIRV_TosaInteger_1DTensorArmOfLength1 : SPIRV_1DTensorArmOfLengthAndType<[1], [SPIRV_TosaInteger]>;
+def SPIRV_TosaNumerical_1DTensorArmOfLength1 : SPIRV_1DTensorArmOfLengthAndType<[1], [SPIRV_TosaNumerical]>;
+def SPIRV_TosaAny_1DTensorArmOfLength1 : SPIRV_1DTensorArmOfLengthAndType<[1], [SPIRV_TosaAny]>;
+
+// Struct type
+
+class IsStructOfLengthPred<int allowedLength> :
+  And<[SPIRV_IsStructType,
+      CPred<[{::llvm::cast<::mlir::spirv::StructType>($_self).getNumElements()
+              == }]
+            # allowedLength>]>;
+
+class IsStructOfLengthAndType<int allowedLength, list<Type> allowedTypes>
+    : MixedContainerType<AnyTypeOf<allowedTypes>, IsStructOfLengthPred<allowedLength>,
+                         "::llvm::cast<::mlir::spirv::StructType>($_self).getElementTypes()",
+                         "Struct">;
+
+def SPIRV_Struct_2_TosaFloat_TensorArm3D : IsStructOfLengthAndType<2, [SPIRV_TosaFloat_TensorArm3D]>;
+
+#endif // MLIR_DIALECT_SPIRV_IR_TOSA_TYPES
diff --git a/mlir/include/mlir/IR/Builders.h b/mlir/include/mlir/IR/Builders.h
index 5a2520b48a7b..4f3390454d79 100644
--- a/mlir/include/mlir/IR/Builders.h
+++ b/mlir/include/mlir/IR/Builders.h
@@ -24,6 +24,7 @@ class Type;
 class IntegerType;
 class FloatType;
 class FunctionType;
+class GraphType;
 class IndexType;
 class MemRefType;
 class VectorType;
@@ -81,6 +82,7 @@ public:
   IntegerType getIntegerType(unsigned width);
   IntegerType getIntegerType(unsigned width, bool isSigned);
   FunctionType getFunctionType(TypeRange inputs, TypeRange results);
+  GraphType getGraphType(TypeRange inputs, TypeRange results);
   TupleType getTupleType(TypeRange elementTypes);
   NoneType getNoneType();
 
diff --git a/mlir/include/mlir/IR/BuiltinTypes.td b/mlir/include/mlir/IR/BuiltinTypes.td
index a0c8acea91dc..08847dd11c68 100644
--- a/mlir/include/mlir/IR/BuiltinTypes.td
+++ b/mlir/include/mlir/IR/BuiltinTypes.td
@@ -403,7 +403,7 @@ def Builtin_Float128 : Builtin_CachedFloatType<"Float128", "f128"> {
 // FunctionType
 //===----------------------------------------------------------------------===//
 
-def Builtin_Function : Builtin_Type<"Function", "function"> {
+class Builtin_FunctionLike<string Name, string typeMnemonic> : Builtin_Type<Name, typeMnemonic> {
   let summary = "Map from a list of inputs to a list of results";
   let description = [{
     Syntax:
@@ -434,6 +434,7 @@ def Builtin_Function : Builtin_Type<"Function", "function"> {
     }]>
   ];
   let skipDefaultBuilders = 1;
+  let storageClass = "FunctionTypeStorage";
   let genStorageClass = 0;
   let extraClassDeclaration = [{
     /// Input types.
@@ -444,23 +445,26 @@ def Builtin_Function : Builtin_Type<"Function", "function"> {
     unsigned getNumResults() const;
     Type getResult(unsigned i) const { return getResults()[i]; }
 
-    /// Returns a clone of this function type with the given argument
+    /// Returns a clone of this function-like type with the given argument
     /// and result types.
-    FunctionType clone(TypeRange inputs, TypeRange results) const;
+    }] # Name # "Type" # [{ clone(TypeRange inputs, TypeRange results) const;
 
-    /// Returns a new function type with the specified arguments and results
+    /// Returns a new function-like type with the specified arguments and results
     /// inserted.
-    FunctionType getWithArgsAndResults(ArrayRef<unsigned> argIndices,
+    }] # Name # "Type" # [{ getWithArgsAndResults(ArrayRef<unsigned> argIndices,
                                        TypeRange argTypes,
                                        ArrayRef<unsigned> resultIndices,
                                        TypeRange resultTypes);
 
-    /// Returns a new function type without the specified arguments and results.
-    FunctionType getWithoutArgsAndResults(const BitVector &argIndices,
+    /// Returns a new function-like type without the specified arguments and results.
+    }] # Name # "Type" # [{ getWithoutArgsAndResults(const BitVector &argIndices,
                                           const BitVector &resultIndices);
   }];
 }
 
+def Builtin_Function : Builtin_FunctionLike<"Function", "function">;
+def Builtin_Graph : Builtin_FunctionLike<"Graph", "graph">;
+
 //===----------------------------------------------------------------------===//
 // IndexType
 //===----------------------------------------------------------------------===//
diff --git a/mlir/include/mlir/IR/CommonTypeConstraints.td b/mlir/include/mlir/IR/CommonTypeConstraints.td
index 45ec1846580f..aab1b01c5cff 100644
--- a/mlir/include/mlir/IR/CommonTypeConstraints.td
+++ b/mlir/include/mlir/IR/CommonTypeConstraints.td
@@ -387,6 +387,13 @@ class OpaqueType<string dialect, string name, string summary>
 def FunctionType : Type<CPred<"::llvm::isa<::mlir::FunctionType>($_self)">,
                               "function type", "::mlir::FunctionType">;
 
+// Graph Type
+
+// Any graph type.
+def GraphType : Type<CPred<"::llvm::isa<::mlir::GraphType>($_self)">,
+                              "graph type", "::mlir::GraphType">;
+
+
 // A container type is a type that has another type embedded within it.
 class ContainerType<Type etype, Pred containerPred, code elementTypeCall,
                     string descr, string cppType = "::mlir::Type"> :
diff --git a/mlir/lib/Conversion/CMakeLists.txt b/mlir/lib/Conversion/CMakeLists.txt
index f84375b6b8d6..70de029200b7 100644
--- a/mlir/lib/Conversion/CMakeLists.txt
+++ b/mlir/lib/Conversion/CMakeLists.txt
@@ -65,6 +65,7 @@ add_subdirectory(TosaToArith)
 add_subdirectory(TosaToLinalg)
 add_subdirectory(TosaToMLProgram)
 add_subdirectory(TosaToSCF)
+add_subdirectory(TosaToSPIRV)
 add_subdirectory(TosaToTensor)
 add_subdirectory(UBToLLVM)
 add_subdirectory(UBToSPIRV)
diff --git a/mlir/lib/Conversion/TosaToSPIRV/CMakeLists.txt b/mlir/lib/Conversion/TosaToSPIRV/CMakeLists.txt
new file mode 100644
index 000000000000..ed2280b37b67
--- /dev/null
+++ b/mlir/lib/Conversion/TosaToSPIRV/CMakeLists.txt
@@ -0,0 +1,21 @@
+add_mlir_conversion_library(MLIRTosaToSPIRV
+  TosaToSPIRV.cpp
+  TosaToSPIRVOps.cpp
+  TosaToSPIRVPass.cpp
+  ConvertTosaConstants.cpp
+
+  ADDITIONAL_HEADER_DIRS
+  ${MLIR_MAIN_INCLUDE_DIR}/mlir/Dialect/Tosa
+  ${MLIR_MAIN_INCLUDE_DIR}/mlir/IR
+
+  DEPENDS
+  MLIRConversionPassIncGen
+
+  LINK_LIBS PUBLIC
+  MLIRSPIRVDialect
+  MLIRIR
+  MLIRPass
+  MLIRTosaDialect
+  MLIRTosaTransforms
+  MLIRSupport
+)
diff --git a/mlir/lib/Conversion/TosaToSPIRV/ConvertTosaConstants.cpp b/mlir/lib/Conversion/TosaToSPIRV/ConvertTosaConstants.cpp
new file mode 100644
index 000000000000..3d8b52821a33
--- /dev/null
+++ b/mlir/lib/Conversion/TosaToSPIRV/ConvertTosaConstants.cpp
@@ -0,0 +1,244 @@
+//===- ConvertTosaConstants.cpp - Tosa constant to SPIR-V constant pass ---===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file implements pass to convert Tosa dialect constants to SPIRV dialect
+// constants.
+//
+//===----------------------------------------------------------------------===//
+
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Tosa/IR/TosaOps.h"
+#include "mlir/Dialect/Tosa/Transforms/Passes.h"
+#include "mlir/IR/TypeUtilities.h"
+#include "mlir/Pass/Pass.h"
+#include "mlir/Pass/PassManager.h"
+
+namespace mlir {
+#define GEN_PASS_DEF_CONVERTTOSACONSTANTSPASS
+#include "mlir/Conversion/Passes.h.inc"
+
+namespace tosa {
+
+static constexpr uint32_t SPIRV_CONSTANT_MAX_SIZE_FOR_TOSA_CONST = 16;
+
+static constexpr uint32_t SPIRV_CONSTANT_MAX_SIZE_FOR_TOSA_CONST_SHAPE = 32;
+
+static constexpr llvm::StringLiteral graphConstAttrName =
+    "spirv_graph_constant_id";
+
+std::optional<uint32_t> getGraphId(Operation *op,
+                                   const llvm::StringRef attrName) {
+  if (auto attr = op->getAttrOfType<IntegerAttr>(attrName)) {
+    return static_cast<uint32_t>(attr.getInt());
+  }
+  return std::nullopt;
+}
+
+std::optional<uint32_t> getGraphIdForConst(Operation *op) {
+  return getGraphId(op, graphConstAttrName);
+}
+
+void setGraphIdForConst(Operation *op, uint32_t id) {
+  const Type tI32 = IntegerType::get(op->getContext(), 32);
+  op->setAttr(graphConstAttrName, IntegerAttr::get(tI32, id));
+}
+
+bool isOpGraphConstantARM(Operation *op) {
+  assert((isa<tosa::ConstOp>(op) || isa<tosa::ConstShapeOp>(op)) &&
+         "Operation is not a tosa.const or tosa.const_shape");
+
+  ElementsAttr valueAttr;
+  if (auto constOp = dyn_cast_or_null<tosa::ConstOp>(op)) {
+    valueAttr = constOp.getValuesAttr();
+  }
+
+  if (auto constShapeOp = dyn_cast_or_null<tosa::ConstShapeOp>(op)) {
+    valueAttr = constShapeOp.getValuesAttr();
+  }
+
+  assert(valueAttr && "Failed to get value attribute");
+
+  uint32_t constantSize = 0;
+  if (auto elemAttr = llvm::dyn_cast<ElementsAttr>(valueAttr)) {
+    if (elemAttr.getShapedType().getRank() > 1) {
+      return true;
+    }
+    constantSize = elemAttr.size();
+  }
+
+  if (auto denseArrayAttr = llvm::dyn_cast<DenseArrayAttr>(valueAttr)) {
+    constantSize = denseArrayAttr.size();
+  }
+
+  uint32_t maxSize = isa<tosa::ConstOp>(op)
+                         ? SPIRV_CONSTANT_MAX_SIZE_FOR_TOSA_CONST
+                         : SPIRV_CONSTANT_MAX_SIZE_FOR_TOSA_CONST_SHAPE;
+
+  return constantSize > maxSize;
+}
+
+bool isOperandCompileTimeConstant(OpOperand &operand) {
+  auto op = operand.getOwner();
+  auto idx = operand.getOperandNumber();
+  return
+      // CTC from tosa.const op
+      (llvm::isa<tosa::AvgPool2dOp>(op) && (idx == 1 || idx == 2)) ||
+      (llvm::isa<tosa::Conv2DOp>(op) && (idx == 3 || idx == 4)) ||
+      (llvm::isa<tosa::Conv3DOp>(op) && (idx == 3 || idx == 4)) ||
+      (llvm::isa<tosa::DepthwiseConv2DOp>(op) && (idx == 3 || idx == 4)) ||
+      (llvm::isa<tosa::MatMulOp>(op) && (idx == 2 || idx == 3)) ||
+      (llvm::isa<tosa::TransposeConv2DOp>(op) && (idx == 3 || idx == 4)) ||
+      (llvm::isa<tosa::MulOp>(op) && idx == 2) ||
+      (llvm::isa<tosa::TableOp>(op) && idx == 1) ||
+      (llvm::isa<tosa::NegateOp>(op) && (idx == 1 || idx == 2)) ||
+      (llvm::isa<tosa::PadOp>(op) && idx == 2) ||
+      (llvm::isa<tosa::RescaleOp>(op) &&
+       (idx == 1 || idx == 2 || idx == 3 || idx == 4)) ||
+
+      // CTC from tosa.const_shape op
+      (llvm::isa<tosa::PadOp>(op) && idx == 1) ||
+      (llvm::isa<tosa::ReshapeOp>(op) && idx == 1) ||
+      (llvm::isa<tosa::SliceOp>(op) && (idx == 1 || idx == 2)) ||
+      (llvm::isa<tosa::TileOp>(op) && idx == 1) ||
+      (llvm::isa<tosa::ResizeOp>(op) && (idx == 1 || idx == 2 || idx == 3));
+}
+
+class OpInfo {
+public:
+  OpInfo(Operation &op, int32_t use_count) : _op(op), _use_count(use_count) {}
+
+  void addUseCount() { _use_count++; }
+  int32_t getUseCount() const { return _use_count; }
+  Operation &getOp() const { return _op; }
+
+private:
+  Operation &_op;
+  int32_t _use_count;
+};
+
+class ConvertTosaConstantsPass
+    : public impl::ConvertTosaConstantsPassBase<ConvertTosaConstantsPass> {
+public:
+  void runOnOperation() override {
+    if (ConvertTosaConstants(getOperation()).failed()) {
+      signalPassFailure();
+    }
+  }
+
+private:
+  using HandleType = size_t;
+
+  HandleType getOpHandle(Operation &op) {
+    return llvm::hash_combine_range(op.result_begin(), op.result_end());
+  }
+
+  bool hasOpInfo(HandleType handle) {
+    return _operations_map.count(handle) != 0;
+  }
+
+  OpInfo &getOpInfo(HandleType handle) {
+    return _operations[_operations_map.at(handle)];
+  }
+
+  void insertOpInfo(HandleType handle, Operation &op, int32_t use_count = 0) {
+    assert(!hasOpInfo(handle));
+    _operations_map.emplace(handle, _operations.size());
+    _operations.emplace_back(OpInfo(op, use_count));
+  }
+
+  HandleType getAttrHandle(Attribute &attr) { return hash_value(attr); }
+
+  bool hasAttrId(HandleType handle) {
+    return _attributes_map.count(handle) != 0;
+  }
+
+  uint32_t getAttrId(HandleType handle) { return _attributes_map.at(handle); }
+
+  void insertAttrId(HandleType handle, uint32_t attr_id) {
+    _attributes_map.emplace(handle, attr_id);
+  }
+
+  uint32_t getOrInsertAttrId(HandleType handle) {
+    if (!hasAttrId(handle)) {
+      insertAttrId(handle, _next_const_id++);
+    }
+    return getAttrId(handle);
+  }
+
+  LogicalResult ConvertTosaConstants(func::FuncOp);
+
+  std::unordered_map<HandleType, uint32_t> _attributes_map;
+  std::unordered_map<HandleType, size_t> _operations_map;
+  std::vector<OpInfo> _operations;
+
+  uint32_t _next_const_id = 0;
+};
+
+LogicalResult
+ConvertTosaConstantsPass::ConvertTosaConstants(func::FuncOp func) {
+  HandleType handle;
+
+  auto &region = func.getRegion();
+  if (!region.hasOneBlock()) {
+    return func.emitError("Invalid MLIR: multiple blocks in a region");
+  }
+
+  auto &block = region.front();
+  if (!block.isEntryBlock()) {
+    return func.emitError("Invalid MLIR: first block is not an entry block");
+  }
+
+  for (auto &op : block) {
+    // Register a tosa.const or tosa.const_shape op in the op map with inital
+    // use count of 0. Use count is incremented when an op uses this const as an
+    // operand.
+    if (llvm::isa<tosa::ConstOp>(op) || llvm::isa<tosa::ConstShapeOp>(op)) {
+      handle = getOpHandle(op);
+      insertOpInfo(handle, op, 0);
+      continue;
+    }
+
+    // For each operation, check if any operand is a tosa.const or
+    // tosa.const_shape and increment use count of its map entry.
+    for (auto &operand : op.getOpOperands()) {
+      if (auto definingOp = operand.get().getDefiningOp()) {
+        // Operand defining op is not tosa.const or tosa.const
+        if (!llvm::isa<tosa::ConstOp>(*definingOp) &&
+            !llvm::isa<tosa::ConstShapeOp>(*definingOp))
+          continue;
+
+        if (isOperandCompileTimeConstant(operand))
+          continue;
+
+        getOpInfo(getOpHandle(*definingOp)).addUseCount();
+      }
+    }
+  }
+
+  for (auto &opInfo : _operations) {
+    if (!opInfo.getUseCount())
+      continue;
+
+    auto &op = opInfo.getOp();
+    if (!isOpGraphConstantARM(&op))
+      continue;
+
+    // Assign graph constant IDs to tosa.const or tosa.const_shape ops having a
+    // non zero use count.
+    setGraphIdForConst(&op, _next_const_id++);
+  }
+
+  return success();
+}
+
+std::unique_ptr<Pass> createConvertTosaConstantsPass() {
+  return std::make_unique<ConvertTosaConstantsPass>();
+}
+
+} // namespace tosa
+} // namespace mlir
diff --git a/mlir/lib/Conversion/TosaToSPIRV/TosaToSPIRV.cpp b/mlir/lib/Conversion/TosaToSPIRV/TosaToSPIRV.cpp
new file mode 100644
index 000000000000..9723ad1e6351
--- /dev/null
+++ b/mlir/lib/Conversion/TosaToSPIRV/TosaToSPIRV.cpp
@@ -0,0 +1,135 @@
+//===- TosaToSPIRV.cpp - Tosa to SPIR-V Patterns --------------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file implements patterns to convert Tosa dialect to SPIRV dialect.
+//
+//===----------------------------------------------------------------------===//
+
+#include "mlir/Conversion/TosaToSPIRV/TosaToSPIRV.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/SPIRV/IR/SPIRVDialect.h"
+#include "mlir/Dialect/SPIRV/IR/SPIRVOps.h"
+#include "mlir/IR/Matchers.h"
+#include "mlir/IR/TypeUtilities.h"
+#include "mlir/Transforms/DialectConversion.h"
+#include "llvm/ADT/ArrayRef.h"
+#include "llvm/Support/FormatVariadic.h"
+
+#include "TosaToSPIRVBasePattern.h"
+
+namespace mlir {
+#define GEN_PASS_DEF_CONVERTTOSATOSPIRV
+#include "mlir/Conversion/Passes.h.inc"
+} // namespace mlir
+
+#define DEBUG_TYPE "tosa-to-spirv-pattern"
+using namespace mlir;
+using namespace tosa;
+
+namespace {
+
+struct FuncGraphConvert final : public TosaOpConversionPattern<func::FuncOp> {
+  using TosaOpConversionPattern<func::FuncOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(func::FuncOp funcOp, func::FuncOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    MLIRContext *context = rewriter.getContext();
+
+    StringRef name = adaptor.getSymName();
+
+    bool entryPoint = !isa<func::FuncOp>(funcOp->getParentOp());
+    if (entryPoint) {
+      auto spvModule = rewriter.create<spirv::ModuleOp>(
+          funcOp.getLoc(), spirv::AddressingModel::Logical,
+          spirv::MemoryModel::Vulkan, std::nullopt,
+          ("_spirv_tosa_" + name).str());
+
+      rewriter.setInsertionPoint(spvModule.getBody(), spvModule.begin());
+    }
+
+    FunctionType ftype = adaptor.getFunctionType();
+    ArrayAttr argAttrs = adaptor.getArgAttrsAttr();
+    ArrayAttr resAttrs = adaptor.getResAttrsAttr();
+
+    TypeConverter::SignatureConversion signatureConverter(ftype.getNumInputs());
+    if (failed(typeConverter->convertSignatureArgs(ftype.getInputs(),
+                                                   signatureConverter))) {
+      return funcOp.emitError("failed to convert function argument types");
+    }
+
+    // Update the signature of the function.
+    SmallVector<Type, 2> newResultTypes;
+    if (failed(getTypeConverter()->convertTypes(ftype.getResults(),
+                                                newResultTypes))) {
+      return funcOp.emitError("failed to convert function result types");
+    }
+
+    auto graphTy = GraphType::get(
+        context, signatureConverter.getConvertedTypes(), newResultTypes);
+    auto entryPointAttr = BoolAttr::get(context, entryPoint);
+    auto graphOp = rewriter.create<spirv::GraphARMOp>(
+        funcOp.getLoc(), graphTy, argAttrs, resAttrs, entryPointAttr, name);
+    graphOp->setAttrs(adaptor.getAttributes());
+    rewriter.inlineRegionBefore(funcOp.getBody(), graphOp.getBody(),
+                                graphOp.end());
+    if (failed(rewriter.convertRegionTypes(
+            &graphOp.getBody(), *getTypeConverter(), &signatureConverter))) {
+      return funcOp.emitError("failed to convert function regions");
+    }
+
+    if (entryPoint) {
+      uint32_t descriptorSet = 0;
+      if (auto descriptorSetAttr =
+              funcOp->getAttrOfType<IntegerAttr>("descriptor_set")) {
+        descriptorSet = static_cast<uint32_t>(descriptorSetAttr.getUInt());
+      }
+
+      StringRef varABIAttrName = spirv::getInterfaceVarABIAttrName();
+      unsigned inputs = ftype.getNumInputs();
+      unsigned outputs = ftype.getNumResults();
+      for (auto argIndex : llvm::seq<unsigned>(0, inputs)) {
+        uint32_t binding = argIndex;
+        auto abiInfo = spirv::InterfaceVarABIAttr::get(descriptorSet, binding,
+                                                       std::nullopt, context);
+        graphOp.setArgAttr(argIndex, varABIAttrName, abiInfo);
+      }
+      for (auto resIndex : llvm::seq<unsigned>(0, outputs)) {
+        uint32_t binding = resIndex + inputs;
+        auto abiInfo = spirv::InterfaceVarABIAttr::get(descriptorSet, binding,
+                                                       std::nullopt, context);
+        graphOp.setResultAttr(resIndex, varABIAttrName, abiInfo);
+      }
+    }
+
+    rewriter.eraseOp(funcOp);
+    return success();
+  }
+};
+
+/// Converts func.return to spirv.Return.
+class ReturnGraphOutputConvert final
+    : public TosaOpConversionPattern<func::ReturnOp> {
+  using TosaOpConversionPattern<func::ReturnOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(func::ReturnOp returnOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    rewriter.replaceOpWithNewOp<spirv::GraphOutputsARMOp>(returnOp,
+                                                         adaptor.getOperands());
+    return success();
+  }
+};
+
+} // namespace
+
+void mlir::tosa::populateTosaToSPIRVConversionPatterns(
+    SPIRVTypeConverter &typeConverter, RewritePatternSet &patterns) {
+  patterns.add<FuncGraphConvert, ReturnGraphOutputConvert>(
+      typeConverter, patterns.getContext());
+}
diff --git a/mlir/lib/Conversion/TosaToSPIRV/TosaToSPIRVBasePattern.h b/mlir/lib/Conversion/TosaToSPIRV/TosaToSPIRVBasePattern.h
new file mode 100644
index 000000000000..321cb7999032
--- /dev/null
+++ b/mlir/lib/Conversion/TosaToSPIRV/TosaToSPIRVBasePattern.h
@@ -0,0 +1,167 @@
+//===-- TosaToSPIRVBasePattern.h - TOSA to SPIR-V patterns ------*- C++ -*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// Provides base pattern to convert Tosa dialect to SPIR-V dialect.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef MLIR_CONVERSION_TOSATOSPIRV_TOSATOSPIRVBASEPATTERN_H
+#define MLIR_CONVERSION_TOSATOSPIRV_TOSATOSPIRVBASEPATTERN_H
+
+#include "mlir/Conversion/TosaToSPIRV/TosaToSPIRV.h"
+#include "mlir/Dialect/Tosa/IR/TosaOps.h"
+#include "mlir/ExecutionEngine/Float16bits.h"
+#include "mlir/IR/TypeUtilities.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+namespace mlir {
+
+namespace tosa {
+
+template <typename SourceOp>
+struct TosaOpConversionPattern : public OpConversionPattern<SourceOp> {
+  using OpConversionPattern<SourceOp>::OpConversionPattern;
+
+  TosaOpConversionPattern(SPIRVTypeConverter &typeConverter,
+                          MLIRContext *context)
+      : OpConversionPattern<SourceOp>::OpConversionPattern(typeConverter,
+                                                           context) {}
+
+  Type convertType(Type type) const {
+    return this->getTypeConverter()->convertType(type);
+  }
+
+  Type getConvertedElementType(Value type) const {
+    return this->getTypeConverter()->convertType(getElementTypeOrSelf(type));
+  }
+
+  uint32_t convertResizeMode(llvm::StringRef mode) const {
+    if (mode == "NEAREST_NEIGHBOR")
+      return 1;
+    if (mode == "BILINEAR")
+      return 2;
+    llvm_unreachable("Unknown mode string");
+  }
+
+  uint32_t convertNanPropagationMode(llvm::StringRef mode) const {
+    if (mode == "PROPAGATE")
+      return 1;
+    if (mode == "IGNORE")
+      return 2;
+    llvm_unreachable("Unknown mode string");
+  }
+
+  uint32_t convertRoundingMode(llvm::StringRef mode) const {
+    if (mode == "SINGLE_ROUND")
+      return 1;
+    if (mode == "INEXACT_ROUND")
+      return 2;
+    if (mode == "DOUBLE_ROUND")
+      return 3;
+    llvm_unreachable("Unknown mode string");
+  }
+
+  uint32_t convertAccType(Type accType) const {
+    if (accType.isInteger(32))
+      return 1;
+    if (accType.isF16())
+      return 2;
+    if (accType.isF32())
+      return 3;
+    if (accType.isInteger(48))
+      return 4;
+    llvm_unreachable("Unknown accumulator type");
+  }
+
+  LogicalResult maybeConvertValue(Location loc, ElementsAttr value,
+                                  ElementsAttr &out) const {
+    const ShapedType type = value.getShapedType();
+    const ShapedType convertedType =
+        llvm::dyn_cast_or_null<ShapedType>(convertType(type));
+    if (convertedType == nullptr) {
+      return emitError(
+          loc, "type conversion did not produce the expected shaped type");
+    }
+
+    // if types are the same, nothing to do here
+    if (type == convertedType) {
+      out = value;
+      return success();
+    }
+
+    const auto attr = llvm::dyn_cast_or_null<DenseElementsAttr>(value);
+    if (attr == nullptr) {
+      return emitError(
+          loc, "the attribute is not of the expected dense element attr type");
+    }
+
+    DenseElementsAttr convertedAttr = attr;
+    // if element types are not the same, convert the elements
+    if (getElementTypeOrSelf(type) != getElementTypeOrSelf(convertedType)) {
+      // type conversion is supposed to be done only for integers
+      const IntegerType intType = llvm::dyn_cast_or_null<IntegerType>(
+          getElementTypeOrSelf(convertedType));
+      if (intType == nullptr) {
+        return emitError(
+            loc, "converted element type is expected to be an integet type");
+      }
+
+      // do the conversion
+      if (convertedAttr.empty() && getElementTypeOrSelf(type).isIndex()) {
+        // special case for converting tensor<0xindex> to
+        // !spirv.arm.tensor<1xi32>
+        convertedAttr = DenseIntElementsAttr::get(convertedType, {1});
+      } else {
+        convertedAttr = attr.mapValues(intType, [&](const APInt &value) {
+          return value.sextOrTrunc(intType.getWidth());
+        });
+      }
+    }
+
+    // reshape if needed (for example from rank 0 to rank 1 dim [1])
+    out = convertedAttr.reshape(convertedType);
+    return success();
+  }
+
+  void splitConcat(int64_t axis, Value axisVal, ValueRange input,
+                   int maxNumInputs, ConversionPatternRewriter &rewriter,
+                   tosa::ConcatOp op) const {
+    SmallVector<int64_t, 6> newShape;
+    SmallVector<Value> values;
+    spirv::TosaConcatOp newOp;
+    for (const auto &[index, value] : llvm::enumerate(input)) {
+      values.push_back(value);
+      TensorType tensorType = llvm::dyn_cast<TensorType>(value.getType());
+      ArrayRef<int64_t> tensorShape = tensorType.getShape();
+      auto elementType = getElementTypeOrSelf(tensorType);
+      bool isLast = index == input.getTypes().size() - 1;
+      if (newShape.empty()) {
+        newShape.append(tensorShape.begin(), tensorShape.end());
+      } else {
+        newShape[axis] += tensorShape[axis];
+      }
+      if (values.size() % maxNumInputs == 0 || isLast) {
+        Type newType = convertType(tensorType.cloneWith(newShape, elementType));
+        if (isLast) {
+          newOp = rewriter.replaceOpWithNewOp<spirv::TosaConcatOp>(
+              op, newType, axisVal, values);
+        } else {
+          newOp = rewriter.create<spirv::TosaConcatOp>(op->getLoc(), newType,
+                                                       axisVal, values);
+          values.clear();
+          values.emplace_back(newOp.getOutput());
+        }
+      }
+    }
+  }
+};
+
+} // namespace tosa
+} // namespace mlir
+
+#endif // MLIR_CONVERSION_TOSATOSPIRV_TOSATOSPIRVBASEPATTERN_H
diff --git a/mlir/lib/Conversion/TosaToSPIRV/TosaToSPIRVOps.cpp b/mlir/lib/Conversion/TosaToSPIRV/TosaToSPIRVOps.cpp
new file mode 100644
index 000000000000..e04af19b41f6
--- /dev/null
+++ b/mlir/lib/Conversion/TosaToSPIRV/TosaToSPIRVOps.cpp
@@ -0,0 +1,1424 @@
+//===- TosaToSPIRVOps.cpp - Tosa to SPIR-V Patterns -----------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file implements patterns to convert Tosa dialect to SPIRV dialect.
+//
+//===----------------------------------------------------------------------===//
+
+#include "mlir/Conversion/TosaToSPIRV/ConvertTosaConstants.h"
+#include "mlir/Conversion/TosaToSPIRV/TosaToSPIRV.h"
+#include "mlir/Dialect/SPIRV/IR/SPIRVDialect.h"
+#include "mlir/Dialect/SPIRV/IR/SPIRVOps.h"
+#include "mlir/Dialect/SPIRV/IR/SPIRVTypes.h"
+#include "mlir/Dialect/Tosa/IR/TosaOps.h"
+#include "mlir/IR/Matchers.h"
+#include "mlir/Transforms/DialectConversion.h"
+#include "llvm/ADT/ArrayRef.h"
+#include "llvm/Support/FormatVariadic.h"
+
+#include "TosaToSPIRVBasePattern.h"
+
+namespace mlir {
+#define GEN_PASS_DEF_CONVERTTOSATOSPIRV
+#include "mlir/Conversion/Passes.h.inc"
+} // namespace mlir
+
+#define DEBUG_TYPE "tosa-to-spirv-ops-pattern"
+using namespace mlir;
+using namespace tosa;
+
+namespace {
+
+DenseIntElementsAttr getI32TensorArmAttr(ArrayRef<int32_t> values,
+                                         ConversionPatternRewriter &rewriter) {
+  return DenseIntElementsAttr::get(
+      spirv::TensorArmType::get(static_cast<int64_t>(values.size()),
+                                IntegerType::get(rewriter.getContext(), 32)),
+      values);
+}
+
+constexpr int MAX_NUM_INPUTS = 200;
+
+struct TosaArgMaxConvert final
+    : public TosaOpConversionPattern<tosa::ArgMaxOp> {
+  using TosaOpConversionPattern<tosa::ArgMaxOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::ArgMaxOp op, tosa::ArgMaxOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Type tI32 = rewriter.getIntegerType(32);
+    Value axis = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tI32, adaptor.getAxisAttr());
+    auto nanModeAttr = rewriter.getIntegerAttr(
+        tI32, convertNanPropagationMode(adaptor.getNanMode()));
+    Value nanMode = rewriter.createOrFold<spirv::ConstantOp>(op.getLoc(), tI32,
+                                                             nanModeAttr);
+    Value input = adaptor.getInput();
+    rewriter.replaceOpWithNewOp<spirv::TosaArgMaxOp>(
+        op, convertType(op.getType()), axis, nanMode, input);
+    return success();
+  }
+};
+
+struct TosaAvgPool2DConvert final
+    : public TosaOpConversionPattern<tosa::AvgPool2dOp> {
+  using TosaOpConversionPattern<tosa::AvgPool2dOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::AvgPool2dOp op, tosa::AvgPool2dOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Type tI32 = rewriter.getIntegerType(32);
+    Type tKernel = spirv::TensorArmType::get({2}, tI32);
+    Type tStride = spirv::TensorArmType::get({2}, tI32);
+    Type tPad = spirv::TensorArmType::get({4}, tI32);
+    auto kernelTensorAttr = getI32TensorArmAttr(
+        SmallVector<int32_t>(adaptor.getKernel()), rewriter);
+    Value kernel = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tKernel, kernelTensorAttr);
+    auto strideTensorAttr = getI32TensorArmAttr(
+        SmallVector<int32_t>(adaptor.getStride()), rewriter);
+    Value stride = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tStride, strideTensorAttr);
+    auto padTensorAttr =
+        getI32TensorArmAttr(SmallVector<int32_t>(adaptor.getPad()), rewriter);
+    Value pad = rewriter.createOrFold<spirv::ConstantOp>(op.getLoc(), tPad,
+                                                         padTensorAttr);
+    auto accTypeAttr =
+        rewriter.getIntegerAttr(tI32, convertAccType(adaptor.getAccType()));
+    Value accType = rewriter.createOrFold<spirv::ConstantOp>(op.getLoc(), tI32,
+                                                             accTypeAttr);
+    Value input = adaptor.getInput();
+    Value inputZp = adaptor.getInputZp();
+    Value outputZp = adaptor.getOutputZp();
+    rewriter.replaceOpWithNewOp<spirv::TosaAvgPool2DOp>(
+        op, convertType(op.getType()), kernel, stride, pad, accType, input,
+        inputZp, outputZp);
+    return success();
+  }
+};
+
+struct TosaConv2DConvert final
+    : public TosaOpConversionPattern<tosa::Conv2DOp> {
+  using TosaOpConversionPattern<tosa::Conv2DOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::Conv2DOp op, tosa::Conv2DOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Type tI32 = rewriter.getIntegerType(32);
+    Type tPad = spirv::TensorArmType::get({4}, tI32);
+    Type tStride = spirv::TensorArmType::get({2}, tI32);
+    Type tDilation = spirv::TensorArmType::get({2}, tI32);
+    Type tI1 = rewriter.getIntegerType(1);
+    auto padTensorAttr =
+        getI32TensorArmAttr(SmallVector<int32_t>(adaptor.getPad()), rewriter);
+    Value pad = rewriter.createOrFold<spirv::ConstantOp>(op.getLoc(), tPad,
+                                                         padTensorAttr);
+    auto strideTensorAttr = getI32TensorArmAttr(
+        SmallVector<int32_t>(adaptor.getStride()), rewriter);
+    Value stride = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tStride, strideTensorAttr);
+    auto dilationTensorAttr = getI32TensorArmAttr(
+        SmallVector<int32_t>(adaptor.getDilation()), rewriter);
+    Value dilation = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tDilation, dilationTensorAttr);
+    auto accTypeAttr =
+        rewriter.getIntegerAttr(tI32, convertAccType(adaptor.getAccType()));
+    Value accType = rewriter.createOrFold<spirv::ConstantOp>(op.getLoc(), tI32,
+                                                             accTypeAttr);
+    Value localBound = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tI1, adaptor.getLocalBoundAttr());
+    Value input = adaptor.getInput();
+    Value weight = adaptor.getWeight();
+    Value bias = adaptor.getBias();
+    Value inputZp = adaptor.getInputZp();
+    Value weightZp = adaptor.getWeightZp();
+    rewriter.replaceOpWithNewOp<spirv::TosaConv2DOp>(
+        op, convertType(op.getType()), pad, stride, dilation, accType,
+        localBound, input, weight, bias, inputZp, weightZp);
+    return success();
+  }
+};
+
+struct TosaConv3DConvert final
+    : public TosaOpConversionPattern<tosa::Conv3DOp> {
+  using TosaOpConversionPattern<tosa::Conv3DOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::Conv3DOp op, tosa::Conv3DOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Type tI32 = rewriter.getIntegerType(32);
+    Type tPad = spirv::TensorArmType::get({6}, tI32);
+    Type tStride = spirv::TensorArmType::get({3}, tI32);
+    Type tDilation = spirv::TensorArmType::get({3}, tI32);
+    Type tI1 = rewriter.getIntegerType(1);
+    auto padTensorAttr =
+        getI32TensorArmAttr(SmallVector<int32_t>(adaptor.getPad()), rewriter);
+    Value pad = rewriter.createOrFold<spirv::ConstantOp>(op.getLoc(), tPad,
+                                                         padTensorAttr);
+    auto strideTensorAttr = getI32TensorArmAttr(
+        SmallVector<int32_t>(adaptor.getStride()), rewriter);
+    Value stride = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tStride, strideTensorAttr);
+    auto dilationTensorAttr = getI32TensorArmAttr(
+        SmallVector<int32_t>(adaptor.getDilation()), rewriter);
+    Value dilation = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tDilation, dilationTensorAttr);
+    auto accTypeAttr =
+        rewriter.getIntegerAttr(tI32, convertAccType(adaptor.getAccType()));
+    Value accType = rewriter.createOrFold<spirv::ConstantOp>(op.getLoc(), tI32,
+                                                             accTypeAttr);
+    Value localBound = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tI1, adaptor.getLocalBoundAttr());
+    Value input = adaptor.getInput();
+    Value weight = adaptor.getWeight();
+    Value bias = adaptor.getBias();
+    Value inputZp = adaptor.getInputZp();
+    Value weightZp = adaptor.getWeightZp();
+    rewriter.replaceOpWithNewOp<spirv::TosaConv3DOp>(
+        op, convertType(op.getType()), pad, stride, dilation, accType,
+        localBound, input, weight, bias, inputZp, weightZp);
+    return success();
+  }
+};
+
+struct TosaDepthwiseConv2DConvert final
+    : public TosaOpConversionPattern<tosa::DepthwiseConv2DOp> {
+  using TosaOpConversionPattern<
+      tosa::DepthwiseConv2DOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::DepthwiseConv2DOp op,
+                  tosa::DepthwiseConv2DOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Type tI32 = rewriter.getIntegerType(32);
+    Type tPad = spirv::TensorArmType::get({4}, tI32);
+    Type tStride = spirv::TensorArmType::get({2}, tI32);
+    Type tDilation = spirv::TensorArmType::get({2}, tI32);
+    Type tI1 = rewriter.getIntegerType(1);
+    auto padTensorAttr =
+        getI32TensorArmAttr(SmallVector<int32_t>(adaptor.getPad()), rewriter);
+    Value pad = rewriter.createOrFold<spirv::ConstantOp>(op.getLoc(), tPad,
+                                                         padTensorAttr);
+    auto strideTensorAttr = getI32TensorArmAttr(
+        SmallVector<int32_t>(adaptor.getStride()), rewriter);
+    Value stride = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tStride, strideTensorAttr);
+    auto dilationTensorAttr = getI32TensorArmAttr(
+        SmallVector<int32_t>(adaptor.getDilation()), rewriter);
+    Value dilation = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tDilation, dilationTensorAttr);
+    auto accTypeAttr =
+        rewriter.getIntegerAttr(tI32, convertAccType(adaptor.getAccType()));
+    Value accType = rewriter.createOrFold<spirv::ConstantOp>(op.getLoc(), tI32,
+                                                             accTypeAttr);
+    Value localBound = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tI1, adaptor.getLocalBoundAttr());
+    Value input = adaptor.getInput();
+    Value weight = adaptor.getWeight();
+    Value bias = adaptor.getBias();
+    Value inputZp = adaptor.getInputZp();
+    Value weightZp = adaptor.getWeightZp();
+    rewriter.replaceOpWithNewOp<spirv::TosaDepthwiseConv2DOp>(
+        op, convertType(op.getType()), pad, stride, dilation, accType,
+        localBound, input, weight, bias, inputZp, weightZp);
+    return success();
+  }
+};
+
+struct TosaFFT2DConvert final : public TosaOpConversionPattern<tosa::FFT2dOp> {
+  using TosaOpConversionPattern<tosa::FFT2dOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::FFT2dOp op, tosa::FFT2dOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Type tI1 = rewriter.getIntegerType(1);
+    Value inverse = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tI1, adaptor.getInverseAttr());
+    Value localBound = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tI1, adaptor.getLocalBoundAttr());
+    Value inputReal = adaptor.getInputReal();
+    Value inputImag = adaptor.getInputImag();
+    spirv::StructType structType = spirv::StructType::get(
+        {convertType(op.getType(0)), convertType(op.getType(1))});
+
+    Value result = rewriter.create<spirv::TosaFFT2DOp>(
+        op->getLoc(), structType, inverse, localBound, inputReal, inputImag);
+
+    Value outputReal = rewriter.create<spirv::CompositeExtractOp>(
+        op->getLoc(), result, llvm::ArrayRef(0));
+    Value outputImag = rewriter.create<spirv::CompositeExtractOp>(
+        op->getLoc(), result, llvm::ArrayRef(1));
+
+    rewriter.replaceOp(op, {outputReal, outputImag});
+
+    return success();
+  }
+};
+
+struct TosaMatMulConvert final
+    : public TosaOpConversionPattern<tosa::MatMulOp> {
+  using TosaOpConversionPattern<tosa::MatMulOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::MatMulOp op, tosa::MatMulOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value A = adaptor.getA();
+    Value B = adaptor.getB();
+    Value AZp = adaptor.getAZp();
+    Value BZp = adaptor.getBZp();
+    rewriter.replaceOpWithNewOp<spirv::TosaMatMulOp>(
+        op, convertType(op.getType()), A, B, AZp, BZp);
+    return success();
+  }
+};
+
+struct TosaMaxPool2DConvert final
+    : public TosaOpConversionPattern<tosa::MaxPool2dOp> {
+  using TosaOpConversionPattern<tosa::MaxPool2dOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::MaxPool2dOp op, tosa::MaxPool2dOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Type tI32 = rewriter.getIntegerType(32);
+    Type tKernel = spirv::TensorArmType::get({2}, tI32);
+    Type tStride = spirv::TensorArmType::get({2}, tI32);
+    Type tPad = spirv::TensorArmType::get({4}, tI32);
+    auto kernelTensorAttr = getI32TensorArmAttr(
+        SmallVector<int32_t>(adaptor.getKernel()), rewriter);
+    Value kernel = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tKernel, kernelTensorAttr);
+    auto strideTensorAttr = getI32TensorArmAttr(
+        SmallVector<int32_t>(adaptor.getStride()), rewriter);
+    Value stride = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tStride, strideTensorAttr);
+    auto padTensorAttr =
+        getI32TensorArmAttr(SmallVector<int32_t>(adaptor.getPad()), rewriter);
+    Value pad = rewriter.createOrFold<spirv::ConstantOp>(op.getLoc(), tPad,
+                                                         padTensorAttr);
+    auto nanModeAttr = rewriter.getIntegerAttr(
+        tI32, convertNanPropagationMode(adaptor.getNanMode()));
+    Value nanMode = rewriter.createOrFold<spirv::ConstantOp>(op.getLoc(), tI32,
+                                                             nanModeAttr);
+    Value input = adaptor.getInput();
+    rewriter.replaceOpWithNewOp<spirv::TosaMaxPool2DOp>(
+        op, convertType(op.getType()), kernel, stride, pad, nanMode, input);
+    return success();
+  }
+};
+
+struct TosaRFFT2DConvert final
+    : public TosaOpConversionPattern<tosa::RFFT2dOp> {
+  using TosaOpConversionPattern<tosa::RFFT2dOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::RFFT2dOp op, tosa::RFFT2dOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Type tI1 = rewriter.getIntegerType(1);
+    Value localBound = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tI1, adaptor.getLocalBoundAttr());
+    Value inputReal = adaptor.getInputReal();
+    spirv::StructType structType = spirv::StructType::get(
+        {convertType(op.getType(0)), convertType(op.getType(1))});
+
+    Value result = rewriter.create<spirv::TosaRFFT2DOp>(
+        op->getLoc(), structType, localBound, inputReal);
+
+    Value outputReal = rewriter.create<spirv::CompositeExtractOp>(
+        op->getLoc(), result, llvm::ArrayRef(0));
+    Value outputImag = rewriter.create<spirv::CompositeExtractOp>(
+        op->getLoc(), result, llvm::ArrayRef(1));
+
+    rewriter.replaceOp(op, {outputReal, outputImag});
+
+    return success();
+  }
+};
+
+struct TosaTransposeConv2DConvert final
+    : public TosaOpConversionPattern<tosa::TransposeConv2DOp> {
+  using TosaOpConversionPattern<
+      tosa::TransposeConv2DOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::TransposeConv2DOp op,
+                  tosa::TransposeConv2DOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Type tI32 = rewriter.getIntegerType(32);
+    Type tOutPad = spirv::TensorArmType::get({4}, tI32);
+    Type tStride = spirv::TensorArmType::get({2}, tI32);
+    Type tI1 = rewriter.getIntegerType(1);
+    auto outPadTensorAttr = getI32TensorArmAttr(
+        SmallVector<int32_t>(adaptor.getOutPad()), rewriter);
+    Value outPad = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tOutPad, outPadTensorAttr);
+    auto strideTensorAttr = getI32TensorArmAttr(
+        SmallVector<int32_t>(adaptor.getStride()), rewriter);
+    Value stride = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tStride, strideTensorAttr);
+    auto accTypeAttr =
+        rewriter.getIntegerAttr(tI32, convertAccType(adaptor.getAccType()));
+    Value accType = rewriter.createOrFold<spirv::ConstantOp>(op.getLoc(), tI32,
+                                                             accTypeAttr);
+    Value localBound = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tI1, adaptor.getLocalBoundAttr());
+    Value input = adaptor.getInput();
+    Value weight = adaptor.getWeight();
+    Value bias = adaptor.getBias();
+    Value inputZp = adaptor.getInputZp();
+    Value weightZp = adaptor.getWeightZp();
+    rewriter.replaceOpWithNewOp<spirv::TosaTransposeConv2DOp>(
+        op, convertType(op.getType()), outPad, stride, accType, localBound,
+        input, weight, bias, inputZp, weightZp);
+    return success();
+  }
+};
+
+struct TosaClampConvert final : public TosaOpConversionPattern<tosa::ClampOp> {
+  using TosaOpConversionPattern<tosa::ClampOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::ClampOp op, tosa::ClampOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Type tOutput = getConvertedElementType(op.getOutput());
+    Type tI32 = rewriter.getIntegerType(32);
+    Value minVal = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tOutput, adaptor.getMinValAttr());
+    Value maxVal = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tOutput, adaptor.getMaxValAttr());
+    auto nanModeAttr = rewriter.getIntegerAttr(
+        tI32, convertNanPropagationMode(adaptor.getNanMode()));
+    Value nanMode = rewriter.createOrFold<spirv::ConstantOp>(op.getLoc(), tI32,
+                                                             nanModeAttr);
+    Value input = adaptor.getInput();
+    rewriter.replaceOpWithNewOp<spirv::TosaClampOp>(
+        op, convertType(op.getType()), minVal, maxVal, nanMode, input);
+    return success();
+  }
+};
+
+struct TosaErfConvert final : public TosaOpConversionPattern<tosa::ErfOp> {
+  using TosaOpConversionPattern<tosa::ErfOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::ErfOp op, tosa::ErfOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input = adaptor.getInput();
+    rewriter.replaceOpWithNewOp<spirv::TosaErfOp>(op, convertType(op.getType()),
+                                                  input);
+    return success();
+  }
+};
+
+struct TosaSigmoidConvert final
+    : public TosaOpConversionPattern<tosa::SigmoidOp> {
+  using TosaOpConversionPattern<tosa::SigmoidOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::SigmoidOp op, tosa::SigmoidOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input = adaptor.getInput();
+    rewriter.replaceOpWithNewOp<spirv::TosaSigmoidOp>(
+        op, convertType(op.getType()), input);
+    return success();
+  }
+};
+
+struct TosaTanhConvert final : public TosaOpConversionPattern<tosa::TanhOp> {
+  using TosaOpConversionPattern<tosa::TanhOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::TanhOp op, tosa::TanhOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input = adaptor.getInput();
+    rewriter.replaceOpWithNewOp<spirv::TosaTanhOp>(
+        op, convertType(op.getType()), input);
+    return success();
+  }
+};
+
+struct TosaAddConvert final : public TosaOpConversionPattern<tosa::AddOp> {
+  using TosaOpConversionPattern<tosa::AddOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::AddOp op, tosa::AddOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    Value input2 = adaptor.getInput2();
+    rewriter.replaceOpWithNewOp<spirv::TosaAddOp>(op, convertType(op.getType()),
+                                                  input1, input2);
+    return success();
+  }
+};
+
+struct TosaArithmeticRightShiftConvert final
+    : public TosaOpConversionPattern<tosa::ArithmeticRightShiftOp> {
+  using TosaOpConversionPattern<
+      tosa::ArithmeticRightShiftOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::ArithmeticRightShiftOp op,
+                  tosa::ArithmeticRightShiftOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Type tI1 = rewriter.getIntegerType(1);
+    Value round = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tI1, adaptor.getRoundAttr());
+    Value input1 = adaptor.getInput1();
+    Value input2 = adaptor.getInput2();
+    rewriter.replaceOpWithNewOp<spirv::TosaArithmeticRightShiftOp>(
+        op, convertType(op.getType()), round, input1, input2);
+    return success();
+  }
+};
+
+struct TosaBitwiseAndConvert final
+    : public TosaOpConversionPattern<tosa::BitwiseAndOp> {
+  using TosaOpConversionPattern<tosa::BitwiseAndOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::BitwiseAndOp op, tosa::BitwiseAndOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    Value input2 = adaptor.getInput2();
+    rewriter.replaceOpWithNewOp<spirv::TosaBitwiseAndOp>(
+        op, convertType(op.getType()), input1, input2);
+    return success();
+  }
+};
+
+struct TosaBitwiseOrConvert final
+    : public TosaOpConversionPattern<tosa::BitwiseOrOp> {
+  using TosaOpConversionPattern<tosa::BitwiseOrOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::BitwiseOrOp op, tosa::BitwiseOrOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    Value input2 = adaptor.getInput2();
+    rewriter.replaceOpWithNewOp<spirv::TosaBitwiseOrOp>(
+        op, convertType(op.getType()), input1, input2);
+    return success();
+  }
+};
+
+struct TosaBitwiseXorConvert final
+    : public TosaOpConversionPattern<tosa::BitwiseXorOp> {
+  using TosaOpConversionPattern<tosa::BitwiseXorOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::BitwiseXorOp op, tosa::BitwiseXorOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    Value input2 = adaptor.getInput2();
+    rewriter.replaceOpWithNewOp<spirv::TosaBitwiseXorOp>(
+        op, convertType(op.getType()), input1, input2);
+    return success();
+  }
+};
+
+struct TosaIntDivConvert final
+    : public TosaOpConversionPattern<tosa::IntDivOp> {
+  using TosaOpConversionPattern<tosa::IntDivOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::IntDivOp op, tosa::IntDivOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    Value input2 = adaptor.getInput2();
+    rewriter.replaceOpWithNewOp<spirv::TosaIntDivOp>(
+        op, convertType(op.getType()), input1, input2);
+    return success();
+  }
+};
+
+struct TosaLogicalAndConvert final
+    : public TosaOpConversionPattern<tosa::LogicalAndOp> {
+  using TosaOpConversionPattern<tosa::LogicalAndOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::LogicalAndOp op, tosa::LogicalAndOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    Value input2 = adaptor.getInput2();
+    rewriter.replaceOpWithNewOp<spirv::TosaLogicalAndOp>(
+        op, convertType(op.getType()), input1, input2);
+    return success();
+  }
+};
+
+struct TosaLogicalLeftShiftConvert final
+    : public TosaOpConversionPattern<tosa::LogicalLeftShiftOp> {
+  using TosaOpConversionPattern<
+      tosa::LogicalLeftShiftOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::LogicalLeftShiftOp op,
+                  tosa::LogicalLeftShiftOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    Value input2 = adaptor.getInput2();
+    rewriter.replaceOpWithNewOp<spirv::TosaLogicalLeftShiftOp>(
+        op, convertType(op.getType()), input1, input2);
+    return success();
+  }
+};
+
+struct TosaLogicalRightShiftConvert final
+    : public TosaOpConversionPattern<tosa::LogicalRightShiftOp> {
+  using TosaOpConversionPattern<
+      tosa::LogicalRightShiftOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::LogicalRightShiftOp op,
+                  tosa::LogicalRightShiftOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    Value input2 = adaptor.getInput2();
+    rewriter.replaceOpWithNewOp<spirv::TosaLogicalRightShiftOp>(
+        op, convertType(op.getType()), input1, input2);
+    return success();
+  }
+};
+
+struct TosaLogicalOrConvert final
+    : public TosaOpConversionPattern<tosa::LogicalOrOp> {
+  using TosaOpConversionPattern<tosa::LogicalOrOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::LogicalOrOp op, tosa::LogicalOrOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    Value input2 = adaptor.getInput2();
+    rewriter.replaceOpWithNewOp<spirv::TosaLogicalOrOp>(
+        op, convertType(op.getType()), input1, input2);
+    return success();
+  }
+};
+
+struct TosaLogicalXorConvert final
+    : public TosaOpConversionPattern<tosa::LogicalXorOp> {
+  using TosaOpConversionPattern<tosa::LogicalXorOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::LogicalXorOp op, tosa::LogicalXorOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    Value input2 = adaptor.getInput2();
+    rewriter.replaceOpWithNewOp<spirv::TosaLogicalXorOp>(
+        op, convertType(op.getType()), input1, input2);
+    return success();
+  }
+};
+
+struct TosaMaximumConvert final
+    : public TosaOpConversionPattern<tosa::MaximumOp> {
+  using TosaOpConversionPattern<tosa::MaximumOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::MaximumOp op, tosa::MaximumOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Type tI32 = rewriter.getIntegerType(32);
+    auto nanModeAttr = rewriter.getIntegerAttr(
+        tI32, convertNanPropagationMode(adaptor.getNanMode()));
+    Value nanMode = rewriter.createOrFold<spirv::ConstantOp>(op.getLoc(), tI32,
+                                                             nanModeAttr);
+    Value input1 = adaptor.getInput1();
+    Value input2 = adaptor.getInput2();
+    rewriter.replaceOpWithNewOp<spirv::TosaMaximumOp>(
+        op, convertType(op.getType()), nanMode, input1, input2);
+    return success();
+  }
+};
+
+struct TosaMinimumConvert final
+    : public TosaOpConversionPattern<tosa::MinimumOp> {
+  using TosaOpConversionPattern<tosa::MinimumOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::MinimumOp op, tosa::MinimumOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Type tI32 = rewriter.getIntegerType(32);
+    auto nanModeAttr = rewriter.getIntegerAttr(
+        tI32, convertNanPropagationMode(adaptor.getNanMode()));
+    Value nanMode = rewriter.createOrFold<spirv::ConstantOp>(op.getLoc(), tI32,
+                                                             nanModeAttr);
+    Value input1 = adaptor.getInput1();
+    Value input2 = adaptor.getInput2();
+    rewriter.replaceOpWithNewOp<spirv::TosaMinimumOp>(
+        op, convertType(op.getType()), nanMode, input1, input2);
+    return success();
+  }
+};
+
+struct TosaMulConvert final : public TosaOpConversionPattern<tosa::MulOp> {
+  using TosaOpConversionPattern<tosa::MulOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::MulOp op, tosa::MulOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    Value input2 = adaptor.getInput2();
+    Value shift = adaptor.getShift();
+    rewriter.replaceOpWithNewOp<spirv::TosaMulOp>(op, convertType(op.getType()),
+                                                  input1, input2, shift);
+    return success();
+  }
+};
+
+struct TosaPowConvert final : public TosaOpConversionPattern<tosa::PowOp> {
+  using TosaOpConversionPattern<tosa::PowOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::PowOp op, tosa::PowOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    Value input2 = adaptor.getInput2();
+    rewriter.replaceOpWithNewOp<spirv::TosaPowOp>(op, convertType(op.getType()),
+                                                  input1, input2);
+    return success();
+  }
+};
+
+struct TosaSubConvert final : public TosaOpConversionPattern<tosa::SubOp> {
+  using TosaOpConversionPattern<tosa::SubOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::SubOp op, tosa::SubOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    Value input2 = adaptor.getInput2();
+    rewriter.replaceOpWithNewOp<spirv::TosaSubOp>(op, convertType(op.getType()),
+                                                  input1, input2);
+    return success();
+  }
+};
+
+struct TosaTableConvert final : public TosaOpConversionPattern<tosa::TableOp> {
+  using TosaOpConversionPattern<tosa::TableOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::TableOp op, tosa::TableOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    Value table = adaptor.getTable();
+    rewriter.replaceOpWithNewOp<spirv::TosaTableOp>(
+        op, convertType(op.getType()), input1, table);
+    return success();
+  }
+};
+
+struct TosaAbsConvert final : public TosaOpConversionPattern<tosa::AbsOp> {
+  using TosaOpConversionPattern<tosa::AbsOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::AbsOp op, tosa::AbsOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    rewriter.replaceOpWithNewOp<spirv::TosaAbsOp>(op, convertType(op.getType()),
+                                                  input1);
+    return success();
+  }
+};
+
+struct TosaBitwiseNotConvert final
+    : public TosaOpConversionPattern<tosa::BitwiseNotOp> {
+  using TosaOpConversionPattern<tosa::BitwiseNotOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::BitwiseNotOp op, tosa::BitwiseNotOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    rewriter.replaceOpWithNewOp<spirv::TosaBitwiseNotOp>(
+        op, convertType(op.getType()), input1);
+    return success();
+  }
+};
+
+struct TosaCeilConvert final : public TosaOpConversionPattern<tosa::CeilOp> {
+  using TosaOpConversionPattern<tosa::CeilOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::CeilOp op, tosa::CeilOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    rewriter.replaceOpWithNewOp<spirv::TosaCeilOp>(
+        op, convertType(op.getType()), input1);
+    return success();
+  }
+};
+
+struct TosaClzConvert final : public TosaOpConversionPattern<tosa::ClzOp> {
+  using TosaOpConversionPattern<tosa::ClzOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::ClzOp op, tosa::ClzOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    rewriter.replaceOpWithNewOp<spirv::TosaClzOp>(op, convertType(op.getType()),
+                                                  input1);
+    return success();
+  }
+};
+
+struct TosaCosConvert final : public TosaOpConversionPattern<tosa::CosOp> {
+  using TosaOpConversionPattern<tosa::CosOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::CosOp op, tosa::CosOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    rewriter.replaceOpWithNewOp<spirv::TosaCosOp>(op, convertType(op.getType()),
+                                                  input1);
+    return success();
+  }
+};
+
+struct TosaExpConvert final : public TosaOpConversionPattern<tosa::ExpOp> {
+  using TosaOpConversionPattern<tosa::ExpOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::ExpOp op, tosa::ExpOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    rewriter.replaceOpWithNewOp<spirv::TosaExpOp>(op, convertType(op.getType()),
+                                                  input1);
+    return success();
+  }
+};
+
+struct TosaFloorConvert final : public TosaOpConversionPattern<tosa::FloorOp> {
+  using TosaOpConversionPattern<tosa::FloorOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::FloorOp op, tosa::FloorOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    rewriter.replaceOpWithNewOp<spirv::TosaFloorOp>(
+        op, convertType(op.getType()), input1);
+    return success();
+  }
+};
+
+struct TosaLogConvert final : public TosaOpConversionPattern<tosa::LogOp> {
+  using TosaOpConversionPattern<tosa::LogOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::LogOp op, tosa::LogOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    rewriter.replaceOpWithNewOp<spirv::TosaLogOp>(op, convertType(op.getType()),
+                                                  input1);
+    return success();
+  }
+};
+
+struct TosaLogicalNotConvert final
+    : public TosaOpConversionPattern<tosa::LogicalNotOp> {
+  using TosaOpConversionPattern<tosa::LogicalNotOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::LogicalNotOp op, tosa::LogicalNotOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    rewriter.replaceOpWithNewOp<spirv::TosaLogicalNotOp>(
+        op, convertType(op.getType()), input1);
+    return success();
+  }
+};
+
+struct TosaNegateConvert final
+    : public TosaOpConversionPattern<tosa::NegateOp> {
+  using TosaOpConversionPattern<tosa::NegateOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::NegateOp op, tosa::NegateOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    Value input1Zp = adaptor.getInput1Zp();
+    Value outputZp = adaptor.getOutputZp();
+    rewriter.replaceOpWithNewOp<spirv::TosaNegateOp>(
+        op, convertType(op.getType()), input1, input1Zp, outputZp);
+    return success();
+  }
+};
+
+struct TosaReciprocalConvert final
+    : public TosaOpConversionPattern<tosa::ReciprocalOp> {
+  using TosaOpConversionPattern<tosa::ReciprocalOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::ReciprocalOp op, tosa::ReciprocalOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    rewriter.replaceOpWithNewOp<spirv::TosaReciprocalOp>(
+        op, convertType(op.getType()), input1);
+    return success();
+  }
+};
+
+struct TosaRsqrtConvert final : public TosaOpConversionPattern<tosa::RsqrtOp> {
+  using TosaOpConversionPattern<tosa::RsqrtOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::RsqrtOp op, tosa::RsqrtOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    rewriter.replaceOpWithNewOp<spirv::TosaRsqrtOp>(
+        op, convertType(op.getType()), input1);
+    return success();
+  }
+};
+
+struct TosaSinConvert final : public TosaOpConversionPattern<tosa::SinOp> {
+  using TosaOpConversionPattern<tosa::SinOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::SinOp op, tosa::SinOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    rewriter.replaceOpWithNewOp<spirv::TosaSinOp>(op, convertType(op.getType()),
+                                                  input1);
+    return success();
+  }
+};
+
+struct TosaSelectConvert final
+    : public TosaOpConversionPattern<tosa::SelectOp> {
+  using TosaOpConversionPattern<tosa::SelectOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::SelectOp op, tosa::SelectOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    Value input2 = adaptor.getInput2();
+    Value input3 = adaptor.getInput3();
+    rewriter.replaceOpWithNewOp<spirv::TosaSelectOp>(
+        op, convertType(op.getType()), input1, input2, input3);
+    return success();
+  }
+};
+
+struct TosaEqualConvert final : public TosaOpConversionPattern<tosa::EqualOp> {
+  using TosaOpConversionPattern<tosa::EqualOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::EqualOp op, tosa::EqualOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    Value input2 = adaptor.getInput2();
+    rewriter.replaceOpWithNewOp<spirv::TosaEqualOp>(
+        op, convertType(op.getType()), input1, input2);
+    return success();
+  }
+};
+
+struct TosaGreaterConvert final
+    : public TosaOpConversionPattern<tosa::GreaterOp> {
+  using TosaOpConversionPattern<tosa::GreaterOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::GreaterOp op, tosa::GreaterOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    Value input2 = adaptor.getInput2();
+    rewriter.replaceOpWithNewOp<spirv::TosaGreaterOp>(
+        op, convertType(op.getType()), input1, input2);
+    return success();
+  }
+};
+
+struct TosaGreaterEqualConvert final
+    : public TosaOpConversionPattern<tosa::GreaterEqualOp> {
+  using TosaOpConversionPattern<tosa::GreaterEqualOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::GreaterEqualOp op, tosa::GreaterEqualOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    Value input2 = adaptor.getInput2();
+    rewriter.replaceOpWithNewOp<spirv::TosaGreaterEqualOp>(
+        op, convertType(op.getType()), input1, input2);
+    return success();
+  }
+};
+
+struct TosaReduceAllConvert final
+    : public TosaOpConversionPattern<tosa::ReduceAllOp> {
+  using TosaOpConversionPattern<tosa::ReduceAllOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::ReduceAllOp op, tosa::ReduceAllOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Type tI32 = rewriter.getIntegerType(32);
+    Value axis = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tI32, adaptor.getAxisAttr());
+    Value input = adaptor.getInput();
+    rewriter.replaceOpWithNewOp<spirv::TosaReduceAllOp>(
+        op, convertType(op.getType()), axis, input);
+    return success();
+  }
+};
+
+struct TosaReduceAnyConvert final
+    : public TosaOpConversionPattern<tosa::ReduceAnyOp> {
+  using TosaOpConversionPattern<tosa::ReduceAnyOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::ReduceAnyOp op, tosa::ReduceAnyOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Type tI32 = rewriter.getIntegerType(32);
+    Value axis = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tI32, adaptor.getAxisAttr());
+    Value input = adaptor.getInput();
+    rewriter.replaceOpWithNewOp<spirv::TosaReduceAnyOp>(
+        op, convertType(op.getType()), axis, input);
+    return success();
+  }
+};
+
+struct TosaReduceMaxConvert final
+    : public TosaOpConversionPattern<tosa::ReduceMaxOp> {
+  using TosaOpConversionPattern<tosa::ReduceMaxOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::ReduceMaxOp op, tosa::ReduceMaxOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Type tI32 = rewriter.getIntegerType(32);
+    Value axis = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tI32, adaptor.getAxisAttr());
+    auto nanModeAttr = rewriter.getIntegerAttr(
+        tI32, convertNanPropagationMode(adaptor.getNanMode()));
+    Value nanMode = rewriter.createOrFold<spirv::ConstantOp>(op.getLoc(), tI32,
+                                                             nanModeAttr);
+    Value input = adaptor.getInput();
+    rewriter.replaceOpWithNewOp<spirv::TosaReduceMaxOp>(
+        op, convertType(op.getType()), axis, nanMode, input);
+    return success();
+  }
+};
+
+struct TosaReduceMinConvert final
+    : public TosaOpConversionPattern<tosa::ReduceMinOp> {
+  using TosaOpConversionPattern<tosa::ReduceMinOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::ReduceMinOp op, tosa::ReduceMinOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Type tI32 = rewriter.getIntegerType(32);
+    Value axis = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tI32, adaptor.getAxisAttr());
+    auto nanModeAttr = rewriter.getIntegerAttr(
+        tI32, convertNanPropagationMode(adaptor.getNanMode()));
+    Value nanMode = rewriter.createOrFold<spirv::ConstantOp>(op.getLoc(), tI32,
+                                                             nanModeAttr);
+    Value input = adaptor.getInput();
+    rewriter.replaceOpWithNewOp<spirv::TosaReduceMinOp>(
+        op, convertType(op.getType()), axis, nanMode, input);
+    return success();
+  }
+};
+
+struct TosaReduceProductConvert final
+    : public TosaOpConversionPattern<tosa::ReduceProductOp> {
+  using TosaOpConversionPattern<tosa::ReduceProductOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::ReduceProductOp op,
+                  tosa::ReduceProductOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Type tI32 = rewriter.getIntegerType(32);
+    Value axis = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tI32, adaptor.getAxisAttr());
+    Value input = adaptor.getInput();
+    rewriter.replaceOpWithNewOp<spirv::TosaReduceProductOp>(
+        op, convertType(op.getType()), axis, input);
+    return success();
+  }
+};
+
+struct TosaReduceSumConvert final
+    : public TosaOpConversionPattern<tosa::ReduceSumOp> {
+  using TosaOpConversionPattern<tosa::ReduceSumOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::ReduceSumOp op, tosa::ReduceSumOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Type tI32 = rewriter.getIntegerType(32);
+    Value axis = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tI32, adaptor.getAxisAttr());
+    Value input = adaptor.getInput();
+    rewriter.replaceOpWithNewOp<spirv::TosaReduceSumOp>(
+        op, convertType(op.getType()), axis, input);
+    return success();
+  }
+};
+
+struct TosaConcatConvert final
+    : public TosaOpConversionPattern<tosa::ConcatOp> {
+  using TosaOpConversionPattern<tosa::ConcatOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::ConcatOp op, tosa::ConcatOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Type tI32 = rewriter.getIntegerType(32);
+    Value axis = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tI32, adaptor.getAxisAttr());
+    ValueRange input1 = adaptor.getInput1();
+    if (input1.getTypes().size() > MAX_NUM_INPUTS) {
+      splitConcat(adaptor.getAxis(), axis, input1, MAX_NUM_INPUTS, rewriter,
+                  op);
+    } else {
+      rewriter.replaceOpWithNewOp<spirv::TosaConcatOp>(
+          op, convertType(op.getType()), axis, input1);
+    }
+    return success();
+  }
+};
+
+struct TosaPadConvert final : public TosaOpConversionPattern<tosa::PadOp> {
+  using TosaOpConversionPattern<tosa::PadOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::PadOp op, tosa::PadOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    Value padding = adaptor.getPadding();
+    Value padConst = adaptor.getPadConst();
+    rewriter.replaceOpWithNewOp<spirv::TosaPadOp>(op, convertType(op.getType()),
+                                                  input1, padding, padConst);
+    return success();
+  }
+};
+
+struct TosaReshapeConvert final
+    : public TosaOpConversionPattern<tosa::ReshapeOp> {
+  using TosaOpConversionPattern<tosa::ReshapeOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::ReshapeOp op, tosa::ReshapeOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    Value shape = adaptor.getShape();
+    rewriter.replaceOpWithNewOp<spirv::TosaReshapeOp>(
+        op, convertType(op.getType()), input1, shape);
+    return success();
+  }
+};
+
+struct TosaReverseConvert final
+    : public TosaOpConversionPattern<tosa::ReverseOp> {
+  using TosaOpConversionPattern<tosa::ReverseOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::ReverseOp op, tosa::ReverseOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Type tI32 = rewriter.getIntegerType(32);
+    Value axis = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tI32, adaptor.getAxisAttr());
+    Value input1 = adaptor.getInput1();
+    rewriter.replaceOpWithNewOp<spirv::TosaReverseOp>(
+        op, convertType(op.getType()), axis, input1);
+    return success();
+  }
+};
+
+struct TosaSliceConvert final : public TosaOpConversionPattern<tosa::SliceOp> {
+  using TosaOpConversionPattern<tosa::SliceOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::SliceOp op, tosa::SliceOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    Value start = adaptor.getStart();
+    Value size = adaptor.getSize();
+    rewriter.replaceOpWithNewOp<spirv::TosaSliceOp>(
+        op, convertType(op.getType()), input1, start, size);
+    return success();
+  }
+};
+
+struct TosaTileConvert final : public TosaOpConversionPattern<tosa::TileOp> {
+  using TosaOpConversionPattern<tosa::TileOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::TileOp op, tosa::TileOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input1 = adaptor.getInput1();
+    Value multiples = adaptor.getMultiples();
+    rewriter.replaceOpWithNewOp<spirv::TosaTileOp>(
+        op, convertType(op.getType()), input1, multiples);
+    return success();
+  }
+};
+
+struct TosaTransposeConvert final
+    : public TosaOpConversionPattern<tosa::TransposeOp> {
+  using TosaOpConversionPattern<tosa::TransposeOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::TransposeOp op, tosa::TransposeOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Type tI32 = rewriter.getIntegerType(32);
+    Type tPerms = spirv::TensorArmType::get(
+        {llvm::cast<mlir::spirv::TensorArmType>(adaptor.getInput1().getType())
+             .getRank()},
+        tI32);
+    auto permsTensorAttr =
+        getI32TensorArmAttr(SmallVector<int32_t>(adaptor.getPerms()), rewriter);
+    Value perms = rewriter.createOrFold<spirv::ConstantOp>(op.getLoc(), tPerms,
+                                                           permsTensorAttr);
+    Value input1 = adaptor.getInput1();
+    rewriter.replaceOpWithNewOp<spirv::TosaTransposeOp>(
+        op, convertType(op.getType()), perms, input1);
+    return success();
+  }
+};
+
+struct TosaGatherConvert final
+    : public TosaOpConversionPattern<tosa::GatherOp> {
+  using TosaOpConversionPattern<tosa::GatherOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::GatherOp op, tosa::GatherOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value values = adaptor.getValues();
+    Value indices = adaptor.getIndices();
+    rewriter.replaceOpWithNewOp<spirv::TosaGatherOp>(
+        op, convertType(op.getType()), values, indices);
+    return success();
+  }
+};
+
+struct TosaScatterConvert final
+    : public TosaOpConversionPattern<tosa::ScatterOp> {
+  using TosaOpConversionPattern<tosa::ScatterOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::ScatterOp op, tosa::ScatterOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value valuesIn = adaptor.getValuesIn();
+    Value indices = adaptor.getIndices();
+    Value input = adaptor.getInput();
+    rewriter.replaceOpWithNewOp<spirv::TosaScatterOp>(
+        op, convertType(op.getType()), valuesIn, indices, input);
+    return success();
+  }
+};
+
+struct TosaResizeConvert final
+    : public TosaOpConversionPattern<tosa::ResizeOp> {
+  using TosaOpConversionPattern<tosa::ResizeOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::ResizeOp op, tosa::ResizeOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Type tI32 = rewriter.getIntegerType(32);
+    auto modeAttr =
+        rewriter.getIntegerAttr(tI32, convertResizeMode(adaptor.getMode()));
+    Value mode =
+        rewriter.createOrFold<spirv::ConstantOp>(op.getLoc(), tI32, modeAttr);
+    Value input = adaptor.getInput();
+    Value scale = adaptor.getScale();
+    Value offset = adaptor.getOffset();
+    Value border = adaptor.getBorder();
+    rewriter.replaceOpWithNewOp<spirv::TosaResizeOp>(
+        op, convertType(op.getType()), mode, input, scale, offset, border);
+    return success();
+  }
+};
+
+struct TosaCastConvert final : public TosaOpConversionPattern<tosa::CastOp> {
+  using TosaOpConversionPattern<tosa::CastOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::CastOp op, tosa::CastOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    Value input = adaptor.getInput();
+    rewriter.replaceOpWithNewOp<spirv::TosaCastOp>(
+        op, convertType(op.getType()), input);
+    return success();
+  }
+};
+
+struct TosaRescaleConvert final
+    : public TosaOpConversionPattern<tosa::RescaleOp> {
+  using TosaOpConversionPattern<tosa::RescaleOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::RescaleOp op, tosa::RescaleOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    Type tI1 = rewriter.getIntegerType(1);
+    Type tI32 = rewriter.getIntegerType(32);
+    Value scale32 = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tI1, adaptor.getScale32Attr());
+    auto roundingModeAttr = rewriter.getIntegerAttr(
+        tI32, convertRoundingMode(adaptor.getRoundingMode()));
+    Value roundingMode = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tI32, roundingModeAttr);
+    Value perChannel = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tI1, adaptor.getPerChannelAttr());
+    Value inputUnsigned = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tI1, adaptor.getInputUnsignedAttr());
+    Value outputUnsigned = rewriter.createOrFold<spirv::ConstantOp>(
+        op.getLoc(), tI1, adaptor.getOutputUnsignedAttr());
+    Value input = adaptor.getInput();
+    Value multiplier = adaptor.getMultiplier();
+    Value shift = adaptor.getShift();
+    Value inputZp = adaptor.getInputZp();
+    Value outputZp = adaptor.getOutputZp();
+    rewriter.replaceOpWithNewOp<spirv::TosaRescaleOp>(
+        op, convertType(op.getType()), scale32, roundingMode, perChannel,
+        inputUnsigned, outputUnsigned, input, multiplier, shift, inputZp,
+        outputZp);
+    return success();
+  }
+};
+
+struct TosaConstConvert final : public TosaOpConversionPattern<tosa::ConstOp> {
+  using TosaOpConversionPattern<tosa::ConstOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::ConstOp op, tosa::ConstOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    if (op->use_empty()) {
+      rewriter.eraseOp(op);
+      return success();
+    }
+
+    auto graphConstId = getGraphIdForConst(op);
+    if (graphConstId.has_value()) {
+      rewriter.replaceOpWithNewOp<spirv::GraphConstantARMOp>(
+          op, convertType(op.getType()), graphConstId.value());
+      return success();
+    }
+
+    ElementsAttr convertedValue;
+    if (maybeConvertValue(op.getLoc(), adaptor.getValues(), convertedValue)
+            .failed()) {
+      return failure();
+    }
+    rewriter.replaceOpWithNewOp<spirv::ConstantOp>(
+        op, convertType(op.getType()), convertedValue);
+    return success();
+  }
+};
+
+struct TosaIdentityConvert final
+    : public TosaOpConversionPattern<tosa::IdentityOp> {
+  using TosaOpConversionPattern<tosa::IdentityOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::IdentityOp op, tosa::IdentityOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+
+    rewriter.replaceOp(op, adaptor.getInput1());
+    return success();
+  }
+};
+
+struct TosaConstShapeConvert final
+    : public TosaOpConversionPattern<tosa::ConstShapeOp> {
+  using TosaOpConversionPattern<tosa::ConstShapeOp>::TosaOpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(tosa::ConstShapeOp op, tosa::ConstShapeOpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override {
+    if (op->use_empty()) {
+      rewriter.eraseOp(op);
+      return success();
+    }
+
+    auto graphConstId = getGraphIdForConst(op);
+    if (graphConstId.has_value()) {
+      rewriter.replaceOpWithNewOp<spirv::GraphConstantARMOp>(
+          op, convertType(op.getType()), graphConstId.value());
+      return success();
+    }
+
+    ElementsAttr convertedValue;
+    if (maybeConvertValue(op.getLoc(), adaptor.getValues(), convertedValue)
+            .failed()) {
+      return failure();
+    }
+    rewriter.replaceOpWithNewOp<spirv::ConstantOp>(
+        op, convertType(op.getType()), convertedValue);
+    return success();
+  }
+};
+
+} // namespace
+
+void mlir::tosa::populateTosaToSPIRVOpsConversionPatterns(
+    SPIRVTypeConverter &typeConverter, RewritePatternSet &patterns) {
+  patterns.add<
+      TosaArgMaxConvert, TosaAvgPool2DConvert, TosaConv2DConvert,
+      TosaConv3DConvert, TosaDepthwiseConv2DConvert, TosaFFT2DConvert,
+      TosaMatMulConvert, TosaMaxPool2DConvert, TosaRFFT2DConvert,
+      TosaTransposeConv2DConvert, TosaClampConvert, TosaErfConvert,
+      TosaSigmoidConvert, TosaTanhConvert, TosaAddConvert,
+      TosaArithmeticRightShiftConvert, TosaBitwiseAndConvert,
+      TosaBitwiseOrConvert, TosaBitwiseXorConvert, TosaIntDivConvert,
+      TosaLogicalAndConvert, TosaLogicalLeftShiftConvert,
+      TosaLogicalRightShiftConvert, TosaLogicalOrConvert, TosaLogicalXorConvert,
+      TosaMaximumConvert, TosaMinimumConvert, TosaMulConvert, TosaPowConvert,
+      TosaSubConvert, TosaTableConvert, TosaAbsConvert, TosaBitwiseNotConvert,
+      TosaCeilConvert, TosaClzConvert, TosaCosConvert, TosaExpConvert,
+      TosaFloorConvert, TosaLogConvert, TosaLogicalNotConvert,
+      TosaNegateConvert, TosaReciprocalConvert, TosaRsqrtConvert,
+      TosaSinConvert, TosaSelectConvert, TosaEqualConvert, TosaGreaterConvert,
+      TosaGreaterEqualConvert, TosaReduceAllConvert, TosaReduceAnyConvert,
+      TosaReduceMaxConvert, TosaReduceMinConvert, TosaReduceProductConvert,
+      TosaReduceSumConvert, TosaConcatConvert, TosaPadConvert,
+      TosaReshapeConvert, TosaReverseConvert, TosaSliceConvert, TosaTileConvert,
+      TosaTransposeConvert, TosaGatherConvert, TosaScatterConvert,
+      TosaResizeConvert, TosaCastConvert, TosaRescaleConvert, TosaConstConvert,
+      TosaIdentityConvert, TosaConstShapeConvert>(typeConverter,
+                                                  patterns.getContext());
+}
diff --git a/mlir/lib/Conversion/TosaToSPIRV/TosaToSPIRVPass.cpp b/mlir/lib/Conversion/TosaToSPIRV/TosaToSPIRVPass.cpp
new file mode 100644
index 000000000000..6ece228571a7
--- /dev/null
+++ b/mlir/lib/Conversion/TosaToSPIRV/TosaToSPIRVPass.cpp
@@ -0,0 +1,174 @@
+//===- TosaToSPIRVPass.cpp - Lowering Tosa to SPIR-V Dialect --------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This transformation pass legalizes Tosa operations to the SPIR-V dialect.
+//
+//===----------------------------------------------------------------------===//
+
+#include "mlir/Conversion/TosaToSPIRV/TosaToSPIRV.h"
+
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/SPIRV/IR/SPIRVDialect.h"
+#include "mlir/Dialect/SPIRV/Transforms/SPIRVConversion.h"
+#include "mlir/Dialect/Tosa/IR/TosaOps.h"
+#include "mlir/Dialect/Tosa/Transforms/Passes.h"
+#include "mlir/IR/TypeUtilities.h"
+#include "mlir/Pass/PassManager.h"
+#include "mlir/Transforms/DialectConversion.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+
+namespace mlir {
+#define GEN_PASS_DEF_TOSATOSPIRV
+#include "mlir/Conversion/Passes.h.inc"
+} // namespace mlir
+
+using namespace mlir;
+using namespace tosa;
+
+namespace {
+
+spirv::VerCapExtAttr getVerCapExtAttr(MLIRContext *context) {
+  return spirv::VerCapExtAttr::get(
+      spirv::Version::V_1_5,
+      {
+          spirv::Capability::VulkanMemoryModel,
+          spirv::Capability::Shader,
+          spirv::Capability::Int8,
+          spirv::Capability::Int16,
+          spirv::Capability::Int64,
+          spirv::Capability::Float16,
+          spirv::Capability::TensorsARM,
+          spirv::Capability::GraphARM,
+          spirv::Capability::ReplicatedCompositesEXT,
+      },
+      {
+          spirv::Extension::SPV_ARM_tensors,
+          spirv::Extension::SPV_ARM_graph,
+          spirv::Extension::SPV_KHR_vulkan_memory_model,
+          spirv::Extension::SPV_EXT_replicated_composites,
+      },
+      context);
+}
+
+struct TosaToSPIRV : public impl::TosaToSPIRVBase<TosaToSPIRV> {
+public:
+  TosaToSPIRV(bool analysis = false) : analysis(analysis) {}
+
+  void runOnOperation() override {
+    MLIRContext *context = &getContext();
+    RewritePatternSet patterns(context);
+    Operation *op = getOperation();
+
+    auto targetAttr = spirv::lookupTargetEnv(op);
+    if (!targetAttr) {
+      targetAttr = spirv::TargetEnvAttr::get(
+          getVerCapExtAttr(context), spirv::getDefaultResourceLimits(context),
+          spirv::ClientAPI::Unknown, spirv::Vendor::Unknown,
+          spirv::DeviceType::Unknown, spirv::TargetEnvAttr::kUnknownDeviceID);
+    }
+
+    std::unique_ptr<ConversionTarget> target =
+        SPIRVConversionTarget::get(targetAttr);
+
+    target->addLegalDialect<spirv::SPIRVDialect>();
+    target->addIllegalDialect<tosa::TosaDialect>();
+
+    SPIRVTypeConverter typeConverter(targetAttr);
+    typeConverter.addConversion([this](IntegerType integerType) {
+      return this->convertIntegerType(integerType);
+    });
+    typeConverter.addConversion([this](TensorType tensorType) {
+      return this->convertTensorType(tensorType);
+    });
+    typeConverter.addConversion([this](tosa::shapeType shapeType) {
+      return this->convertShapeType(shapeType);
+    });
+
+    populateTosaToSPIRVConversionPatterns(typeConverter, patterns);
+    populateTosaToSPIRVOpsConversionPatterns(typeConverter, patterns);
+
+    auto targetEnvAttrName = spirv::getTargetEnvAttrName();
+    auto symbolTableOp = SymbolTable::getNearestSymbolTable(op);
+    if (symbolTableOp && !symbolTableOp->hasAttrOfType<spirv::TargetEnvAttr>(
+                             targetEnvAttrName)) {
+      symbolTableOp->setAttr(targetEnvAttrName, targetAttr);
+    }
+
+    FrozenRewritePatternSet frozenPatterns(std::move(patterns));
+
+    if (analysis) {
+      DenseSet<Operation *> convertedOps;
+      ConversionConfig config;
+      config.legalizableOps = &convertedOps;
+      if (failed(applyAnalysisConversion(op, *target, frozenPatterns, config)))
+        signalPassFailure();
+
+      for (Operation *op : convertedOps) {
+        if (llvm::isa<TosaDialect>(op->getDialect())) {
+          llvm::errs() << "Successfully lowered: " << op->getName() << " at "
+                       << op->getLoc() << "\n";
+        }
+      }
+    }
+
+    if (failed(applyPartialConversion(op, *target, frozenPatterns))) {
+      signalPassFailure();
+    }
+  }
+
+private:
+  bool analysis;
+
+  IntegerType convertIntegerType(IntegerType integerType) {
+    if (integerType.getWidth() == 48) {
+      return IntegerType::get(&getContext(), 64, integerType.getSignedness());
+    }
+
+    if (integerType.getWidth() == 4) {
+      return IntegerType::get(&getContext(), 8, integerType.getSignedness());
+    }
+
+    return integerType;
+  }
+
+  spirv::TensorArmType convertTensorType(TensorType tensorType) {
+    auto tensorShape = tensorType.getShape();
+    bool requiresRankConversion = tensorShape.size() == 0;
+    auto elementType = getElementTypeOrSelf(tensorType);
+    if (elementType.isIndex()) {
+      // special case for converting tensor<0xindex> to !spirv.arm.tensor<1xi32>
+      return spirv::TensorArmType::get(
+          (tensorShape.size() == 1 && tensorShape[0] == 0)
+              ? SmallVector<int64_t>({1})
+              : tensorShape,
+          IntegerType::get(&getContext(), 32));
+    }
+    if (elementType.isInteger(48) || elementType.isInteger(4)) {
+      auto integerType =
+          convertIntegerType(llvm::dyn_cast<IntegerType>(elementType));
+      return spirv::TensorArmType::get(
+          requiresRankConversion ? SmallVector<int64_t>({1}) : tensorShape,
+          integerType);
+    }
+    if (requiresRankConversion) {
+      return spirv::TensorArmType::get(SmallVector<int64_t>({1}), elementType);
+    }
+    return spirv::TensorArmType::get(tensorShape, elementType);
+  }
+
+  spirv::TensorArmType convertShapeType(tosa::shapeType shapeType) {
+    const auto rank = std::max(shapeType.getRank(), 1);
+    return spirv::TensorArmType::get({rank},
+                                     IntegerType::get(&getContext(), 32));
+  }
+};
+} // namespace
+
+std::unique_ptr<Pass> mlir::tosa::createTosaToSPIRV(bool analysis) {
+  return std::make_unique<TosaToSPIRV>(analysis);
+}
diff --git a/mlir/lib/Dialect/SPIRV/IR/SPIRVDialect.cpp b/mlir/lib/Dialect/SPIRV/IR/SPIRVDialect.cpp
index f32c53b8f0b9..8418ad71f3f0 100644
--- a/mlir/lib/Dialect/SPIRV/IR/SPIRVDialect.cpp
+++ b/mlir/lib/Dialect/SPIRV/IR/SPIRVDialect.cpp
@@ -1015,8 +1015,14 @@ LogicalResult SPIRVDialect::verifyRegionArgAttribute(Operation *op,
   return verifyRegionAttribute(op->getLoc(), argType, attribute);
 }
 
-LogicalResult SPIRVDialect::verifyRegionResultAttribute(
-    Operation *op, unsigned /*regionIndex*/, unsigned /*resultIndex*/,
-    NamedAttribute attribute) {
-  return op->emitError("cannot attach SPIR-V attributes to region result");
+LogicalResult SPIRVDialect::verifyRegionResultAttribute(Operation *op,
+                                                        unsigned regionIndex,
+                                                        unsigned resultIndex,
+                                                        NamedAttribute attribute) {
+  auto funcOp = dyn_cast<FunctionOpInterface>(op);
+  if (!funcOp)
+    return op->emitError("cannot attach SPIR-V attributes to region result which is "
+                         "not a FunctionOpInterface type");
+  return verifyRegionAttribute(
+      op->getLoc(), funcOp.getResultTypes()[resultIndex], attribute);
 }
diff --git a/mlir/lib/Dialect/SPIRV/IR/SPIRVOpDefinition.cpp b/mlir/lib/Dialect/SPIRV/IR/SPIRVOpDefinition.cpp
index d8dfe164458e..2f3a28ff1617 100644
--- a/mlir/lib/Dialect/SPIRV/IR/SPIRVOpDefinition.cpp
+++ b/mlir/lib/Dialect/SPIRV/IR/SPIRVOpDefinition.cpp
@@ -31,6 +31,18 @@ static bool isNestedInFunctionOpInterface(Operation *op) {
   return isNestedInFunctionOpInterface(op->getParentOp());
 }
 
+/// Returns true if the given op is a GraphARM op or nested in a
+/// GraphARM op without a module-like op in the middle.
+static bool isNestedInGraphARMOpInterface(Operation *op) {
+  if (!op)
+    return false;
+  if (op->hasTrait<OpTrait::SymbolTable>())
+    return false;
+  if (isa<spirv::GraphARMOp>(op))
+    return true;
+  return isNestedInGraphARMOpInterface(op->getParentOp());
+}
+
 /// Returns true if the given op is an module-like op that maintains a symbol
 /// table.
 static bool isDirectInModuleLikeOp(Operation *op) {
diff --git a/mlir/lib/Dialect/SPIRV/IR/SPIRVOps.cpp b/mlir/lib/Dialect/SPIRV/IR/SPIRVOps.cpp
index 656236246b1a..73d5e1793c05 100644
--- a/mlir/lib/Dialect/SPIRV/IR/SPIRVOps.cpp
+++ b/mlir/lib/Dialect/SPIRV/IR/SPIRVOps.cpp
@@ -767,19 +767,22 @@ void mlir::spirv::AddressOfOp::getAsmResultNames(
 // spirv.EXTConstantCompositeReplicate
 //===----------------------------------------------------------------------===//
 
+static Type getValueType(Attribute attr) {
+  if (auto typedAttr = dyn_cast<TypedAttr>(attr)) {
+    return typedAttr.getType();
+  }
+
+  if (auto arrayAttr = dyn_cast<ArrayAttr>(attr)) {
+    return spirv::ArrayType::get(getValueType(arrayAttr[0]), arrayAttr.size());
+  }
+
+  return nullptr;
+}
+
 LogicalResult spirv::EXTConstantCompositeReplicateOp::verify() {
-  Type valueType;
-  if (auto typedAttr = dyn_cast<TypedAttr>(getValue())) {
-    valueType = typedAttr.getType();
-  } else if (auto arrayAttr = dyn_cast<ArrayAttr>(getValue())) {
-    auto typedElemAttr = dyn_cast<TypedAttr>(arrayAttr[0]);
-    if (!typedElemAttr)
-      return emitError("value attribute is not typed");
-    valueType =
-        spirv::ArrayType::get(typedElemAttr.getType(), arrayAttr.size());
-  } else {
+  Type valueType = getValueType(getValue());
+  if (!valueType)
     return emitError("unknown value attribute type");
-  }
 
   auto compositeType = dyn_cast<spirv::CompositeType>(getType());
   if (!compositeType)
@@ -1120,6 +1123,236 @@ void spirv::FuncOp::build(OpBuilder &builder, OperationState &state,
   state.addRegion();
 }
 
+//===----------------------------------------------------------------------===//
+// spirv.GraphEntryPointARM
+//===----------------------------------------------------------------------===//
+
+void spirv::GraphEntryPointARMOp::build(OpBuilder &builder,
+                                        OperationState &state,
+                                        spirv::GraphARMOp graph,
+                                        ArrayRef<Attribute> interfaceVars) {
+  build(builder, state, SymbolRefAttr::get(graph),
+        builder.getArrayAttr(interfaceVars));
+}
+
+ParseResult spirv::GraphEntryPointARMOp::parse(OpAsmParser &parser,
+                                               OperationState &result) {
+  SmallVector<Type, 0> idTypes;
+  SmallVector<Attribute, 4> interfaceVars;
+
+  FlatSymbolRefAttr fn;
+  if (parser.parseAttribute(fn, Type(), kFnNameAttrName, result.attributes)) {
+    return failure();
+  }
+
+  if (!parser.parseOptionalComma()) {
+    // Parse the interface variables
+    if (parser.parseCommaSeparatedList([&]() -> ParseResult {
+          // The name of the interface variable attribute isnt important
+          FlatSymbolRefAttr var;
+          NamedAttrList attrs;
+          if (parser.parseAttribute(var, Type(), "var_symbol", attrs))
+            return failure();
+          interfaceVars.push_back(var);
+          return success();
+        }))
+      return failure();
+  }
+  result.addAttribute("interface",
+                      parser.getBuilder().getArrayAttr(interfaceVars));
+  return success();
+}
+
+void spirv::GraphEntryPointARMOp::print(OpAsmPrinter &printer) {
+  printer << " ";
+  printer.printSymbolName(getFn());
+  auto interfaceVars = getInterface().getValue();
+  if (!interfaceVars.empty()) {
+    printer << ", ";
+    llvm::interleaveComma(interfaceVars, printer);
+  }
+}
+
+LogicalResult spirv::GraphEntryPointARMOp::verify() {
+  // Checks for fn and interface symbol reference are done in spirv::ModuleOp
+  // verification.
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// spirv.ARM.Graph
+//===----------------------------------------------------------------------===//
+
+ParseResult spirv::GraphARMOp::parse(OpAsmParser &parser,
+                                     OperationState &result) {
+  SmallVector<OpAsmParser::Argument> entryArgs;
+  SmallVector<DictionaryAttr> resultAttrs;
+  SmallVector<Type> resultTypes;
+  auto &builder = parser.getBuilder();
+
+  // Parse the name as a symbol.
+  StringAttr nameAttr;
+  if (parser.parseSymbolName(nameAttr, SymbolTable::getSymbolAttrName(),
+                             result.attributes))
+    return failure();
+
+  // Parse the function signature.
+  bool isVariadic = false;
+  if (function_interface_impl::parseFunctionSignatureWithArguments(
+          parser, /*allowVariadic=*/false, entryArgs, isVariadic, resultTypes,
+          resultAttrs))
+    return failure();
+
+  SmallVector<Type> argTypes;
+  for (auto &arg : entryArgs)
+    argTypes.push_back(arg.type);
+  auto grType = builder.getGraphType(argTypes, resultTypes);
+  result.addAttribute(getFunctionTypeAttrName(result.name),
+                      TypeAttr::get(grType));
+
+  // If additional attributes are present, parse them.
+  if (parser.parseOptionalAttrDictWithKeyword(result.attributes))
+    return failure();
+
+  // Add the attributes to the function arguments.
+  assert(resultAttrs.size() == resultTypes.size());
+  call_interface_impl::addArgAndResultAttrs(
+      builder, result, entryArgs, resultAttrs, getArgAttrsAttrName(result.name),
+      getResAttrsAttrName(result.name));
+
+  // Parse the optional function body.
+  auto *body = result.addRegion();
+  OptionalParseResult parseResult =
+      parser.parseOptionalRegion(*body, entryArgs);
+  return failure(parseResult.has_value() && failed(*parseResult));
+}
+
+void spirv::GraphARMOp::print(OpAsmPrinter &printer) {
+  // Print graph name, signature, and control.
+  printer << " ";
+  printer.printSymbolName(getSymName());
+  auto grType = getFunctionType();
+  function_interface_impl::printFunctionSignature(
+      printer, *this, grType.getInputs(),
+      /*isVariadic=*/false, grType.getResults());
+  function_interface_impl::printFunctionAttributes(printer, *this,
+                                                   {getFunctionTypeAttrName(),
+                                                    getArgAttrsAttrName(),
+                                                    getResAttrsAttrName()});
+
+  // Print the body.
+  Region &body = this->getBody();
+  if (!body.empty()) {
+    printer << ' ';
+    printer.printRegion(body, /*printEntryBlockArgs=*/false,
+                        /*printBlockTerminators=*/true);
+  }
+}
+
+LogicalResult spirv::GraphARMOp::verifyType() {
+  if (getFunctionType().getNumResults() < 1)
+    return emitOpError("there should be at least one result");
+  return success();
+}
+
+LogicalResult spirv::GraphARMOp::verifyBody() {
+  GraphType grType = getFunctionType();
+  if (!isExternal()) {
+    Block &entryBlock = front();
+
+    unsigned numArguments = this->getNumArguments();
+    if (entryBlock.getNumArguments() != numArguments)
+      return emitOpError("entry block must have ")
+             << numArguments << " arguments to match graph signature";
+
+    for (auto [index, grArgType, blockArgType] :
+         llvm::enumerate(getArgumentTypes(), entryBlock.getArgumentTypes())) {
+      if (blockArgType != grArgType) {
+        return emitOpError("type of entry block argument #")
+               << index << '(' << blockArgType
+               << ") must match the type of the corresponding argument in "
+               << "graph signature(" << grArgType << ')';
+      }
+    }
+  }
+
+  auto walkResult = walk([grType](Operation *op) -> WalkResult {
+    if (auto graphOutputsARMOp = dyn_cast<spirv::GraphOutputsARMOp>(op)) {
+      if (grType.getNumResults() != graphOutputsARMOp.getNumOperands())
+        return graphOutputsARMOp.emitOpError("has GraphOutputsARM returning ")
+               << graphOutputsARMOp.getNumOperands()
+               << "value(s) but enclosing graph requires "
+               << grType.getNumResults() << " results";
+
+      auto graphOutputOperandTypes = graphOutputsARMOp.getValue().getType();
+      for (unsigned i = 0; i < graphOutputOperandTypes.size(); ++i) {
+        auto graphOutputOperandType = graphOutputOperandTypes[i];
+        auto grResultType = grType.getResult(i);
+        if (graphOutputOperandType != grResultType)
+          return graphOutputsARMOp.emitError("type of return operand ")
+                 << i << " (" << graphOutputOperandType
+                 << ") doesn't match graph result type (" << grResultType
+                 << ")";
+      }
+    }
+    return WalkResult::advance();
+  });
+
+  return failure(walkResult.wasInterrupted());
+}
+
+void spirv::GraphARMOp::build(OpBuilder &builder, OperationState &state,
+                              StringRef name, GraphType type,
+                              ArrayRef<NamedAttribute> attrs, bool entryPoint) {
+  state.addAttribute(SymbolTable::getSymbolAttrName(),
+                     builder.getStringAttr(name));
+  state.addAttribute(getFunctionTypeAttrName(state.name), TypeAttr::get(type));
+  state.attributes.append(attrs.begin(), attrs.end());
+  state.addAttribute(getEntryPointAttrName(state.name),
+                     builder.getBoolAttr(entryPoint));
+  state.addRegion();
+}
+
+// Returns the argument types of this function.
+ArrayRef<Type> spirv::GraphARMOp::getArgumentTypes() {
+  return getFunctionType().getInputs();
+}
+
+// Returns the result types of this function.
+ArrayRef<Type> spirv::GraphARMOp::getResultTypes() {
+  return getFunctionType().getResults();
+}
+
+// CallableOpInterface
+Region *spirv::GraphARMOp::getCallableRegion() {
+  return isExternal() ? nullptr : &getBody();
+}
+
+//===----------------------------------------------------------------------===//
+// spirv.GraphOutputsARM
+//===----------------------------------------------------------------------===//
+
+LogicalResult spirv::GraphOutputsARMOp::verify() {
+  auto graph = cast<GraphARMOp>((*this)->getParentOp());
+
+  // The operand number and types must match the graph signature.
+  const auto &results = graph.getFunctionType().getResults();
+  if (getNumOperands() != results.size())
+    return emitOpError("has ")
+           << getNumOperands() << " operands, but enclosing graph (@"
+           << graph.getName() << ") returns " << results.size();
+
+  for (unsigned i = 0; i < results.size(); i++)
+    if (getOperand(i).getType() != results[i])
+      return emitError() << "type of return operand " << i << " ("
+                         << getOperand(i).getType()
+                         << ") doesn't match graph result type (" << results[i]
+                         << ")"
+                         << " in graph @" << graph.getName();
+
+  return success();
+}
+
 //===----------------------------------------------------------------------===//
 // spirv.GLFClampOp
 //===----------------------------------------------------------------------===//
@@ -2128,3 +2361,1153 @@ LogicalResult spirv::VectorTimesScalarOp::verify() {
     return emitOpError("scalar operand and result element type match");
   return success();
 }
+
+//===----------------------------------------------------------------------===//
+// TOSA Operator Verifiers.
+//===----------------------------------------------------------------------===//
+
+// Get value attr from spirv::ConstantOp or
+// spirv::EXTConstantCompositeReplicateOp
+template <typename TAttr>
+static LogicalResult getConstAttr(Value value, TAttr &valAttr) {
+  if (auto constOp = value.template getDefiningOp<spirv::ConstantOp>()) {
+    valAttr = dyn_cast<TAttr>(constOp.getValue());
+  }
+
+  if (auto constCompositeReplicateOp =
+          value.template getDefiningOp<
+              spirv::EXTConstantCompositeReplicateOp>()) {
+    auto splatAttr = constCompositeReplicateOp.getValue();
+    auto denseValAttr = SplatElementsAttr::get(
+        cast<ShapedType>(constCompositeReplicateOp.getType()), splatAttr);
+    valAttr = dyn_cast<TAttr>(denseValAttr);
+  }
+
+  return valAttr ? success() : failure();
+}
+
+template <typename T, typename TAdaptor>
+static LogicalResult verifyConvOp(T op, TAdaptor adaptor) {
+  auto inputTy = llvm::dyn_cast<spirv::TensorArmType>(op.getInput().getType());
+  auto weightTy =
+      llvm::dyn_cast<spirv::TensorArmType>(op.getWeight().getType());
+  auto biasTy = llvm::dyn_cast<spirv::TensorArmType>(op.getBias().getType());
+  auto resultTy = llvm::dyn_cast<spirv::TensorArmType>(op.getType());
+
+  if (!inputTy) {
+    return op.emitOpError("expect a ranked tensorARM for input, got ")
+           << op.getInput();
+  }
+
+  if (!weightTy) {
+    return op.emitOpError("expect a ranked tensorARM for weight, got ")
+           << op.getWeight();
+  }
+
+  if (!biasTy) {
+    return op.emitOpError("expect a ranked tensorARM for bias, got ")
+           << op.getWeight();
+  }
+
+  if constexpr (std::is_same_v<T, spirv::TosaConv3DOp>) {
+    if (inputTy.getRank() != 5) {
+      return op.emitOpError("input rank must be 5");
+    }
+
+    if (weightTy.getRank() != 5) {
+      return op.emitOpError("weight rank must be 5");
+    }
+
+    if (resultTy.getRank() != 5) {
+      return op.emitOpError("result rank must be 5");
+    }
+  } else {
+    if (inputTy.getRank() != 4) {
+      return op.emitOpError("input rank must be 4");
+    }
+
+    if (weightTy.getRank() != 4) {
+      return op.emitOpError("weight rank must be 4");
+    }
+
+    if (resultTy.getRank() != 4) {
+      return op.emitOpError("result rank must be 4");
+    }
+  }
+
+  if (biasTy.getRank() != 1) {
+    return op.emitOpError("bias rank must be 1");
+  }
+
+  auto inputETy = inputTy.getElementType();
+  auto weightETy = weightTy.getElementType();
+  auto biasETy = biasTy.getElementType();
+  auto resultETy = resultTy.getElementType();
+
+  if (inputETy.isInteger(8) && !resultETy.isInteger(32)) {
+    return op.emitOpError("expect result type to be i32, got ") << resultETy;
+  }
+
+  if (inputETy.isInteger(16) && !resultETy.isInteger(64)) {
+    return op.emitOpError("expect result type to be i64, got ") << resultETy;
+  }
+
+  if (inputETy.isF16() && !resultETy.isF16()) {
+    return op.emitOpError("expect result type to be f16, got ") << resultETy;
+  }
+
+  if (inputETy.isF32() && !resultETy.isF32()) {
+    return op.emitOpError("expect result type to be f32, got ") << resultETy;
+  }
+
+  if (biasETy != resultETy) {
+    return op.emitOpError("element types of bias and result must be the same");
+  }
+
+  DenseIntOrFPElementsAttr inputZpAttr;
+  if (getConstAttr(adaptor.getInputZp(), inputZpAttr).failed()) {
+    return op.emitOpError(
+        "input_zp must be a tensorARM of an integer/float constant");
+  }
+
+  if (inputZpAttr.size() != 1) {
+    return op.emitOpError("input_zp must have a single element");
+  }
+
+  auto inputZpETy = inputZpAttr.getElementType();
+  if (inputZpETy != inputETy) {
+    return op.emitOpError(
+        "element types of input_zp and input must be the same");
+  }
+
+  DenseIntOrFPElementsAttr weightZpAttr;
+  if (getConstAttr(adaptor.getWeightZp(), weightZpAttr).failed()) {
+    return op.emitOpError(
+        "weight_zp must be a tensorARM of an integer/float constant");
+  }
+
+  if (weightZpAttr.size() != 1) {
+    return op.emitOpError("weight_zp must have a single element");
+  }
+
+  auto weightZpETy = weightZpAttr.getElementType();
+  if (weightZpETy != weightETy) {
+    return op.emitOpError(
+        "element types of weight_zp and weight must be the same");
+  }
+
+  if (llvm::isa<IntegerType>(inputZpETy)) {
+    if ((inputZpETy.getIntOrFloatBitWidth() != 8) &&
+        !inputZpAttr.getValues<APInt>()[0].isZero()) {
+      return op.emitOpError(
+          "input_zp element value must be zero for non-int8 types.");
+    }
+  } else {
+    if (!inputZpAttr.getValues<APFloat>()[0].isZero()) {
+      return op.emitOpError(
+          "input_zp element value must be zero for non-int8 types.");
+    }
+  }
+
+  if (llvm::isa<IntegerType>(weightZpETy)) {
+    if ((weightZpETy.getIntOrFloatBitWidth() != 8) &&
+        !weightZpAttr.getValues<APInt>()[0].isZero()) {
+      return op.emitOpError(
+          "weight_zp element value must be zero for non-int8 types.");
+    }
+  } else {
+    if (!weightZpAttr.getValues<APFloat>()[0].isZero()) {
+      return op.emitOpError(
+          "weight_zp element value must be zero for non-int8 types.");
+    }
+  }
+
+  BoolAttr localBoundAttr;
+  if ((getConstAttr(adaptor.getLocalBound(), localBoundAttr).failed())) {
+    return op.emitOpError("local bound must be a constant boolean");
+  }
+
+  return success();
+}
+
+template <typename T>
+static LogicalResult verifyConvOpModes(T op) {
+
+  IntegerAttr accTypeAttr;
+  if (getConstAttr(op.getAccType(), accTypeAttr).failed()) {
+    return op.emitOpError("accumulator type must be a constant integer");
+  }
+
+  int accType = accTypeAttr.getInt();
+  if (accType != 1 && accType != 2 && accType != 3 && accType != 4) {
+    return op.emitOpError("accumulator type can only have values 1/2/3/4 "
+                          "corresponding to i32/f16/f32/i48");
+  }
+
+  auto inputTy = llvm::dyn_cast<spirv::TensorArmType>(op.getInput().getType());
+  auto inputETy = inputTy.getElementType();
+
+  if (inputETy.isInteger(8) && accType != 1) {
+    return op.emitOpError("accumulator type for i8 tensorARM is not i32");
+  }
+
+  if (inputETy.isInteger(16) && accType != 4) {
+    return op.emitOpError("accumulator type for i16 tensorARM is not i48");
+  }
+
+  if (inputETy.isF16() && !(accType == 2 || accType == 3)) {
+    return op.emitOpError(
+        "accumulator type for f16 tensorARM is not f16 or f32");
+  }
+
+  if (inputETy.isBF16() && accType != 3) {
+    return op.emitOpError("accumulator type for bf16 tensorARM is not f32");
+  }
+
+  if (inputETy.isF32() && accType != 3) {
+    return op.emitOpError("accumulator type for f32 tensorARM is not f32");
+  }
+
+  return success();
+}
+
+// Verify that inType and outType have same element types
+template <typename TOp>
+static LogicalResult verifySameElementTypes(TOp op, Type inType, Type outType) {
+
+  auto inputType = llvm::dyn_cast<ShapedType>(inType);
+  auto outputType = llvm::dyn_cast<ShapedType>(outType);
+
+  if (!inputType) {
+    op.emitOpError("expect shaped tensorARM for input, got ") << inType;
+    return failure();
+  }
+  if (!outputType) {
+    op.emitOpError("expect shaped tensorARM for output, got ") << outType;
+    return failure();
+  }
+  auto inputElementType = inputType.getElementType();
+  auto outputElementType = outputType.getElementType();
+
+  if (inputElementType != outputElementType) {
+    op.emitOpError("expect input and output to have same element type, got ")
+        << inputElementType << " and " << outputElementType;
+    return failure();
+  }
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// spirv.TosaArgmaxOp
+//===----------------------------------------------------------------------===//
+
+LogicalResult spirv::TosaArgMaxOp::verify() {
+  auto inputTy = llvm::cast<ShapedType>(getInput().getType());
+  auto resultTy = llvm::cast<ShapedType>(getType());
+
+  if (resultTy.getRank() !=
+      (inputTy.getRank() > 1 ? inputTy.getRank() - 1 : 1)) {
+    return emitOpError("result rank must be max of 1 and (input rank - 1)");
+  }
+
+  auto resultETy = resultTy.getElementType();
+  if (!resultETy.isIntOrIndex()) {
+    return emitOpError("result is not of integer type");
+  }
+
+  IntegerAttr axisAttr;
+  if (getConstAttr(getAxis(), axisAttr).failed()) {
+    return emitOpError("axis type must be a constant integer");
+  }
+
+  const int axis = axisAttr.getInt();
+  if (inputTy.hasRank() && ((axis < 0) || axis >= inputTy.getRank())) {
+    return emitOpError("specified axis is outside the rank of input");
+  }
+
+  IntegerAttr nanModeAttr;
+  if (getConstAttr(getNanMode(), nanModeAttr).failed()) {
+    return emitOpError("nan_mode type must be a constant integer");
+  }
+
+  int nanMode = nanModeAttr.getInt();
+  if (nanMode != 1 && nanMode != 2) {
+    return emitOpError("nan_mode can only have values 1 and 2 corresponding to "
+                       "PROPAGATE/IGNORE");
+  }
+
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// spirv.TosaAvgPool2DOp
+//===----------------------------------------------------------------------===//
+
+LogicalResult spirv::TosaAvgPool2DOp::verify() {
+  auto inputTy = llvm::cast<ShapedType>(getInput().getType());
+  if (inputTy.getRank() != 4) {
+    return emitOpError("input rank must be 4");
+  }
+
+  auto resultTy = llvm::cast<ShapedType>(getType());
+  if (resultTy.getRank() != 4) {
+    return emitOpError("result rank must be 4");
+  }
+
+  IntegerAttr accTypeAttr;
+  if (getConstAttr(getAccType(), accTypeAttr).failed()) {
+    return emitOpError("accumulator type must be a constant integer");
+  }
+
+  int accType = accTypeAttr.getInt();
+  if (accType != 1 && accType != 2 && accType != 3) {
+    return emitOpError("accumulator type can only have values 1/2/3 "
+                       "corresponding to i32/f16/f32");
+  }
+
+  auto inputETy = inputTy.getElementType();
+  auto resultETy = resultTy.getElementType();
+
+  if (llvm::isa<IntegerType>(inputETy) && accType != 1) {
+    return emitOpError("accumulator type for integer tensorARM is not i32");
+  }
+
+  if (inputETy.isF16() && !(accType == 2 || accType == 3)) {
+    return emitOpError("accumulator type for f16 tensorARM is not f16/f32");
+  }
+
+  if (inputETy.isF32() && accType != 3) {
+    return emitOpError("accumulator type for f32 tensorARM is not f32");
+  }
+
+  if (inputETy != resultETy) {
+    return emitOpError("input and output element types must be the same");
+  }
+
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// spirv.TosaConv2DOp
+//===----------------------------------------------------------------------===//
+
+LogicalResult spirv::TosaConv2DOp::verify() {
+  if (verifyConvOp(*this, TosaConv2DOp::Adaptor(*this)).failed() ||
+      verifyConvOpModes(*this).failed())
+    return failure();
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// spirv.TosaConv3DOp
+//===----------------------------------------------------------------------===//
+
+LogicalResult spirv::TosaConv3DOp::verify() {
+  if (verifyConvOp(*this, TosaConv3DOp::Adaptor(*this)).failed() ||
+      verifyConvOpModes(*this).failed())
+    return failure();
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// SPIRV Tosa DepthwiseConv2D Ops:
+//===----------------------------------------------------------------------===//
+
+LogicalResult spirv::TosaDepthwiseConv2DOp::verify() {
+  if (verifyConvOp(*this, TosaDepthwiseConv2DOp::Adaptor(*this)).failed() ||
+      verifyConvOpModes(*this).failed())
+    return failure();
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// spirv.TosaFFT2DOp
+//===----------------------------------------------------------------------===//
+
+LogicalResult spirv::TosaFFT2DOp::verify() {
+  auto inputRealTy = llvm::dyn_cast<ShapedType>(getInputReal().getType());
+  auto inputImagTy = llvm::dyn_cast<ShapedType>(getInputImag().getType());
+  auto resultTy = llvm::cast<StructType>(getType());
+  auto resultRealTy = llvm::dyn_cast<ShapedType>(resultTy.getElementType(0));
+  auto resultImagTy = llvm::dyn_cast<ShapedType>(resultTy.getElementType(1));
+
+  if (inputRealTy.getRank() != 3) {
+    return emitOpError("real input rank must be 3");
+  }
+
+  if (inputImagTy.getRank() != 3) {
+    return emitOpError("imaginary input rank must be 3");
+  }
+
+  if (resultImagTy.getRank() != 3) {
+    return emitOpError("real result rank must be 3");
+  }
+
+  if (resultImagTy.getRank() != 3) {
+    return emitOpError("imaginary result rank must be 3");
+  }
+
+  if (inputRealTy != inputImagTy || inputRealTy != resultRealTy ||
+      inputImagTy != resultImagTy) {
+    return emitOpError("real input type, imaginary input type, and types of "
+                       "real and imaginary parts of result must be the same");
+  }
+
+  BoolAttr inverseAttr;
+  if ((getConstAttr(getInverse(), inverseAttr).failed())) {
+    return emitOpError("inverse must be a constant boolean");
+  }
+
+  BoolAttr localBoundAttr;
+  if ((getConstAttr(getLocalBound(), localBoundAttr).failed())) {
+    return emitOpError("local bound must be a constant boolean");
+  }
+
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// spirv.TosaMatMulOp
+//===----------------------------------------------------------------------===//
+
+LogicalResult spirv::TosaMatMulOp::verify() {
+  auto aTy = llvm::dyn_cast<ShapedType>(getA().getType());
+  auto bTy = llvm::dyn_cast<ShapedType>(getB().getType());
+  auto resultTy = llvm::cast<ShapedType>(getType());
+
+  if (!aTy || !bTy) {
+    return emitOpError("expected shaped tensors for inputs, got ")
+           << getA().getType() << " and " << getB().getType();
+  }
+
+  if (aTy.getRank() != 3) {
+    return emitOpError("tensorARM A rank must be 3");
+  }
+
+  if (bTy.getRank() != 3) {
+    return emitOpError("tensorARM B rank must be 3");
+  }
+
+  if (resultTy.getRank() != 3) {
+    return emitOpError("result rank must be 3");
+  }
+
+  auto aETy = aTy.getElementType();
+  auto bETy = bTy.getElementType();
+  auto resultETy = resultTy.getElementType();
+
+  if (aETy != bETy) {
+    return emitOpError("expect same element type for inputs a and b, got ")
+           << aETy << " and " << bETy;
+  }
+
+  if (aETy.isInteger(8) && !resultETy.isInteger(32)) {
+    return emitOpError("expect result element type to be i32, got ")
+           << resultETy;
+  }
+
+  if (aETy.isInteger(16) && !resultETy.isInteger(64)) {
+    return emitOpError("expect result element type to be i64, got ")
+           << resultETy;
+  }
+
+  if (aETy.isF16() && !(resultETy.isF16() || resultETy.isF32())) {
+    return emitOpError("expect result element type to be f16 or f32, got ")
+           << resultETy;
+  }
+
+  if (aETy.isF32() && !resultETy.isF32()) {
+    return emitOpError("expect result element type to be f32, got ")
+           << resultETy;
+  }
+
+  DenseIntOrFPElementsAttr aZpAttr;
+  if (getConstAttr(getAZp(), aZpAttr).failed()) {
+    return emitOpError("a_zp must be a tensorARM of an integer/float constant");
+  }
+
+  if (aZpAttr.size() != 1) {
+    return emitOpError("a_zp must have a single element");
+  }
+
+  DenseIntOrFPElementsAttr bZpAttr;
+  if (getConstAttr(getBZp(), bZpAttr).failed()) {
+    return emitOpError("b_zp must be a tensorARM of an integer/float constant");
+  }
+
+  if (bZpAttr.size() != 1) {
+    return emitOpError("b_zp must have a single element");
+  }
+
+  if (llvm::isa<IntegerType>(aETy)) {
+    if ((aETy.getIntOrFloatBitWidth() != 8) &&
+        (!aZpAttr.getValues<APInt>()[0].isZero() ||
+         !bZpAttr.getValues<APInt>()[0].isZero())) {
+      return emitOpError("a_zp and b_zp must be zero for non-int8 types.");
+    }
+  } else {
+    if (!aZpAttr.getValues<APFloat>()[0].isZero() ||
+        !bZpAttr.getValues<APFloat>()[0].isZero()) {
+      return emitOpError("a_zp and b_zp must be zero for non-int8 types.");
+    }
+  }
+
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// spirv.TosaMaxPool2DOp
+//===----------------------------------------------------------------------===//
+
+LogicalResult spirv::TosaMaxPool2DOp::verify() {
+  auto inputTy = llvm::dyn_cast<ShapedType>(getInput().getType());
+  if (inputTy.getRank() != 4) {
+    return emitOpError("input rank must be 4");
+  }
+
+  auto resultTy = llvm::cast<ShapedType>(getType());
+  if (resultTy.getRank() != 4) {
+    return emitOpError("result rank must be 4");
+  }
+
+  IntegerAttr nanModeAttr;
+  if (getConstAttr(getNanMode(), nanModeAttr).failed()) {
+    return emitOpError("nan_mode type must be a constant integer");
+  }
+
+  int nanMode = nanModeAttr.getInt();
+  if (nanMode != 1 && nanMode != 2) {
+    return emitOpError("nan_mode can only have values 1 and 2 corresponding to "
+                       "PROPAGATE/IGNORE");
+  }
+
+  return verifySameElementTypes(*this, getInput().getType(),
+                                getOutput().getType());
+}
+
+//===----------------------------------------------------------------------===//
+// spirv.TosaRFFT2DOp
+//===----------------------------------------------------------------------===//
+
+LogicalResult spirv::TosaRFFT2DOp::verify() {
+  auto inputTy = llvm::dyn_cast<ShapedType>(getInputReal().getType());
+  auto resultTy = llvm::cast<StructType>(getType());
+  auto resultRealTy = llvm::dyn_cast<ShapedType>(resultTy.getElementType(0));
+  auto resultImagTy = llvm::dyn_cast<ShapedType>(resultTy.getElementType(1));
+
+  if (inputTy.getRank() != 3) {
+    return emitOpError("input rank must be 3");
+  }
+
+  if (resultImagTy.getRank() != 3) {
+    return emitOpError("real result rank must be 3");
+  }
+
+  if (resultImagTy.getRank() != 3) {
+    return emitOpError("imaginary result rank must be 3");
+  }
+
+  if (inputTy.getElementType() != resultRealTy.getElementType() ||
+      inputTy.getElementType() != resultImagTy.getElementType()) {
+    return emitOpError(
+        "input element type and element types of real and imaginary parts of "
+        "result must be the same");
+  }
+
+  BoolAttr localBoundAttr;
+  if ((getConstAttr(getLocalBound(), localBoundAttr).failed())) {
+    return emitOpError("local bound must be a constant boolean");
+  }
+
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// SPIRV Tosa TransposeConv2D Ops:
+//===----------------------------------------------------------------------===//
+
+LogicalResult spirv::TosaTransposeConv2DOp::verify() {
+  if (verifyConvOp(*this, TosaTransposeConv2DOp::Adaptor(*this)).failed() ||
+      verifyConvOpModes(*this).failed())
+    return failure();
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// spirv.TosaMulOp
+//===----------------------------------------------------------------------===//
+LogicalResult spirv::TosaMulOp::verify() {
+  auto resElemType = getElementTypeOrSelf(getOutput());
+
+  // Verify if the element type amoung operands and result match tosa
+  // specification.
+  if (auto resIntType = llvm::dyn_cast<IntegerType>(resElemType)) {
+    IntegerType lhsIntType =
+        llvm::cast<IntegerType>(getElementTypeOrSelf(getInput1()));
+    IntegerType rhsIntType =
+        llvm::cast<IntegerType>(getElementTypeOrSelf(getInput2()));
+    if (lhsIntType != rhsIntType)
+      return emitOpError(
+          "requires the same element type for all input operands");
+
+    // Though the spec requires the element type of result to be i32, a more
+    // relaxed way is provided at dialect level for easier cooperating with
+    // other dialects.
+    if (lhsIntType.getWidth() > resIntType.getWidth())
+      return emitOpError("invalid data type size for operands or result");
+  } else {
+    // For other supported type, the spec requires requires the same element
+    // type for all operands (excludes `shift` operand) and results.
+    for (int i = 0; i < 2; ++i) {
+      if (getElementTypeOrSelf(getOperand(i)) != resElemType)
+        return emitOpError(
+            "requires the same element type for all operands and results");
+    }
+  }
+
+  auto getRank = [](const Type type) {
+    return llvm::cast<ShapedType>(type).getRank();
+  };
+  for (int i = 0; i < 2; ++i) {
+    if (getRank(getOperand(i).getType()) != getRank(getType()))
+      return emitOpError("result type has different rank than operands");
+  }
+  if (getRank(getOperand(0).getType()) != getRank(getOperand(1).getType())) {
+    return emitOpError("operands don't have matching ranks");
+  }
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// spirv.TosaCastOp
+//===----------------------------------------------------------------------===//
+LogicalResult spirv::TosaCastOp::verify() {
+  auto inputType = llvm::cast<ShapedType>(getInput().getType());
+  auto inputETy = inputType.getElementType();
+  auto outputETy = llvm::cast<ShapedType>(getType()).getElementType();
+
+  // input element type: bool
+  if (inputETy.isInteger(1)) {
+    if (outputETy.isInteger(8) || outputETy.isInteger(16) ||
+        outputETy.isInteger(32)) {
+      return success();
+    }
+  }
+  // input element type: int8
+  if (inputETy.isInteger(8)) {
+    if (outputETy.isInteger(1) || outputETy.isInteger(16) ||
+        outputETy.isInteger(32) || outputETy.isF16() || outputETy.isBF16() ||
+        outputETy.isF32()) {
+      return success();
+    }
+  }
+  // input element type: int16
+  if (inputETy.isInteger(16)) {
+    if (outputETy.isInteger(1) || outputETy.isInteger(8) ||
+        outputETy.isInteger(32) || outputETy.isF16() || outputETy.isBF16() ||
+        outputETy.isF32()) {
+      return success();
+    }
+  }
+  // input element type: int32
+  if (inputETy.isInteger(32)) {
+    if (outputETy.isInteger(1) || outputETy.isInteger(8) ||
+        outputETy.isInteger(16) || outputETy.isF16() || outputETy.isBF16() ||
+        outputETy.isF32()) {
+      return success();
+    }
+  }
+  // input element type: bf16 or fp16
+  if (inputETy.isBF16() || inputETy.isF16()) {
+    if (outputETy.isInteger(8) || outputETy.isInteger(16) ||
+        outputETy.isInteger(32) || outputETy.isF32()) {
+      return success();
+    }
+  }
+  // input element type: fp32
+  if (inputETy.isF32()) {
+    if (outputETy.isInteger(8) || outputETy.isInteger(16) ||
+        outputETy.isInteger(32) || outputETy.isF16() || outputETy.isBF16()) {
+      return success();
+    }
+  }
+
+  return emitOpError("input/output element types are incompatible: ")
+         << inputETy << " and " << outputETy;
+}
+
+//===----------------------------------------------------------------------===//
+// spirv.TosaClampOp
+//===----------------------------------------------------------------------===//
+LogicalResult spirv::TosaClampOp::verify() {
+  auto inputETy = llvm::cast<ShapedType>(getInput().getType()).getElementType();
+  auto outputETy =
+      llvm::cast<ShapedType>(getOutput().getType()).getElementType();
+
+  if (inputETy != outputETy)
+    return emitOpError("input/output element types are incompatible");
+
+  unsigned dataTypeBitWidth = inputETy.getIntOrFloatBitWidth();
+
+  if (inputETy.isInteger(dataTypeBitWidth)) {
+    IntegerAttr minValAttr, maxValAttr;
+    if ((getConstAttr(getMinVal(), minValAttr).failed()) ||
+        (getConstAttr(getMaxVal(), maxValAttr).failed()) ||
+        (minValAttr.getType() != maxValAttr.getType()) ||
+        (minValAttr.getType() != inputETy))
+
+      return emitOpError("min/max attributes types are incompatible with "
+                         "input/output element types.");
+  } else {
+    FloatAttr minValAttr, maxValAttr;
+    if ((getConstAttr(getMinVal(), minValAttr).failed()) ||
+        (getConstAttr(getMaxVal(), maxValAttr).failed()) ||
+        (minValAttr.getType() != maxValAttr.getType()) ||
+        (minValAttr.getType() != inputETy))
+
+      return emitOpError("min/max attributes types are incompatible with "
+                         "input/output element types.");
+  }
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// spirv.TosaConcatOp
+//===----------------------------------------------------------------------===//
+LogicalResult spirv::TosaConcatOp::verify() {
+  auto outType = getOutput().getType();
+  for (auto input : getInput1()) {
+    if (verifySameElementTypes(*this, input.getType(), outType).failed()) {
+      return failure();
+    }
+  }
+  IntegerAttr axisAttr;
+  if (getConstAttr(getAxis(), axisAttr).failed()) {
+    return emitOpError("Axis must be an integer constant");
+  }
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// spirv.TosaPadOp
+//===----------------------------------------------------------------------===//
+LogicalResult spirv::TosaPadOp::verify() {
+  if (verifySameElementTypes(*this, getInput1().getType(),
+                             getOutput().getType())
+          .failed()) {
+    return failure();
+  }
+
+  auto inputETy =
+      llvm::cast<ShapedType>(getInput1().getType()).getElementType();
+
+  DenseIntOrFPElementsAttr padConstAttr;
+  if ((getConstAttr(getPadConst(), padConstAttr).failed()) ||
+      (padConstAttr.getElementType() != inputETy)) {
+    return emitOpError(
+        "PadConst element type is not same as input element type.");
+  }
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// spirv.TosaSliceOp
+//===----------------------------------------------------------------------===//
+LogicalResult spirv::TosaSliceOp::verify() {
+  if (verifySameElementTypes(*this, getInput1().getType(),
+                             getOutput().getType())
+          .failed()) {
+    return failure();
+  }
+
+  auto input_rank = llvm::cast<ShapedType>(getInput1().getType()).getRank();
+  auto start_shape_rank =
+      llvm::cast<ShapedType>(getStart().getType()).getShape()[0];
+
+  if (input_rank != start_shape_rank) {
+    return emitOpError("length of start is not equal to rank of input shape");
+  }
+
+  auto size_shape_rank =
+      llvm::cast<ShapedType>(getSize().getType()).getShape()[0];
+
+  if (input_rank != size_shape_rank) {
+    return emitOpError("length of size is not equal to rank of input shape");
+  }
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// spirv.TosaTileOp
+//===----------------------------------------------------------------------===//
+LogicalResult spirv::TosaTileOp::verify() {
+
+  auto input_rank = llvm::cast<ShapedType>(getInput1().getType()).getRank();
+  auto output_rank = llvm::cast<ShapedType>(getOutput().getType()).getRank();
+  auto multiples_shape_rank =
+      llvm::cast<ShapedType>(getMultiples().getType()).getShape()[0];
+
+  if (verifySameElementTypes(*this, getInput1().getType(),
+                             getOutput().getType())
+          .failed()) {
+    return failure();
+  }
+  if (input_rank != output_rank) {
+    return emitOpError("expect same input and output tensorARM rank");
+  }
+  if (input_rank != multiples_shape_rank) {
+    return emitOpError("expect 'multiples' array to have length ")
+           << input_rank << " but got " << multiples_shape_rank;
+  }
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// spirv.TosaTransposeOp
+//===----------------------------------------------------------------------===//
+LogicalResult spirv::TosaTransposeOp::verify() {
+  if (verifySameElementTypes(*this, getInput1().getType(),
+                             getOutput().getType())
+          .failed()) {
+    return failure();
+  }
+
+  auto input_rank = llvm::cast<ShapedType>(getInput1().getType()).getRank();
+  auto output_rank = llvm::cast<ShapedType>(getOutput().getType()).getRank();
+  auto perms_rank = llvm::cast<ShapedType>(getPerms().getType()).getRank();
+
+  if (input_rank != output_rank) {
+    return emitOpError("expect same input and output tensorARM rank");
+  }
+  if (perms_rank != 1) {
+    return emitOpError(
+               "expected permutation tensorARM to be rank 1 but got rank ")
+           << perms_rank;
+  }
+  auto perms_length =
+      llvm::cast<ShapedType>(getPerms().getType()).getShape()[0];
+
+  if (input_rank != perms_length) {
+    return emitOpError("expect permutation tensorARM to have length ")
+           << input_rank << " but got " << perms_length;
+  }
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// spirv.TosaGatherOp
+//===----------------------------------------------------------------------===//
+LogicalResult spirv::TosaGatherOp::verify() {
+  return verifySameElementTypes(*this, getValues().getType(),
+                                getOutput().getType());
+}
+
+//===----------------------------------------------------------------------===//
+// spirv.TosaScatterOp
+//===----------------------------------------------------------------------===//
+LogicalResult spirv::TosaScatterOp::verify() {
+  if (verifySameElementTypes(*this, getValuesIn().getType(),
+                             getValuesOut().getType())
+          .failed()) {
+    return failure();
+  }
+  if (verifySameElementTypes(*this, getInput().getType(),
+                             getValuesOut().getType())
+          .failed()) {
+    return failure();
+  }
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// spirv.TosaRescaleOp
+//===----------------------------------------------------------------------===//
+
+LogicalResult spirv::TosaRescaleOp::verify() {
+  auto inputTy = llvm::dyn_cast<ShapedType>(getInput().getType());
+  if (!inputTy) {
+    return emitOpError("expect shaped tensorARM for input, got ")
+           << getInput().getType();
+  }
+
+  auto inputETy = inputTy.getElementType();
+  if (!llvm::isa<IntegerType>(inputETy)) {
+    return emitOpError("expect input to have integer element type, got ")
+           << inputETy;
+  }
+
+  auto outputTy = llvm::dyn_cast<ShapedType>(getOutput().getType());
+  if (!outputTy) {
+    return emitOpError("expect shaped tensorARM for output, got ")
+           << getOutput().getType();
+  }
+
+  if (inputTy.getShape() != outputTy.getShape()) {
+    return emitOpError("Shape of input and output must be same");
+  }
+
+  auto outputETy = outputTy.getElementType();
+  if (!llvm::isa<IntegerType>(outputETy)) {
+    return emitOpError("expect output to have integer element type, got ")
+           << outputETy;
+  }
+
+  DenseIntElementsAttr inputZpAttr;
+  if ((getConstAttr(getInputZp(), inputZpAttr).failed())) {
+    return emitOpError(
+        "input_zp must be single element tensorARM of an integer constant");
+  }
+
+  if (inputZpAttr.size() != 1) {
+    return emitOpError("input_zp must have a single element");
+  }
+
+  auto inputZPETy = inputZpAttr.getElementType();
+  if (inputZPETy != inputETy) {
+    return emitOpError(
+        "input_zp element type is not same as input element type");
+  }
+
+  if (auto inputAPInt = inputZpAttr.getValues<APInt>()[0];
+      !inputAPInt.isZero()) {
+    if (!inputETy.isInteger(8) &&
+        !(inputETy.isInteger(16) && getInputUnsigned())) {
+      return emitOpError("expect input_zp of 0, got ")
+             << inputAPInt.getZExtValue();
+    }
+    if (inputETy.isInteger(16) && getInputUnsigned()) {
+      if (auto input_zp = inputAPInt.getZExtValue(); input_zp != 32768u) {
+        return emitOpError("expect input_zp of 0 or 32768 for unsigned int16 "
+                           "input, got ")
+               << input_zp;
+      }
+    }
+  }
+
+  DenseIntElementsAttr outputZpAttr;
+  if ((getConstAttr(getOutputZp(), outputZpAttr).failed())) {
+    return emitOpError(
+        "output_zp must be single element tensorARM of an integer constant");
+  }
+
+  if (outputZpAttr.size() != 1) {
+    return emitOpError("output_zp must have a single element");
+  }
+
+  auto outputZPETy = outputZpAttr.getElementType();
+  if (outputZPETy != outputETy) {
+    return emitOpError(
+        "output_zp element type is not same as output element type");
+  }
+
+  if (auto outputAPInt = outputZpAttr.getValues<APInt>()[0];
+      !outputAPInt.isZero()) {
+    if (!outputETy.isInteger(8) &&
+        !(outputETy.isInteger(16) && getOutputUnsigned())) {
+      return emitOpError("expect output_zp of 0, got ")
+             << outputAPInt.getZExtValue();
+    }
+    if (outputETy.isInteger(16) && getOutputUnsigned()) {
+      if (auto output_zp = outputAPInt.getZExtValue(); output_zp != 32768u) {
+        return emitOpError("expect output_zp of 0 or 32768 for unsigned int16 "
+                           "output, got ")
+               << output_zp;
+      }
+    }
+  }
+
+  auto shiftTy = llvm::dyn_cast<ShapedType>(getShift().getType());
+  if (!shiftTy) {
+    return emitOpError("expect shaped tensorARM for shift, got ")
+           << getShift().getType();
+  }
+
+  auto shiftETy = shiftTy.getElementType();
+  if (!shiftETy.isInteger(8)) {
+    return emitOpError("shift element type must be i8");
+  }
+
+  BoolAttr scale32Attr;
+  if ((getConstAttr(getScale32(), scale32Attr).failed())) {
+    return emitOpError("scale32 must be a constant boolean");
+  }
+
+  auto multiplierTy = llvm::dyn_cast<ShapedType>(getMultiplier().getType());
+  if (!multiplierTy) {
+    return emitOpError("expect shaped tensorARM for multiplier, got ")
+           << getMultiplier().getType();
+  }
+
+  auto multiplierETy = multiplierTy.getElementType();
+  if (scale32Attr.getValue() && !multiplierETy.isInteger(32)) {
+    return emitOpError(
+               "expect i32 element type for multiplier for scale32=true, got ")
+           << multiplierETy;
+  }
+
+  if (!scale32Attr.getValue() && !multiplierETy.isInteger(16)) {
+    return emitOpError(
+               "expect i16 element type for multiplier for scale32=false, got ")
+           << multiplierETy;
+  }
+
+  IntegerAttr roundingModeAttr;
+  if ((getConstAttr(getRoundingMode(), roundingModeAttr).failed())) {
+    return emitOpError("rounding_mode must be a constant integer");
+  }
+
+  if (auto roundingMode = roundingModeAttr.getInt();
+      (roundingMode != 1 && roundingMode != 2 && roundingMode != 3)) {
+    return emitOpError(
+               "rounding mode must be an integer of value 1/2/3 "
+               "corresponding to SINGLE_ROUND/INEXACT_ROUND/DOUBLE_ROUND, got ")
+           << roundingMode;
+  }
+
+  BoolAttr perChannelAttr;
+  if ((getConstAttr(getPerChannel(), perChannelAttr).failed())) {
+    return emitOpError("per_channel must be a constant boolean");
+  }
+
+  // multiplier/shift must have shape = {numChannels},
+  // where numChannel is 1 if per_channel = false
+  // otherwise numChannel is dimension in input shape's last axis
+  int64_t numChannels = 1;
+  if (perChannelAttr.getValue()) {
+    ArrayRef<int64_t> inputShape = inputTy.getShape();
+    numChannels = inputShape[inputShape.size() - 1];
+  }
+
+  ArrayRef<int64_t> multiplierShape = multiplierTy.getShape();
+  if (multiplierShape[0] != numChannels) {
+    return emitOpError("expect shape of { ")
+           << numChannels << " } for multiplier input, got { "
+           << multiplierShape[0] << " }";
+  }
+
+  ArrayRef<int64_t> shiftShape = shiftTy.getShape();
+  if (shiftShape[0] != numChannels) {
+    return emitOpError("expect shape of { ")
+           << numChannels << " } for shift input, got { " << shiftShape[0]
+           << " }";
+  }
+
+  BoolAttr inputUnsignedAttr;
+  if ((getConstAttr(getInputUnsigned(), inputUnsignedAttr).failed())) {
+    return emitOpError("input_unsigned must be a constant boolean");
+  }
+
+  BoolAttr outputUnsignedAttr;
+  if ((getConstAttr(getOutputUnsigned(), outputUnsignedAttr).failed())) {
+    return emitOpError("output_unsigned must be a constant boolean");
+  }
+
+  if (inputETy.isInteger(8) || inputETy.isInteger(16) ||
+      inputETy.isInteger(32) || inputETy.isInteger(64)) {
+    if (outputETy.isInteger(8) || outputETy.isInteger(16) ||
+        outputETy.isInteger(32)) {
+      return success();
+    }
+  }
+
+  return emitOpError("input/output element types are incompatible: ")
+         << inputETy << " and " << outputETy;
+}
+
+//===----------------------------------------------------------------------===//
+// spirv.TosaReverseOp
+//===----------------------------------------------------------------------===//
+
+LogicalResult spirv::TosaReverseOp::verify() {
+  if (verifySameElementTypes(*this, getInput1().getType(),
+                             getOutput().getType())
+          .failed()) {
+    return failure();
+  }
+
+  auto inputRank = llvm::cast<ShapedType>(getInput1().getType()).getRank();
+  auto outputRank = llvm::cast<ShapedType>(getOutput().getType()).getRank();
+
+  if (inputRank != outputRank) {
+    return emitOpError(
+        "expect output tensorARM rank to be equal to input rank");
+  }
+
+  IntegerAttr axisAttr;
+  if ((getConstAttr(getAxis(), axisAttr).failed())) {
+    return emitOpError("axis must be a constant integer");
+  }
+
+  int32_t reverseAxis = axisAttr.getInt();
+  if (reverseAxis < 0) {
+    return emitOpError("expected non-negative reverse axis");
+  }
+
+  if (reverseAxis >= inputRank && !(reverseAxis == 0 && inputRank == 0)) {
+    return emitOpError("expect input rank (")
+           << inputRank << ") to be larger than reverse axis (" << reverseAxis
+           << ")";
+  }
+
+  if (reverseAxis >= outputRank && !(reverseAxis == 0 && outputRank == 0)) {
+    return emitOpError("expect output tensorARM rank (")
+           << outputRank << ") to be larger than reverse axis (" << reverseAxis
+           << ")";
+  }
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// spirv.TosaSelectOp
+//===----------------------------------------------------------------------===//
+LogicalResult spirv::TosaSelectOp::verify() {
+  if (verifySameElementTypes(*this, getInput2().getType(),
+                             getOutput().getType())
+          .failed()) {
+    return failure();
+  }
+  if (verifySameElementTypes(*this, getInput3().getType(),
+                             getOutput().getType())
+          .failed()) {
+    return failure();
+  }
+
+  auto predicateType = llvm::dyn_cast<ShapedType>(getInput1().getType());
+  if (!predicateType) {
+    emitOpError("expect shaped tensorARM for input1, got ")
+        << getInput1().getType();
+    return failure();
+  }
+
+  auto predicateElementType = predicateType.getElementType();
+  if (!predicateElementType.isInteger(1)) {
+    emitOpError("expect element type of bool for input1, got ")
+        << predicateElementType;
+    return failure();
+  }
+  return success();
+}
+
+//===----------------------------------------------------------------------===//
+// spirv.TosaReshapeOp
+//===----------------------------------------------------------------------===//
+
+mlir::LogicalResult spirv::TosaReshapeOp::verify() {
+  if (verifySameElementTypes(*this, getInput1().getType(),
+                             getOutput().getType())
+          .failed()) {
+    return failure();
+  }
+  ShapedType inputType = llvm::cast<ShapedType>(getInput1().getType());
+  ShapedType outputType = llvm::cast<ShapedType>(getType());
+
+  if (inputType.hasStaticShape() && outputType.hasStaticShape()) {
+    int64_t inputElementsNum = inputType.getNumElements();
+    int64_t outputElementsNum = outputType.getNumElements();
+    if (inputElementsNum != outputElementsNum) {
+      return emitOpError() << "Cannot reshape " << inputElementsNum
+                           << " elements into " << outputElementsNum;
+    }
+  }
+  return mlir::success();
+}
diff --git a/mlir/lib/Dialect/SPIRV/Transforms/LowerABIAttributesPass.cpp b/mlir/lib/Dialect/SPIRV/Transforms/LowerABIAttributesPass.cpp
index 85525a5a02fa..57e27188e788 100644
--- a/mlir/lib/Dialect/SPIRV/Transforms/LowerABIAttributesPass.cpp
+++ b/mlir/lib/Dialect/SPIRV/Transforms/LowerABIAttributesPass.cpp
@@ -75,10 +75,36 @@ createGlobalVarForEntryPointArgument(OpBuilder &builder, spirv::FuncOp funcOp,
       abiInfo.getBinding());
 }
 
+/// Creates a global variable for an argument or result based on the ABI info.
+static spirv::GlobalVariableOp
+createGlobalVarForGraphEntryPoint(OpBuilder &builder, spirv::GraphARMOp graphOp,
+                                  unsigned index, bool isArg,
+                                  spirv::InterfaceVarABIAttr abiInfo) {
+  auto spirvModule = graphOp->getParentOfType<spirv::ModuleOp>();
+  if (!spirvModule)
+    return nullptr;
+
+  OpBuilder::InsertionGuard moduleInsertionGuard(builder);
+  builder.setInsertionPoint(graphOp.getOperation());
+  std::string varName = graphOp.getName().str() + (isArg ? "_arg_" : "_res_") +
+                        std::to_string(index);
+
+  auto varType = isArg ? graphOp.getFunctionType().getInput(index)
+                       : graphOp.getFunctionType().getResult(index);
+
+  auto pointerType = spirv::PointerType::get(
+      varType,
+      abiInfo.getStorageClass().value_or(spirv::StorageClass::UniformConstant));
+
+  return builder.create<spirv::GlobalVariableOp>(
+      graphOp.getLoc(), pointerType, varName, abiInfo.getDescriptorSet(),
+      abiInfo.getBinding());
+}
+
 /// Gets the global variables that need to be specified as interface variable
 /// with an spirv.EntryPointOp. Traverses the body of a entry function to do so.
 static LogicalResult
-getInterfaceVariables(spirv::FuncOp funcOp,
+getInterfaceVariables(mlir::FunctionOpInterface funcOp,
                       SmallVectorImpl<Attribute> &interfaceVars) {
   auto module = funcOp->getParentOfType<spirv::ModuleOp>();
   if (!module) {
@@ -214,6 +240,21 @@ public:
                   ConversionPatternRewriter &rewriter) const override;
 };
 
+/// A pattern to convert graph signature according to interface variable ABI
+/// attributes.
+///
+/// Specifically, this pattern creates global variables according to interface
+/// variable ABI attributes attached to graph arguments and results.
+class ProcessGraphInterfaceVarABI final
+    : public OpConversionPattern<spirv::GraphARMOp> {
+public:
+  using OpConversionPattern<spirv::GraphARMOp>::OpConversionPattern;
+
+  LogicalResult
+  matchAndRewrite(spirv::GraphARMOp graphOp, OpAdaptor adaptor,
+                  ConversionPatternRewriter &rewriter) const override;
+};
+
 /// Pass to implement the ABI information specified as attributes.
 class LowerABIAttributesPass final
     : public spirv::impl::SPIRVLowerABIAttributesPassBase<
@@ -287,6 +328,89 @@ LogicalResult ProcessInterfaceVarABI::matchAndRewrite(
   return success();
 }
 
+namespace {
+
+/// Lowers the graph entry point
+LogicalResult lowerGraphEntryPoint(OpBuilder &builder,
+                                   spirv::GraphARMOp graphOp,
+                                   ArrayRef<Attribute> interfaceVars) {
+  if (!graphOp.getEntryPoint().value_or(false)) {
+    return failure();
+  }
+
+  OpBuilder::InsertionGuard insertionGuard(builder);
+  builder.setInsertionPoint(graphOp);
+  builder.create<spirv::GraphEntryPointARMOp>(graphOp.getLoc(), graphOp,
+                                              interfaceVars);
+  return success();
+}
+} // namespace
+
+LogicalResult ProcessGraphInterfaceVarABI::matchAndRewrite(
+    spirv::GraphARMOp graphOp, OpAdaptor adaptor,
+    ConversionPatternRewriter &rewriter) const {
+  if (!graphOp.getEntryPoint().value_or(false)) {
+    // Non-entry point graphs are not handled.
+    return failure();
+  }
+  TypeConverter::SignatureConversion signatureConverter(
+      graphOp.getFunctionType().getNumInputs());
+
+  auto attrName = spirv::getInterfaceVarABIAttrName();
+
+  SmallVector<Attribute, 2> interfaceVars;
+
+  // Convert arguments
+  for (const auto &argType :
+       llvm::enumerate(graphOp.getFunctionType().getInputs())) {
+    auto abiInfo = graphOp.getArgAttrOfType<spirv::InterfaceVarABIAttr>(
+        argType.index(), attrName);
+    if (!abiInfo) {
+      // Non-entry point graphs are not handled in this ABI lowering and will
+      // produce an error.
+      return failure();
+    }
+    spirv::GlobalVariableOp var = createGlobalVarForGraphEntryPoint(
+        rewriter, graphOp, argType.index(), true, abiInfo);
+    if (!var)
+      return failure();
+    interfaceVars.push_back(
+        SymbolRefAttr::get(rewriter.getContext(), var.getSymName()));
+  }
+
+  for (const auto &resType :
+       llvm::enumerate(graphOp.getFunctionType().getResults())) {
+    auto abiInfo = graphOp.getResultAttrOfType<spirv::InterfaceVarABIAttr>(
+        resType.index(), attrName);
+    if (!abiInfo) {
+      // Non-entry point graphs are not handled in this ABI lowering and will
+      // produce an error.
+      return failure();
+    }
+    spirv::GlobalVariableOp var = createGlobalVarForGraphEntryPoint(
+        rewriter, graphOp, resType.index(), false, abiInfo);
+    if (!var)
+      return failure();
+    interfaceVars.push_back(
+        SymbolRefAttr::get(rewriter.getContext(), var.getSymName()));
+  }
+
+  // Creates a new function with the update signature.
+  rewriter.modifyOpInPlace(graphOp, [&] {
+    for (const auto &argType :
+         llvm::enumerate(graphOp.getFunctionType().getInputs())) {
+      graphOp.removeArgAttr(argType.index(), attrName);
+    }
+    for (const auto &resType :
+         llvm::enumerate(graphOp.getFunctionType().getResults())) {
+      graphOp.removeResultAttr(resType.index(),
+                               rewriter.getStringAttr(attrName));
+    }
+  });
+
+  return lowerGraphEntryPoint(rewriter, graphOp, interfaceVars);
+}
+
 void LowerABIAttributesPass::runOnOperation() {
   // Uses the signature conversion methodology of the dialect conversion
   // framework to implement the conversion.
@@ -313,6 +437,7 @@ void LowerABIAttributesPass::runOnOperation() {
 
   RewritePatternSet patterns(context);
   patterns.add<ProcessInterfaceVarABI>(typeConverter, context);
+  patterns.add<ProcessGraphInterfaceVarABI>(typeConverter, context);
 
   ConversionTarget target(*context);
   // "Legal" function ops should have no interface variable ABI attributes.
@@ -323,6 +448,17 @@ void LowerABIAttributesPass::runOnOperation() {
         return false;
     return true;
   });
+  target.addDynamicallyLegalOp<spirv::GraphARMOp>([&](spirv::GraphARMOp op) {
+    StringRef attrName = spirv::getInterfaceVarABIAttrName();
+    for (unsigned i = 0, e = op.getNumArguments(); i < e; ++i)
+      if (op.getArgAttr(i, attrName))
+        return false;
+    for (unsigned i = 0, e = op.getNumResults(); i < e; ++i)
+      if (op.getResultAttr(i, attrName))
+        return false;
+    return true;
+  });
+
   // All other SPIR-V ops are legal.
   target.markUnknownOpDynamicallyLegal([](Operation *op) {
     return op->getDialect()->getNamespace() ==
diff --git a/mlir/lib/Dialect/SPIRV/Transforms/UpdateVCEPass.cpp b/mlir/lib/Dialect/SPIRV/Transforms/UpdateVCEPass.cpp
index 6a9b951ca61d..b4bb5eb3ed42 100644
--- a/mlir/lib/Dialect/SPIRV/Transforms/UpdateVCEPass.cpp
+++ b/mlir/lib/Dialect/SPIRV/Transforms/UpdateVCEPass.cpp
@@ -95,6 +95,16 @@ static LogicalResult checkAndUpdateCapabilityRequirements(
   return success();
 }
 
+static SetVector<spirv::Capability>
+withImpliedCapabilities(SetVector<spirv::Capability> &caps) {
+  SetVector<spirv::Capability> allCaps(caps.begin(), caps.end());
+  for (auto cap : caps) {
+    ArrayRef<spirv::Capability> directCaps = getDirectImpliedCapabilities(cap);
+    allCaps.insert(directCaps.begin(), directCaps.end());
+  }
+  return allCaps;
+}
+
 void UpdateVCEPass::runOnOperation() {
   spirv::ModuleOp module = getOperation();
 
@@ -151,6 +161,14 @@ void UpdateVCEPass::runOnOperation() {
     if (auto globalVar = dyn_cast<spirv::GlobalVariableOp>(op))
       valueTypes.push_back(globalVar.getType());
 
+    // If the op is FunctionLike make sure to process input and result types
+    if (auto funcOpInterface = dyn_cast<FunctionOpInterface>(op)) {
+      auto inputTypes = funcOpInterface.getArgumentTypes();
+      auto resultTypes = funcOpInterface.getResultTypes();
+      valueTypes.append(inputTypes.begin(), inputTypes.end());
+      valueTypes.append(resultTypes.begin(), resultTypes.end());
+    }
+
     // Requirements from values' types
     SmallVector<ArrayRef<spirv::Extension>, 4> typeExtensions;
     SmallVector<ArrayRef<spirv::Capability>, 8> typeCapabilities;
@@ -168,12 +186,30 @@ void UpdateVCEPass::runOnOperation() {
         return WalkResult::interrupt();
     }
 
+    deducedCapabilities = withImpliedCapabilities(deducedCapabilities);
+
     return WalkResult::advance();
   });
 
   if (walkResult.wasInterrupted())
     return signalPassFailure();
 
+  // Update min version requirement for capabilities after deducing them
+  for (auto &cap : deducedCapabilities) {
+    std::optional<spirv::Version> minVersion = spirv::getMinVersion(cap);
+    if (minVersion) {
+      deducedVersion = std::max(deducedVersion, *minVersion);
+      if (deducedVersion > allowedVersion) {
+        module.emitError("Capability '")
+            << spirv::stringifyCapability(cap) << "' requires min version "
+            << spirv::stringifyVersion(deducedVersion)
+            << " but target environment allows up to "
+            << spirv::stringifyVersion(allowedVersion);
+        return signalPassFailure();
+      }
+    }
+  }
+
   // TODO: verify that the deduced version is consistent with
   // SPIR-V ops' maximal version requirements.
 
diff --git a/mlir/lib/IR/AsmPrinter.cpp b/mlir/lib/IR/AsmPrinter.cpp
index f95ad290a198..58fa14c0f525 100644
--- a/mlir/lib/IR/AsmPrinter.cpp
+++ b/mlir/lib/IR/AsmPrinter.cpp
@@ -104,7 +104,8 @@ void OpAsmPrinter::printFunctionalType(Operation *op) {
   // it is a function (avoiding a grammar ambiguity).
   bool wrapped = op->getNumResults() != 1;
   if (!wrapped && op->getResult(0).getType() &&
-      llvm::isa<FunctionType>(op->getResult(0).getType()))
+      (llvm::isa<FunctionType>(op->getResult(0).getType()) ||
+       llvm::isa<GraphType>(op->getResult(0).getType())))
     wrapped = true;
 
   if (wrapped)
@@ -2837,6 +2838,20 @@ void AsmPrinter::Impl::printTypeImpl(Type type) {
         os << '>';
       })
       .Case<NoneType>([&](Type) { os << "none"; })
+      .Case<GraphType>([&](GraphType graphTy) {
+        os << '(';
+        interleaveComma(graphTy.getInputs(), [&](Type ty) { printType(ty); });
+        os << ") -> ";
+        ArrayRef<Type> results = graphTy.getResults();
+        if (results.size() == 1 && !(llvm::isa<FunctionType>(results[0]) ||
+                                     llvm::isa<GraphType>(results[0]))) {
+          printType(results[0]);
+        } else {
+          os << '(';
+          interleaveComma(results, [&](Type ty) { printType(ty); });
+          os << ')';
+        }
+      })
       .Default([&](Type type) { return printDialectType(type); });
 }
 
diff --git a/mlir/lib/IR/Builders.cpp b/mlir/lib/IR/Builders.cpp
index f657db142eeb..3d366276b437 100644
--- a/mlir/lib/IR/Builders.cpp
+++ b/mlir/lib/IR/Builders.cpp
@@ -76,6 +76,10 @@ FunctionType Builder::getFunctionType(TypeRange inputs, TypeRange results) {
   return FunctionType::get(context, inputs, results);
 }
 
+GraphType Builder::getGraphType(TypeRange inputs, TypeRange results) {
+  return GraphType::get(context, inputs, results);
+}
+
 TupleType Builder::getTupleType(TypeRange elementTypes) {
   return TupleType::get(context, elementTypes);
 }
diff --git a/mlir/lib/IR/BuiltinTypes.cpp b/mlir/lib/IR/BuiltinTypes.cpp
index 1604ebba190a..0ed2549bcc9b 100644
--- a/mlir/lib/IR/BuiltinTypes.cpp
+++ b/mlir/lib/IR/BuiltinTypes.cpp
@@ -179,6 +179,49 @@ FunctionType::getWithoutArgsAndResults(const BitVector &argIndices,
   return clone(newArgTypes, newResultTypes);
 }
 
+//===----------------------------------------------------------------------===//
+// GraphType
+//===----------------------------------------------------------------------===//
+
+unsigned GraphType::getNumInputs() const { return getImpl()->numInputs; }
+
+ArrayRef<Type> GraphType::getInputs() const {
+  return getImpl()->getInputs();
+}
+
+unsigned GraphType::getNumResults() const { return getImpl()->numResults; }
+
+ArrayRef<Type> GraphType::getResults() const {
+  return getImpl()->getResults();
+}
+
+GraphType GraphType::clone(TypeRange inputs, TypeRange results) const {
+  return get(getContext(), inputs, results);
+}
+
+/// Returns a new function type with the specified arguments and results
+/// inserted.
+GraphType GraphType::getWithArgsAndResults(ArrayRef<unsigned> argIndices,
+                                           TypeRange argTypes,
+                                           ArrayRef<unsigned> resultIndices,
+                                           TypeRange resultTypes) {
+  SmallVector<Type> argStorage, resultStorage;
+  TypeRange newArgTypes =
+      insertTypesInto(getInputs(), argIndices, argTypes, argStorage);
+  TypeRange newResultTypes =
+      insertTypesInto(getResults(), resultIndices, resultTypes, resultStorage);
+  return clone(newArgTypes, newResultTypes);
+}
+
+/// Returns a new function type without the specified arguments and results.
+GraphType GraphType::getWithoutArgsAndResults(const BitVector &argIndices,
+                                            const BitVector &resultIndices) {
+  SmallVector<Type> argStorage, resultStorage;
+  TypeRange newArgTypes = filterTypesOut(getInputs(), argIndices, argStorage);
+  TypeRange newResultTypes =
+      filterTypesOut(getResults(), resultIndices, resultStorage);
+  return clone(newArgTypes, newResultTypes);
+}
 //===----------------------------------------------------------------------===//
 // OpaqueType
 //===----------------------------------------------------------------------===//
diff --git a/mlir/lib/Target/SPIRV/Deserialization/DeserializeOps.cpp b/mlir/lib/Target/SPIRV/Deserialization/DeserializeOps.cpp
index 9fa03725d05e..289c3c755049 100644
--- a/mlir/lib/Target/SPIRV/Deserialization/DeserializeOps.cpp
+++ b/mlir/lib/Target/SPIRV/Deserialization/DeserializeOps.cpp
@@ -85,6 +85,12 @@ Value spirv::Deserializer::getValue(uint32_t id) {
   if (auto undef = getUndefType(id)) {
     return opBuilder.create<spirv::UndefOp>(unknownLoc, undef);
   }
+  if (auto graphConstantARMInfo = getGraphConstantARM(id)) {
+    auto graphConstantID = graphConstantARMInfo->graphConstantID;
+    auto resultType = graphConstantARMInfo->resultType;
+    return opBuilder.create<spirv::GraphConstantARMOp>(unknownLoc, resultType,
+                                                       graphConstantID);
+  }
   return valueMap.lookup(id);
 }
 
@@ -179,6 +185,7 @@ LogicalResult spirv::Deserializer::processInstruction(
   case spirv::Opcode::OpTypeStruct:
   case spirv::Opcode::OpTypePointer:
   case spirv::Opcode::OpTypeTensorARM:
+  case spirv::Opcode::OpTypeGraphARM:
   case spirv::Opcode::OpTypeCooperativeMatrixKHR:
     return processType(opcode, operands);
   case spirv::Opcode::OpTypeForwardPointer:
@@ -207,12 +214,26 @@ LogicalResult spirv::Deserializer::processInstruction(
     return processConstantBool(/*isTrue=*/false, operands, /*isSpec=*/true);
   case spirv::Opcode::OpConstantNull:
     return processConstantNull(operands);
+  case spirv::Opcode::OpGraphConstantARM:
+    return processGraphConstantARM(operands);
   case spirv::Opcode::OpDecorate:
     return processDecoration(operands);
   case spirv::Opcode::OpMemberDecorate:
     return processMemberDecoration(operands);
   case spirv::Opcode::OpFunction:
     return processFunction(operands);
+  case spirv::Opcode::OpGraphEntryPointARM:
+    if (deferInstructions) {
+      deferredInstructions.emplace_back(opcode, operands);
+      return success();
+    }
+    return processGraphEntryPointARM(operands);
+  case spirv::Opcode::OpGraphARM:
+    return processGraphARM(operands);
+  case spirv::Opcode::OpGraphSetOutputARM:
+    return processOpGraphSetOutputARM(operands);
+  case spirv::Opcode::OpGraphEndARM:
+    return processGraphARMEnd(operands);
   case spirv::Opcode::OpLabel:
     return processLabel(operands);
   case spirv::Opcode::OpBranch:
diff --git a/mlir/lib/Target/SPIRV/Deserialization/Deserializer.cpp b/mlir/lib/Target/SPIRV/Deserialization/Deserializer.cpp
index d133d0332e27..8ed10e570713 100644
--- a/mlir/lib/Target/SPIRV/Deserialization/Deserializer.cpp
+++ b/mlir/lib/Target/SPIRV/Deserialization/Deserializer.cpp
@@ -670,6 +670,213 @@ spirv::Deserializer::processFunctionEnd(ArrayRef<uint32_t> operands) {
   return success();
 }
 
+LogicalResult
+spirv::Deserializer::processGraphEntryPointARM(ArrayRef<uint32_t> operands) {
+  unsigned wordIndex = 0;
+  if (wordIndex >= operands.size()) {
+    return emitError(unknownLoc,
+                     "missing graph defintion in OpGraphEntryPointARM");
+  }
+
+  uint32_t grID = operands[wordIndex++];
+  if (!graphMap.count(grID)) {
+    return emitError(unknownLoc,
+                     "missing graph definition/declaration with id ")
+           << grID;
+  }
+
+  spirv::GraphARMOp graphARM = graphMap[grID];
+  StringRef name = decodeStringLiteral(operands, wordIndex);
+  graphARM.setSymName(name);
+  graphARM.setEntryPoint(true);
+
+  SmallVector<Attribute, 4> interface;
+  while (wordIndex < operands.size()) {
+    auto arg = getGlobalVariable(operands[wordIndex]);
+    if (!arg) {
+      return emitError(unknownLoc, "undefined result <id> ")
+             << operands[wordIndex] << " while decoding OpGraphEntryPoint";
+    }
+    interface.push_back(SymbolRefAttr::get(arg.getOperation()));
+    wordIndex++;
+  }
+
+  // RAII guard to reset the insertion point to previous value when done.
+  OpBuilder::InsertionGuard insertionGuard(opBuilder);
+  opBuilder.setInsertionPoint(graphARM);
+  opBuilder.create<spirv::GraphEntryPointARMOp>(
+      unknownLoc, SymbolRefAttr::get(opBuilder.getContext(), name),
+      opBuilder.getArrayAttr(interface));
+
+  return success();
+}
+
+LogicalResult
+spirv::Deserializer::processGraphARM(ArrayRef<uint32_t> operands) {
+  if (curGraph) {
+    return emitError(unknownLoc, "found graph inside graph");
+  }
+  // Get the result type
+  if (operands.size() < 2) {
+    return emitError(unknownLoc, "OpGraphARM must have at least 2 parameters");
+  }
+
+  Type grType = getType(operands[0]);
+  if (!grType || !llvm::isa<GraphType>(grType)) {
+    return emitError(unknownLoc, "unknown graph type from <id> ")
+           << operands[0];
+  }
+  auto graphType = llvm::cast<GraphType>(grType);
+  if (graphType.getNumResults() <= 0) {
+    return emitError(unknownLoc, "expected at least one result");
+  }
+
+  uint32_t grID = operands[1];
+  if (graphMap.count(grID)) {
+    return emitError(unknownLoc, "duplicate graph definition/declaration");
+  }
+
+  std::string grName = getGraphSymbol(grID);
+  auto graphOp =
+      opBuilder.create<spirv::GraphARMOp>(unknownLoc, grName, graphType);
+  curGraph = graphMap[grID] = graphOp;
+  auto *entryBlock = graphOp.addEntryBlock();
+  LLVM_DEBUG({
+    logger.startLine()
+        << "//===-------------------------------------------===//\n";
+    logger.startLine() << "[graph] name: " << grName << "\n";
+    logger.startLine() << "[graph] type: " << grType << "\n";
+    logger.startLine() << "[graph] ID: " << grID << "\n";
+    logger.startLine() << "[graph] entry block: " << entryBlock << "\n";
+    logger.indent();
+  });
+
+  // Parse the op argument instructions
+  if (graphType.getNumInputs()) {
+    for (size_t i = 0, e = graphType.getNumInputs(); i != e; ++i) {
+      auto argType = graphType.getInput(i);
+      spirv::Opcode opcode = spirv::Opcode::OpNop;
+      ArrayRef<uint32_t> operands;
+      if (failed(sliceInstruction(opcode, operands,
+                                  spirv::Opcode::OpGraphInputARM))) {
+        return failure();
+      }
+      if (opcode != spirv::Opcode::OpGraphInputARM) {
+        return emitError(unknownLoc,
+                         "missing OpGraphInputARM instruction for argument ")
+               << i;
+      }
+
+      if (operands.size() != 3) {
+        return emitError(unknownLoc, "expected result type, result <id> and "
+                                     "input index for OpGraphInputARM");
+      }
+
+      auto argDefinedType = getType(operands[0]);
+      if (!argDefinedType) {
+        return emitError(unknownLoc, "unknown operand type <id> ")
+               << operands[0];
+      }
+
+      if (argDefinedType != argType) {
+        return emitError(unknownLoc,
+                         "mismatch in argument type between graph type "
+                         "definition ")
+               << graphType << " and argument type definition "
+               << argDefinedType << " at argument " << i;
+      }
+      if (getValue(operands[1])) {
+        return emitError(unknownLoc, "duplicate definition of result <id> ")
+               << operands[1];
+      }
+
+      auto inputIndexAttr = getConstantInt(operands[2]);
+      if (inputIndexAttr == nullptr) {
+        return emitError(unknownLoc,
+                         "unable to read inputIndex value from constant op ")
+               << operands[2];
+      }
+      auto argValue = graphOp.getArgument(inputIndexAttr.getInt());
+      valueMap[operands[1]] = argValue;
+    }
+  }
+
+  graphOutputs.resize(graphType.getNumResults());
+
+  // RAII guard to reset the insertion point to the module's region after
+  // deserializing the body of this function.
+  OpBuilder::InsertionGuard moduleInsertionGuard(opBuilder);
+
+  spirv::Opcode opcode = spirv::Opcode::OpNop;
+
+  blockMap[grID] = entryBlock;
+  if (failed(createGraphBlock(grID))) {
+    return failure();
+  }
+
+  // Process all the instructions in the graph until and including
+  // OpGraphEndARM.
+  ArrayRef<uint32_t> instOperands;
+  do {
+    if (failed(sliceInstruction(opcode, instOperands, std::nullopt))) {
+      return failure();
+    }
+
+    if (failed(processInstruction(opcode, instOperands))) {
+      return failure();
+    }
+  } while (opcode != spirv::Opcode::OpGraphEndARM);
+
+  return success();
+}
+
+LogicalResult
+spirv::Deserializer::processOpGraphSetOutputARM(ArrayRef<uint32_t> operands) {
+
+  if (operands.size() != 2) {
+    return emitError(
+        unknownLoc,
+        "expected value id and output index for OpGraphSetOutputARM");
+  }
+
+  auto id = operands[0];
+  auto value = getValue(id);
+  if (!value) {
+    return emitError(unknownLoc, "could not find result <id> ") << id;
+  }
+
+  auto outputIndexAttr = getConstantInt(operands[1]);
+  if (outputIndexAttr == nullptr) {
+    return emitError(unknownLoc,
+                     "unable to read outputIndex value from constant op ")
+           << operands[1];
+  }
+  graphOutputs[outputIndexAttr.getInt()] = value;
+  return success();
+}
+
+LogicalResult
+spirv::Deserializer::processGraphARMEnd(ArrayRef<uint32_t> operands) {
+  // Create GraphOutputsARM instruction
+  opBuilder.create<spirv::GraphOutputsARMOp>(unknownLoc, graphOutputs);
+
+  // Process OpGraphEndARM.
+  if (!operands.empty()) {
+    return emitError(unknownLoc, "unexpected operands for OpGraphEndARM");
+  }
+
+  curBlock = nullptr;
+  curGraph = std::nullopt;
+  graphOutputs.clear();
+
+  LLVM_DEBUG({
+    logger.unindent();
+    logger.startLine()
+        << "//===-------------------------------------------===//\n";
+  });
+  return success();
+}
+
 std::optional<std::pair<Attribute, Type>>
 spirv::Deserializer::getConstant(uint32_t id) {
   auto constIt = constantMap.find(id);
@@ -702,6 +909,14 @@ std::string spirv::Deserializer::getFunctionSymbol(uint32_t id) {
   return funcName;
 }
 
+std::string spirv::Deserializer::getGraphSymbol(uint32_t id) {
+  auto graphName = nameMap.lookup(id).str();
+  if (graphName.empty()) {
+    graphName = "spirv_graph_" + std::to_string(id);
+  }
+  return graphName;
+}
+
 std::string spirv::Deserializer::getSpecConstantSymbol(uint32_t id) {
   auto constName = nameMap.lookup(id).str();
   if (constName.empty()) {
@@ -724,6 +939,14 @@ spirv::Deserializer::createSpecConstant(Location loc, uint32_t resultID,
   return op;
 }
 
+std::optional<spirv::GraphConstantARMOpMaterializationInfo>
+spirv::Deserializer::getGraphConstantARM(uint32_t id) {
+  auto graphConstIt = graphConstantMap.find(id);
+  if (graphConstIt == graphConstantMap.end())
+    return std::nullopt;
+  return graphConstIt->getSecond();
+}
+
 LogicalResult
 spirv::Deserializer::processGlobalVariable(ArrayRef<uint32_t> operands) {
   unsigned wordIndex = 0;
@@ -945,6 +1168,8 @@ LogicalResult spirv::Deserializer::processType(spirv::Opcode opcode,
     return processMatrixType(operands);
   case spirv::Opcode::OpTypeTensorARM:
     return processTensorARMType(operands);
+  case spirv::Opcode::OpTypeGraphARM:
+    return processGraphTypeARM(operands);
   default:
     return emitError(unknownLoc, "unhandled type instruction");
   }
@@ -1297,6 +1522,35 @@ spirv::Deserializer::processTensorARMType(ArrayRef<uint32_t> operands) {
   return success();
 }
 
+LogicalResult
+spirv::Deserializer::processGraphTypeARM(ArrayRef<uint32_t> operands) {
+  unsigned size = operands.size();
+  if (size < 2) {
+    return emitError(unknownLoc, "OpTypeGraphARM must have at least 2 operands "
+                                 "(result_id, num_inputs, (inout0_type, "
+                                 "inout1_type, ...))")
+           << size;
+  }
+  uint32_t numInputs = operands[1];
+  SmallVector<Type, 1> argTypes;
+  SmallVector<Type, 1> returnTypes;
+  for (unsigned i = 2; i < size; i++) {
+    Type inOutTy = getType(operands[i]);
+    if (!inOutTy) {
+      return emitError(unknownLoc,
+                       "OpTypeGraphARM references undefined element type.")
+             << operands[i];
+    }
+    if (i - 2 >= numInputs) {
+      returnTypes.push_back(inOutTy);
+    } else {
+      argTypes.push_back(inOutTy);
+    }
+  }
+  typeMap[operands[0]] = GraphType::get(context, argTypes, returnTypes);
+  return success();
+}
+
 LogicalResult
 spirv::Deserializer::processTypeForwardPointer(ArrayRef<uint32_t> operands) {
   if (operands.size() != 2)
@@ -1768,7 +2022,7 @@ LogicalResult
 spirv::Deserializer::processConstantNull(ArrayRef<uint32_t> operands) {
   if (operands.size() != 2) {
     return emitError(unknownLoc,
-                     "OpConstantNull must have type <id> and result <id>");
+                     "OpConstantNull must only have type <id> and result <id>");
   }
 
   Type resultType = getType(operands[0]);
@@ -1778,8 +2032,17 @@ spirv::Deserializer::processConstantNull(ArrayRef<uint32_t> operands) {
   }
 
   auto resultID = operands[1];
+  Attribute attr;
   if (resultType.isIntOrFloat() || isa<VectorType>(resultType)) {
-    auto attr = opBuilder.getZeroAttr(resultType);
+    attr = opBuilder.getZeroAttr(resultType);
+  } else if (isa<TensorArmType>(resultType)) {
+    auto shapedType = cast<ShapedType>(resultType);
+    auto element = opBuilder.getZeroAttr(shapedType.getElementType());
+    if (element)
+      attr = DenseElementsAttr::get(shapedType, element);
+  }
+
+  if (attr) {
     // For normal constants, we just record the attribute (and its type) for
     // later materialization at use sites.
     constantMap.try_emplace(resultID, attr, resultType);
@@ -1790,6 +2053,38 @@ spirv::Deserializer::processConstantNull(ArrayRef<uint32_t> operands) {
          << resultType;
 }
 
+LogicalResult
+spirv::Deserializer::processGraphConstantARM(ArrayRef<uint32_t> operands) {
+  if (operands.size() < 2) {
+    return emitError(unknownLoc)
+           << "OpGraphConstantARM must have type <id> and result <id>";
+  }
+  if (operands.size() < 3) {
+    return emitError(unknownLoc)
+           << "OpGraphConstantARM must have at least 1 more parameter";
+  }
+
+  Type resultType = getType(operands[0]);
+  if (!resultType) {
+    return emitError(unknownLoc, "undefined result type from <id> ")
+           << operands[0];
+  }
+
+  auto resultID = operands[1];
+
+  if (!llvm::dyn_cast<spirv::TensorArmType>(resultType)) {
+    return emitError(unknownLoc, "result must be of type OpTypeTensorARM");
+  }
+
+  APInt graph_constant_id = APInt(32, operands[2], /*isSigned=*/true);
+  Type i32Ty = opBuilder.getIntegerType(32);
+  auto attr = opBuilder.getIntegerAttr(i32Ty, graph_constant_id);
+  graphConstantMap.try_emplace(
+      resultID, GraphConstantARMOpMaterializationInfo{resultType, attr});
+
+  return success();
+}
+
 //===----------------------------------------------------------------------===//
 // Control flow
 //===----------------------------------------------------------------------===//
@@ -1887,6 +2182,24 @@ LogicalResult spirv::Deserializer::processLabel(ArrayRef<uint32_t> operands) {
   return success();
 }
 
+LogicalResult spirv::Deserializer::createGraphBlock(uint32_t graphID) {
+  if (!curGraph) {
+    return emitError(unknownLoc, "a graph block must appear inside a graph");
+  }
+
+  // We may have forward declared this block.
+  auto *block = getOrCreateBlock(graphID);
+  LLVM_DEBUG(logger.startLine()
+             << "[block] populating block " << block << "\n");
+  // If we have seen this block, make sure it was just a forward declaration.
+  assert(block->empty() && "re-deserialize the same block!");
+
+  opBuilder.setInsertionPointToStart(block);
+  blockMap[graphID] = curBlock = block;
+
+  return success();
+}
+
 LogicalResult
 spirv::Deserializer::processSelectionMerge(ArrayRef<uint32_t> operands) {
   if (!curBlock) {
diff --git a/mlir/lib/Target/SPIRV/Deserialization/Deserializer.h b/mlir/lib/Target/SPIRV/Deserialization/Deserializer.h
index 20482bd2bf50..dff6c69ed34a 100644
--- a/mlir/lib/Target/SPIRV/Deserialization/Deserializer.h
+++ b/mlir/lib/Target/SPIRV/Deserialization/Deserializer.h
@@ -105,6 +105,13 @@ struct SpecConstOperationMaterializationInfo {
   SmallVector<uint32_t> enclosedOpOperands;
 };
 
+/// A struct that collects the info needed to materialize/emit a
+/// GraphConstantARMOp.
+struct GraphConstantARMOpMaterializationInfo {
+  Type resultType;
+  IntegerAttr graphConstantID;
+};
+
 //===----------------------------------------------------------------------===//
 // Deserializer Declaration
 //===----------------------------------------------------------------------===//
@@ -210,9 +217,14 @@ private:
   /// exists; otherwise creates one based on the <id>.
   std::string getFunctionSymbol(uint32_t id);
 
-  /// Returns a symbol to be used for the specialization constant with the given
-  /// result <id>. This tries to use the specialization constant's OpName if
+  /// Returns a symbol to be used for the graph name with the given
+  /// result <id>. This tries to use the graph's OpName if
   /// exists; otherwise creates one based on the <id>.
+  std::string getGraphSymbol(uint32_t id);
+
+  /// Returns a symbol to be used for the specialization constant with the
+  /// given result <id>. This tries to use the specialization constant's
+  /// OpName if exists; otherwise creates one based on the <id>.
   std::string getSpecConstantSymbol(uint32_t id);
 
   /// Gets the specialization constant with the given result <id>.
@@ -236,6 +248,11 @@ private:
   spirv::SpecConstantOp createSpecConstant(Location loc, uint32_t resultID,
                                            TypedAttr defaultValue);
 
+  /// Gets the GraphConstantARM ID attribute and result type with the given
+  /// result <id>.
+  std::optional<spirv::GraphConstantARMOpMaterializationInfo>
+  getGraphConstantARM(uint32_t id);
+
   /// Processes the OpVariable instructions at current `offset` into `binary`.
   /// It is expected that this method is used for variables that are to be
   /// defined at module scope and will be deserialized into a
@@ -305,6 +322,16 @@ private:
 
   LogicalResult processTensorARMType(ArrayRef<uint32_t> operands);
 
+  LogicalResult processGraphTypeARM(ArrayRef<uint32_t> operands);
+
+  LogicalResult processGraphEntryPointARM(ArrayRef<uint32_t> operands);
+
+  LogicalResult processGraphARM(ArrayRef<uint32_t> operands);
+
+  LogicalResult processOpGraphSetOutputARM(ArrayRef<uint32_t> operands);
+
+  LogicalResult processGraphARMEnd(ArrayRef<uint32_t> operands);
+
   LogicalResult processTypeForwardPointer(ArrayRef<uint32_t> operands);
 
   //===--------------------------------------------------------------------===//
@@ -352,6 +379,10 @@ private:
   /// Processes a SPIR-V OpConstantNull instruction with the given `operands`.
   LogicalResult processConstantNull(ArrayRef<uint32_t> operands);
 
+  /// Processes a SPIR-V OpGraphConstantARM instruction with the given
+  /// `operands`.
+  LogicalResult processGraphConstantARM(ArrayRef<uint32_t> operands);
+
   //===--------------------------------------------------------------------===//
   // Debug
   //===--------------------------------------------------------------------===//
@@ -449,6 +480,9 @@ private:
   /// blocks declared as selection/loop headers are handled.
   LogicalResult structurizeControlFlow();
 
+  /// Creates a block for graph with the given graphID
+  LogicalResult createGraphBlock(uint32_t graphID);
+
   //===--------------------------------------------------------------------===//
   // Instruction
   //===--------------------------------------------------------------------===//
@@ -545,6 +579,9 @@ private:
   /// The current function under construction.
   std::optional<spirv::FuncOp> curFunction;
 
+  /// The current graph under construction.
+  std::optional<spirv::GraphARMOp> curGraph;
+
   /// The current block under construction.
   Block *curBlock = nullptr;
 
@@ -598,12 +635,19 @@ private:
   DenseMap<uint32_t, SpecConstOperationMaterializationInfo>
       specConstOperationMap;
 
+  // Result <id> to GraphConstantARM ID attribute and result type.
+  DenseMap<uint32_t, spirv::GraphConstantARMOpMaterializationInfo>
+      graphConstantMap;
+
   // Result <id> to variable mapping.
   DenseMap<uint32_t, spirv::GlobalVariableOp> globalVariableMap;
 
   // Result <id> to function mapping.
   DenseMap<uint32_t, spirv::FuncOp> funcMap;
 
+  // Result <id> to function mapping.
+  DenseMap<uint32_t, spirv::GraphARMOp> graphMap;
+
   // Result <id> to block mapping.
   DenseMap<uint32_t, Block *> blockMap;
 
@@ -667,6 +711,9 @@ private:
   /// Deserialization options.
   DeserializationOptions options;
 
+  /// List of IDs assigned to graph outputs.
+  SmallVector<Value> graphOutputs;
+
 #ifndef NDEBUG
   /// A logger used to emit information during the deserialzation process.
   llvm::ScopedPrinter logger;
diff --git a/mlir/lib/Target/SPIRV/Serialization/SerializeOps.cpp b/mlir/lib/Target/SPIRV/Serialization/SerializeOps.cpp
index d62529b85b3a..032b7156236d 100644
--- a/mlir/lib/Target/SPIRV/Serialization/SerializeOps.cpp
+++ b/mlir/lib/Target/SPIRV/Serialization/SerializeOps.cpp
@@ -203,6 +203,16 @@ Serializer::processSpecConstantOperationOp(spirv::SpecConstantOperationOp op) {
   return success();
 }
 
+LogicalResult
+Serializer::processGraphConstantARMOp(spirv::GraphConstantARMOp op) {
+  if (auto resultID = prepareGraphConstantId(op.getLoc(), op.getType(),
+                                             op.getGraphConstantIdAttr())) {
+    valueIDMap[op.getResult()] = resultID;
+    return success();
+  }
+  return failure();
+}
+
 LogicalResult Serializer::processUndefOp(spirv::UndefOp op) {
   auto undefType = op.getType();
   auto &id = undefValIDMap[undefType];
@@ -368,6 +378,122 @@ LogicalResult Serializer::processFuncOp(spirv::FuncOp op) {
   return success();
 }
 
+LogicalResult Serializer::processGraphARMOp(spirv::GraphARMOp op) {
+
+  if (op.getNumResults() < 1) {
+    return op.emitError("cannot serialize graph with no return types");
+  }
+
+  LLVM_DEBUG(llvm::dbgs() << "-- start graph '" << op.getName() << "' --\n");
+  assert(functionHeader.empty() && functionBody.empty());
+
+  uint32_t funcID = getOrCreateFunctionID(op.getName());
+  uint32_t fnTypeID = 0;
+  // Generate type of the function.
+  if (failed(processType(op.getLoc(), op.getFunctionType(), fnTypeID)))
+    return failure();
+  encodeInstructionInto(functionHeader, spirv::Opcode::OpGraphARM,
+                        {fnTypeID, funcID});
+
+  // Declare the parameters.
+  for (auto [idx, arg] : llvm::enumerate(op.getArguments())) {
+    uint32_t argTypeID = 0;
+    SmallVector<uint32_t, 3> inputOperands;
+
+    if (failed(processType(op.getLoc(), arg.getType(), argTypeID))) {
+      return failure();
+    }
+
+    uint32_t argValueID = getNextID();
+    valueIDMap[arg] = argValueID;
+
+    auto attr = IntegerAttr::get(IntegerType::get(op.getContext(), 32), idx);
+    auto indexID = prepareConstantInt(op.getLoc(), attr, false);
+
+    inputOperands.push_back(argTypeID);
+    inputOperands.push_back(argValueID);
+    inputOperands.push_back(indexID);
+
+    encodeInstructionInto(functionHeader, spirv::Opcode::OpGraphInputARM,
+                          inputOperands);
+  }
+
+  // Process the body.
+  if (op.isExternal()) {
+    return op.emitError("external function is unhandled");
+  }
+
+  if (failed(processBlock(&op.front(), /*omitLabel=*/true)))
+    return failure();
+  if (failed(visitInPrettyBlockOrder(
+          &op.front(), [&](Block *block) { return processBlock(block); },
+          /*skipHeader=*/true))) {
+    return failure();
+  }
+
+  LLVM_DEBUG(llvm::dbgs() << "-- completed graph '" << op.getName()
+                          << "' --\n");
+  // Insert OpFunctionEnd.
+  encodeInstructionInto(functionBody, spirv::Opcode::OpGraphEndARM, {});
+
+  graphs.append(functionHeader.begin(), functionHeader.end());
+  graphs.append(functionBody.begin(), functionBody.end());
+  functionHeader.clear();
+  functionBody.clear();
+
+  return success();
+}
+
+LogicalResult
+Serializer::processGraphEntryPointARMOp(spirv::GraphEntryPointARMOp op) {
+  SmallVector<uint32_t, 4> operands;
+  auto graph = op.getFn();
+  // Add the graph <id>.
+  uint32_t graphID = getOrCreateFunctionID(graph);
+  operands.push_back(graphID);
+  // Add the name of the graph.
+  spirv::encodeStringLiteralInto(operands, graph);
+
+  // Add the interface values.
+  if (auto interface = op.getInterface()) {
+    for (auto var : interface.getValue()) {
+      auto id = getVariableID(llvm::cast<FlatSymbolRefAttr>(var).getValue());
+      if (!id) {
+        return op.emitError(
+            "referencing undefined global variable."
+            "spirv.GraphEntryPointARM is at the end of spirv.module. All "
+            "referenced variables should already be defined");
+      }
+      operands.push_back(id);
+    }
+  }
+  encodeInstructionInto(graphs, spirv::Opcode::OpGraphEntryPointARM, operands);
+  return success();
+}
+
+LogicalResult Serializer::processGraphOutputsARMOp(spirv::GraphOutputsARMOp op) {
+  for (auto [idx, value] : llvm::enumerate(op->getOperands())) {
+    SmallVector<uint32_t, 2> outputOperands;
+
+    auto resType = value.getType();
+    uint32_t resTypeID = 0;
+    if (failed(processType(op.getLoc(), resType, resTypeID))) {
+      return failure();
+    }
+
+    uint32_t outputID = getValueID(value);
+    auto attr = IntegerAttr::get(IntegerType::get(op.getContext(), 32), idx);
+    auto indexID = prepareConstantInt(op.getLoc(), attr, false);
+
+    outputOperands.push_back(outputID);
+    outputOperands.push_back(indexID);
+
+    encodeInstructionInto(functionBody, spirv::Opcode::OpGraphSetOutputARM,
+                          outputOperands);
+  }
+  return success();
+}
+
 LogicalResult Serializer::processVariableOp(spirv::VariableOp op) {
   SmallVector<uint32_t, 4> operands;
   SmallVector<StringRef, 2> elidedAttrs;
diff --git a/mlir/lib/Target/SPIRV/Serialization/Serializer.cpp b/mlir/lib/Target/SPIRV/Serialization/Serializer.cpp
index 3400fcf6374e..3a493e1c751c 100644
--- a/mlir/lib/Target/SPIRV/Serialization/Serializer.cpp
+++ b/mlir/lib/Target/SPIRV/Serialization/Serializer.cpp
@@ -70,6 +70,22 @@ static Block *getPhiIncomingBlock(Block *block) {
   return block;
 }
 
+static bool isNull(Attribute attr) {
+  if (auto floatAttr = dyn_cast<FloatAttr>(attr)) {
+    return floatAttr.getValue().isZero();
+  }
+  if (auto boolAttr = dyn_cast<BoolAttr>(attr)) {
+    return !boolAttr.getValue();
+  }
+  if (auto intAttr = dyn_cast<IntegerAttr>(attr)) {
+    return intAttr.getValue().isZero();
+  }
+  if (auto denseElemAttr = dyn_cast<DenseElementsAttr>(attr)) {
+    return all_of(denseElemAttr.getValues<Attribute>(), isNull);
+  }
+  return false;
+}
+
 namespace mlir {
 namespace spirv {
 
@@ -115,7 +131,7 @@ void Serializer::collect(SmallVectorImpl<uint32_t> &binary) {
                     extensions.size() + extendedSets.size() +
                     memoryModel.size() + entryPoints.size() +
                     executionModes.size() + decorations.size() +
-                    typesGlobalValues.size() + functions.size();
+                    typesGlobalValues.size() + functions.size() + graphs.size();
 
   binary.clear();
   binary.reserve(moduleSize);
@@ -133,6 +149,7 @@ void Serializer::collect(SmallVectorImpl<uint32_t> &binary) {
   binary.append(decorations.begin(), decorations.end());
   binary.append(typesGlobalValues.begin(), typesGlobalValues.end());
   binary.append(functions.begin(), functions.end());
+  binary.append(graphs.begin(), graphs.end());
 }
 
 #ifndef NDEBUG
@@ -446,6 +463,19 @@ LogicalResult Serializer::processType(Location loc, Type type,
 LogicalResult
 Serializer::processTypeImpl(Location loc, Type type, uint32_t &typeID,
                             SetVector<StringRef> &serializationCtx) {
+
+  // Map unsigned integer types to singless integer types.
+  // This is needed otherwise the generated spirv assembly will contain
+  // twice a type declaration (like OpTypeInt 32 0) which is no permitted and
+  // such module fails validation. Indeed at MLIR level the two types are
+  // different and lookup in the cache below misses.
+  // Note: This conversion needs to happen here before the type is looked up in
+  // the cache.
+  if (type.isUnsignedInteger()) {
+    type = IntegerType::get(loc->getContext(), type.getIntOrFloatBitWidth(),
+                            IntegerType::SignednessSemantics::Signless);
+  }
+
   typeID = getTypeID(type);
   if (typeID)
     return success();
@@ -457,9 +487,12 @@ Serializer::processTypeImpl(Location loc, Type type, uint32_t &typeID,
   auto typeEnum = spirv::Opcode::OpTypeVoid;
   bool deferSerialization = false;
 
-  if ((isa<FunctionType>(type) &&
-       succeeded(prepareFunctionType(loc, cast<FunctionType>(type), typeEnum,
-                                     operands))) ||
+  if ((llvm::isa<FunctionType>(type) &&
+       succeeded(prepareFunctionType(loc, llvm::cast<FunctionType>(type),
+                                     typeEnum, operands))) ||
+      (llvm::isa<GraphType>(type) &&
+       succeeded(prepareGraphType(loc, llvm::cast<GraphType>(type), typeEnum,
+                                  operands))) ||
       succeeded(prepareBasicType(loc, type, typeID, typeEnum, operands,
                                  deferSerialization, serializationCtx))) {
     if (deferSerialization)
@@ -490,6 +523,7 @@ Serializer::processTypeImpl(Location loc, Type type, uint32_t &typeID,
     return success();
   }
 
+  emitError(loc, "failed to process type: ") << type;
   return failure();
 }
 
@@ -805,6 +839,35 @@ Serializer::prepareFunctionType(Location loc, FunctionType type,
   return success();
 }
 
+LogicalResult
+Serializer::prepareGraphType(Location loc, GraphType type,
+                             spirv::Opcode &typeEnum,
+                             SmallVectorImpl<uint32_t> &operands) {
+  typeEnum = spirv::Opcode::OpTypeGraphARM;
+  assert(type.getNumResults() >= 1 &&
+         "serialization requires at least a return value");
+
+  operands.push_back(type.getNumInputs());
+
+  for (auto &res : type.getInputs()) {
+    uint32_t argTypeID = 0;
+    if (failed(processType(loc, res, argTypeID))) {
+      return failure();
+    }
+    operands.push_back(argTypeID);
+  }
+
+  for (auto &res : type.getResults()) {
+    uint32_t resultID = 0;
+    if (failed(processType(loc, res, resultID))) {
+      return failure();
+    }
+    operands.push_back(resultID);
+  }
+
+  return success();
+}
+
 //===----------------------------------------------------------------------===//
 // Constant
 //===----------------------------------------------------------------------===//
@@ -923,6 +986,30 @@ Serializer::prepareDenseElementsConstant(Location loc, Type constType,
     } else {
       return 0;
     }
+  } else if (isa<spirv::TensorArmType>(constType)) {
+    if (isNull(valueAttr)) {
+      encodeInstructionInto(typesGlobalValues, spirv::Opcode::OpConstantNull,
+                            {typeID, resultID});
+      return resultID;
+    }
+    numberOfConstituents = shapedType.getNumElements();
+    operands.reserve(numberOfConstituents + 2);
+    for (int i = 0; i < numberOfConstituents; ++i) {
+      uint32_t elementID = 0;
+      if (auto attr = dyn_cast<DenseIntElementsAttr>(valueAttr)) {
+        elementID =
+            elementType.isInteger(1)
+                ? prepareConstantBool(loc, attr.getValues<BoolAttr>()[i])
+                : prepareConstantInt(loc, attr.getValues<IntegerAttr>()[i]);
+      }
+      if (auto attr = dyn_cast<DenseFPElementsAttr>(valueAttr)) {
+        elementID = prepareConstantFp(loc, attr.getValues<FloatAttr>()[i]);
+      }
+      if (!elementID) {
+        return 0;
+      }
+      operands.push_back(elementID);
+    }
   } else {
     operands.reserve(numberOfConstituents + 2);
     for (int i = 0; i < numberOfConstituents; ++i) {
@@ -1056,6 +1143,41 @@ uint32_t Serializer::prepareConstantInt(Location loc, IntegerAttr intAttr,
   return resultID;
 }
 
+uint32_t Serializer::prepareGraphConstantId(Location loc, Type graphConstType,
+                                            IntegerAttr intAttr) {
+  // De-duplicate graph constants.
+  if (auto id = getGraphConstantARMId(intAttr)) {
+    return id;
+  }
+
+  // Process the type for this graph constant.
+  uint32_t typeID = 0;
+  if (failed(processType(loc, graphConstType, typeID))) {
+    return 0;
+  }
+
+  auto resultID = getNextID();
+  APInt value = intAttr.getValue();
+  unsigned bitwidth = value.getBitWidth();
+  if (bitwidth > 32) {
+    emitError(loc, "Too wide attribute for OpGraphConstantARM: ")
+        << bitwidth << " bits";
+    return 0;
+  }
+  bool isSigned = value.isSignedIntN(bitwidth);
+
+  uint32_t word = 0;
+  if (isSigned) {
+    word = static_cast<int32_t>(value.getSExtValue());
+  } else {
+    word = static_cast<uint32_t>(value.getZExtValue());
+  }
+  encodeInstructionInto(typesGlobalValues, spirv::Opcode::OpGraphConstantARM,
+                        {typeID, resultID, word});
+  graphConstIDMap[intAttr] = resultID;
+  return resultID;
+}
+
 uint32_t Serializer::prepareConstantFp(Location loc, FloatAttr floatAttr,
                                        bool isSpec) {
   if (!isSpec) {
@@ -1109,6 +1231,18 @@ uint32_t Serializer::prepareConstantFp(Location loc, FloatAttr floatAttr,
   return resultID;
 }
 
+static Type getValueType(Attribute attr) {
+  if (auto typedAttr = dyn_cast<TypedAttr>(attr)) {
+    return typedAttr.getType();
+  }
+
+  if (auto arrayAttr = dyn_cast<ArrayAttr>(attr)) {
+    return spirv::ArrayType::get(getValueType(arrayAttr[0]), arrayAttr.size());
+  }
+
+  return nullptr;
+}
+
 uint32_t Serializer::prepareConstantCompositeReplicate(Location loc,
                                                        Type resultType,
                                                        Attribute valueAttr) {
@@ -1122,18 +1256,9 @@ uint32_t Serializer::prepareConstantCompositeReplicate(Location loc,
     return 0;
   }
 
-  Type valueType;
-  if (auto typedAttr = dyn_cast<TypedAttr>(valueAttr)) {
-    valueType = typedAttr.getType();
-  } else if (auto arrayAttr = dyn_cast<ArrayAttr>(valueAttr)) {
-    auto typedElemAttr = dyn_cast<TypedAttr>(arrayAttr[0]);
-    if (!typedElemAttr)
-      return 0;
-    valueType =
-        spirv::ArrayType::get(typedElemAttr.getType(), arrayAttr.size());
-  } else {
+  Type valueType = getValueType(valueAttr);
+  if (!valueAttr)
     return 0;
-  }
 
   auto compositeType = dyn_cast<CompositeType>(resultType);
   if (!compositeType)
@@ -1148,11 +1273,14 @@ uint32_t Serializer::prepareConstantCompositeReplicate(Location loc,
   }
 
   uint32_t resultID = getNextID();
-  uint32_t operands[] = {typeID, resultID, constandID};
-
-  encodeInstructionInto(typesGlobalValues,
-                        spirv::Opcode::OpConstantCompositeReplicateEXT,
-                        operands);
+  if (dyn_cast<spirv::TensorArmType>(resultType) && isNull(valueAttr)) {
+    encodeInstructionInto(typesGlobalValues, spirv::Opcode::OpConstantNull,
+                          {typeID, resultID});
+  } else {
+    encodeInstructionInto(typesGlobalValues,
+                          spirv::Opcode::OpConstantCompositeReplicateEXT,
+                          {typeID, resultID, constandID});
+  }
 
   constCompositeReplicateIDMap[valueTypePair] = resultID;
   return resultID;
@@ -1381,9 +1509,18 @@ LogicalResult Serializer::processOperation(Operation *opInst) {
         return processConstantCompositeReplicateOp(op);
       })
       .Case([&](spirv::FuncOp op) { return processFuncOp(op); })
+      .Case([&](spirv::GraphARMOp op) { return processGraphARMOp(op); })
+      .Case([&](spirv::GraphEntryPointARMOp op) {
+        return processGraphEntryPointARMOp(op);
+      })
+      .Case(
+          [&](spirv::GraphOutputsARMOp op) { return processGraphOutputsARMOp(op); })
       .Case([&](spirv::GlobalVariableOp op) {
         return processGlobalVariableOp(op);
       })
+      .Case([&](spirv::GraphConstantARMOp op) {
+        return processGraphConstantARMOp(op);
+      })
       .Case([&](spirv::LoopOp op) { return processLoopOp(op); })
       .Case([&](spirv::ReferenceOfOp op) { return processReferenceOfOp(op); })
       .Case([&](spirv::SelectionOp op) { return processSelectionOp(op); })
diff --git a/mlir/lib/Target/SPIRV/Serialization/Serializer.h b/mlir/lib/Target/SPIRV/Serialization/Serializer.h
index 7047869bca4c..324db84f353e 100644
--- a/mlir/lib/Target/SPIRV/Serialization/Serializer.h
+++ b/mlir/lib/Target/SPIRV/Serialization/Serializer.h
@@ -122,6 +122,8 @@ private:
   LogicalResult
   processSpecConstantOperationOp(spirv::SpecConstantOperationOp op);
 
+  LogicalResult processGraphConstantARMOp(spirv::GraphConstantARMOp op);
+
   /// SPIR-V dialect supports OpUndef using spirv.UndefOp that produces a SSA
   /// value to use with other operations. The SPIR-V spec recommends that
   /// OpUndef be generated at module level. The serialization generates an
@@ -135,6 +137,15 @@ private:
   LogicalResult processFuncOp(spirv::FuncOp op);
   LogicalResult processFuncParameter(spirv::FuncOp op);
 
+  /// Processes a SPIR-V GraphARM op.
+  LogicalResult processGraphARMOp(spirv::GraphARMOp op);
+
+  /// Processes a SPIR-V GraphEntryPointARM op.
+  LogicalResult processGraphEntryPointARMOp(spirv::GraphEntryPointARMOp op);
+
+  /// Processes a SPIR-V GraphOutputsARMOp op.
+  LogicalResult processGraphOutputsARMOp(spirv::GraphOutputsARMOp op);
+
   LogicalResult processVariableOp(spirv::VariableOp op);
 
   /// Process a SPIR-V GlobalVariableOp
@@ -189,6 +200,10 @@ private:
                                     spirv::Opcode &typeEnum,
                                     SmallVectorImpl<uint32_t> &operands);
 
+  LogicalResult prepareGraphType(Location loc, GraphType type,
+                                 spirv::Opcode &typeEnum,
+                                 SmallVectorImpl<uint32_t> &operands);
+
   //===--------------------------------------------------------------------===//
   // Constant
   //===--------------------------------------------------------------------===//
@@ -238,6 +253,13 @@ private:
   uint32_t prepareConstantInt(Location loc, IntegerAttr intAttr,
                               bool isSpec = false);
 
+  uint32_t getGraphConstantARMId(Attribute value) const {
+    return graphConstIDMap.lookup(value);
+  }
+
+  uint32_t prepareGraphConstantId(Location loc, Type graphConstType,
+                                  IntegerAttr intAttr);
+
   uint32_t prepareConstantFp(Location loc, FloatAttr floatAttr,
                              bool isSpec = false);
 
@@ -340,7 +362,7 @@ private:
   spirv::ModuleOp module;
 
   /// An MLIR builder for getting MLIR constructs.
-  mlir::Builder mlirBuilder;
+  mlir::OpBuilder mlirBuilder;
 
   /// Serialization options.
   SerializationOptions options;
@@ -372,6 +394,7 @@ private:
   SmallVector<uint32_t, 0> decorations;
   SmallVector<uint32_t, 0> typesGlobalValues;
   SmallVector<uint32_t, 0> functions;
+  SmallVector<uint32_t, 0> graphs;
 
   /// Recursive struct references are serialized as OpTypePointer instructions
   /// to the recursive struct type. However, the OpTypePointer instruction
@@ -388,15 +411,22 @@ private:
       recursiveStructInfos;
 
   /// `functionHeader` contains all the instructions that must be in the first
-  /// block in the function, and `functionBody` contains the rest. After
-  /// processing FuncOp, the encoded instructions of a function are appended to
-  /// `functions`. An example of instructions in `functionHeader` in order:
+  /// block in the function or graph, and `functionBody` contains the rest.
+  /// After processing FuncOp/GraphARMOp, the encoded instructions of a function
+  /// or graph are appended to `functions` or `graphs` respectively. Examples of
+  /// instructions in `functionHeader` in order:
+  ///
+  /// For a FuncOp:
   /// OpFunction ...
   /// OpFunctionParameter ...
   /// OpFunctionParameter ...
   /// OpLabel ...
   /// OpVariable ...
   /// OpVariable ...
+  ///
+  /// For a GraphARMOp
+  /// OpGraphARM ...
+  /// OpGraphInputARM ...
   SmallVector<uint32_t, 0> functionHeader;
   SmallVector<uint32_t, 0> functionBody;
 
@@ -412,6 +442,9 @@ private:
   /// Map from specialization constant names to their <id>s.
   llvm::StringMap<uint32_t> specConstIDMap;
 
+  /// Map from graph constant ID value to their <id>s.
+  DenseMap<Attribute, uint32_t> graphConstIDMap;
+
   /// Map from GlobalVariableOps name to <id>s.
   llvm::StringMap<uint32_t> globalVarIDMap;
 
diff --git a/mlir/test/CMakeLists.txt b/mlir/test/CMakeLists.txt
index ac8b44f53aeb..89568e7766ae 100644
--- a/mlir/test/CMakeLists.txt
+++ b/mlir/test/CMakeLists.txt
@@ -68,6 +68,7 @@ endif()
 llvm_canonicalize_cmake_booleans(
   LLVM_BUILD_EXAMPLES
   LLVM_HAS_NVPTX_TARGET
+  LLVM_INCLUDE_SPIRV_TOOLS_TESTS
   MLIR_ENABLE_BINDINGS_PYTHON
   MLIR_ENABLE_CUDA_RUNNER
   MLIR_ENABLE_ROCM_CONVERSIONS
@@ -217,6 +218,11 @@ if(MLIR_ENABLE_BINDINGS_PYTHON)
   )
 endif()
 
+if (LLVM_INCLUDE_SPIRV_TOOLS_TESTS)
+  list(APPEND MLIR_TEST_DEPENDS spirv-as)
+  list(APPEND MLIR_TEST_DEPENDS spirv-val)
+endif()
+
 # This target can be used to just build the dependencies
 # for the check-mlir target without executing the tests.
 # This is useful for bots when splitting the build step
diff --git a/mlir/test/Dialect/SPIRV/IR/availability.mlir b/mlir/test/Dialect/SPIRV/IR/availability.mlir
index f56bc3967b4b..bc1505d32d4d 100644
--- a/mlir/test/Dialect/SPIRV/IR/availability.mlir
+++ b/mlir/test/Dialect/SPIRV/IR/availability.mlir
@@ -306,3 +306,20 @@ func.func @constant_composite_replicate() -> () {
   %0 = spirv.EXT.ConstantCompositeReplicate [1 : i32] : vector<2xi32>
   spirv.Return
 }
+
+//===----------------------------------------------------------------------===//
+// GraphARM ops
+//===----------------------------------------------------------------------===//
+
+// CHECK-LABEL: graph_arm
+spirv.ARM.Graph @graph_arm(%arg0: !spirv.arm.tensor<1x16x16x16xi8>) -> !spirv.arm.tensor<1x16x16x16xi8> {
+  // CHECK: spirv.ARM.GraphOutputs min version: v1.0
+  // CHECK: spirv.ARM.GraphOutputs max version: v1.6
+  // CHECK: spirv.ARM.GraphOutputs extensions: [ [SPV_ARM_graph, SPV_ARM_tensors, SPV_KHR_vulkan_memory_model] ]
+  // CHECK: spirv.ARM.GraphOutputs capabilities: [ [GraphARM] ]
+  spirv.ARM.GraphOutputs %arg0 : !spirv.arm.tensor<1x16x16x16xi8>
+// CHECK: spirv.ARM.Graph min version: v1.0
+// CHECK: spirv.ARM.Graph max version: v1.6
+// CHECK: spirv.ARM.Graph extensions: [ [SPV_ARM_graph, SPV_ARM_tensors, SPV_KHR_vulkan_memory_model] ]
+// CHECK: spirv.ARM.Graph capabilities: [ [GraphARM] ]
+}
diff --git a/mlir/test/Dialect/SPIRV/IR/target-and-abi.mlir b/mlir/test/Dialect/SPIRV/IR/target-and-abi.mlir
index 10fbcf06eb05..515162bf99ae 100644
--- a/mlir/test/Dialect/SPIRV/IR/target-and-abi.mlir
+++ b/mlir/test/Dialect/SPIRV/IR/target-and-abi.mlir
@@ -14,7 +14,7 @@ func.func @unknown_attr_on_region(%arg: i32 {spirv.something}) {
 
 // -----
 
-// expected-error @+1 {{cannot attach SPIR-V attributes to region result}}
+// expected-error @+1 {{found unsupported 'spirv.something' attribute on region argument}}
 func.func @unknown_attr_on_region() -> (i32 {spirv.something}) {
   %0 = arith.constant 10.0 : f32
   return %0: f32
@@ -101,6 +101,27 @@ func.func @interface_var(
 
 // -----
 
+// CHECK: {spirv.interface_var_abi = #spirv.interface_var_abi<(0, 1)>}
+func.func @interface_var(%arg: f32) -> (
+    f32 {spirv.interface_var_abi = #spirv.interface_var_abi<(0, 1)>}
+) { return %arg : f32 }
+
+// -----
+
+// CHECK: {spirv.interface_var_abi = #spirv.interface_var_abi<(0, 1), Uniform>}
+func.func @interface_var(%arg: f32) -> (
+    f32 {spirv.interface_var_abi = #spirv.interface_var_abi<(0, 1), Uniform>}
+) { return %arg : f32 }
+
+// -----
+
+// expected-error @+1 {{'spirv.interface_var_abi' attribute cannot specify storage class when attaching to a non-scalar value}}
+func.func @interface_var(%arg0 : memref<4xf32>) -> (
+  memref<4xf32> {spirv.interface_var_abi = #spirv.interface_var_abi<(0, 1), Uniform>}
+) { return %arg0 : memref<4xf32> }
+
+// -----
+
 //===----------------------------------------------------------------------===//
 // spirv.resource_limits
 //===----------------------------------------------------------------------===//
diff --git a/mlir/test/Dialect/SPIRV/Transforms/abi-interface.mlir b/mlir/test/Dialect/SPIRV/Transforms/abi-interface.mlir
index bd51a0784365..9f5694135d62 100644
--- a/mlir/test/Dialect/SPIRV/Transforms/abi-interface.mlir
+++ b/mlir/test/Dialect/SPIRV/Transforms/abi-interface.mlir
@@ -35,6 +35,28 @@ spirv.module Logical GLSL450 {
 
 // -----
 
+module attributes {
+  spirv.target_env = #spirv.target_env<
+     #spirv.vce<v1.0, [VulkanMemoryModel, Shader, Int8, TensorsARM, GraphARM], [SPV_ARM_tensors, SPV_ARM_graph]>, #spirv.resource_limits<>>
+} {
+
+// CHECK-LABEL: spirv.module
+spirv.module Logical Vulkan {
+  //  CHECK-DAG:    spirv.GlobalVariable [[VARARG0:@.*]] bind(0, 0) : !spirv.ptr<!spirv.arm.tensor<1x16x16x16xi8>, UniformConstant>
+  //  CHECK-DAG:    spirv.GlobalVariable [[VARRES0:@.*]] bind(0, 1) : !spirv.ptr<!spirv.arm.tensor<1x16x16x16xi8>, UniformConstant>
+
+  //      CHECK:    spirv.ARM.GraphEntryPoint [[GN:@.*]], [[VARARG0]], [[VARRES0]]
+  //      CHECK:    spirv.ARM.Graph [[GN]]([[ARG0:%.*]]: !spirv.arm.tensor<1x16x16x16xi8>) -> !spirv.arm.tensor<1x16x16x16xi8> attributes {entry_point = true}
+  spirv.ARM.Graph @main(%arg0: !spirv.arm.tensor<1x16x16x16xi8> {spirv.interface_var_abi = #spirv.interface_var_abi<(0, 0)>})
+                  -> (!spirv.arm.tensor<1x16x16x16xi8> {spirv.interface_var_abi = #spirv.interface_var_abi<(0, 1)>}) attributes {entry_point = true} {
+    spirv.ARM.GraphOutputs %arg0 : !spirv.arm.tensor<1x16x16x16xi8>
+  }
+} // end spirv.module
+
+} // end module
+
+// -----
+
 module {
 // expected-error@+1 {{'spirv.module' op missing SPIR-V target env attribute}}
 spirv.module Logical GLSL450 {}
diff --git a/mlir/test/Dialect/SPIRV/Transforms/vce-deduction.mlir b/mlir/test/Dialect/SPIRV/Transforms/vce-deduction.mlir
index 2b237665ffc4..51c884fa7d16 100644
--- a/mlir/test/Dialect/SPIRV/Transforms/vce-deduction.mlir
+++ b/mlir/test/Dialect/SPIRV/Transforms/vce-deduction.mlir
@@ -7,7 +7,7 @@
 // Test deducing minimal version.
 // spirv.IAdd is available from v1.0.
 
-// CHECK: requires #spirv.vce<v1.0, [Shader], []>
+// CHECK: requires #spirv.vce<v1.0, [Shader, Matrix], []>
 spirv.module Logical GLSL450 attributes {
   spirv.target_env = #spirv.target_env<
     #spirv.vce<v1.5, [Shader], []>, #spirv.resource_limits<>>
@@ -21,7 +21,7 @@ spirv.module Logical GLSL450 attributes {
 // Test deducing minimal version.
 // spirv.GroupNonUniformBallot is available since v1.3.
 
-// CHECK: requires #spirv.vce<v1.3, [GroupNonUniformBallot, Shader], []>
+// CHECK: requires #spirv.vce<v1.3, [GroupNonUniformBallot, GroupNonUniform, Shader, Matrix], []>
 spirv.module Logical GLSL450 attributes {
   spirv.target_env = #spirv.target_env<
     #spirv.vce<v1.5, [Shader, GroupNonUniformBallot], []>, #spirv.resource_limits<>>
@@ -32,7 +32,7 @@ spirv.module Logical GLSL450 attributes {
   }
 }
 
-// CHECK: requires #spirv.vce<v1.4, [Shader], []>
+// CHECK: requires #spirv.vce<v1.4, [Shader, Matrix], []>
 spirv.module Logical GLSL450 attributes {
   spirv.target_env = #spirv.target_env<#spirv.vce<v1.6, [Shader], []>, #spirv.resource_limits<>>
 } {
@@ -48,7 +48,7 @@ spirv.module Logical GLSL450 attributes {
 
 // Test minimal capabilities.
 
-// CHECK: requires #spirv.vce<v1.0, [Shader], []>
+// CHECK: requires #spirv.vce<v1.0, [Shader, Matrix], []>
 spirv.module Logical GLSL450 attributes {
   spirv.target_env = #spirv.target_env<
     #spirv.vce<v1.0, [Shader, Float16, Float64, Int16, Int64, VariablePointers], []>, #spirv.resource_limits<>>
@@ -61,7 +61,7 @@ spirv.module Logical GLSL450 attributes {
 
 // Test Physical Storage Buffers are deduced correctly.
 
-// CHECK: spirv.module PhysicalStorageBuffer64 GLSL450 requires #spirv.vce<v1.0, [PhysicalStorageBufferAddresses, Shader], [SPV_EXT_physical_storage_buffer]>
+// CHECK: spirv.module PhysicalStorageBuffer64 GLSL450 requires #spirv.vce<v1.0, [PhysicalStorageBufferAddresses, Shader, Matrix], [SPV_EXT_physical_storage_buffer]>
 spirv.module PhysicalStorageBuffer64 GLSL450 attributes {
   spirv.target_env = #spirv.target_env<
     #spirv.vce<v1.0, [Shader, PhysicalStorageBufferAddresses], [SPV_EXT_physical_storage_buffer]>, #spirv.resource_limits<>>
@@ -74,7 +74,7 @@ spirv.module PhysicalStorageBuffer64 GLSL450 attributes {
 // Test deducing implied capability.
 // AtomicStorage implies Shader.
 
-// CHECK: requires #spirv.vce<v1.0, [Shader], []>
+// CHECK: requires #spirv.vce<v1.0, [Shader, Matrix], []>
 spirv.module Logical GLSL450 attributes {
   spirv.target_env = #spirv.target_env<
     #spirv.vce<v1.0, [AtomicStorage], []>, #spirv.resource_limits<>>
@@ -95,7 +95,7 @@ spirv.module Logical GLSL450 attributes {
 // * GroupNonUniformArithmetic
 // * GroupNonUniformBallot
 
-// CHECK: requires #spirv.vce<v1.3, [GroupNonUniformArithmetic, Shader], []>
+// CHECK: requires #spirv.vce<v1.3, [GroupNonUniformArithmetic, GroupNonUniform, Shader, Matrix], []>
 spirv.module Logical GLSL450 attributes {
   spirv.target_env = #spirv.target_env<
     #spirv.vce<v1.3, [Shader, GroupNonUniformArithmetic], []>, #spirv.resource_limits<>>
@@ -106,7 +106,7 @@ spirv.module Logical GLSL450 attributes {
   }
 }
 
-// CHECK: requires #spirv.vce<v1.3, [GroupNonUniformClustered, GroupNonUniformBallot, Shader], []>
+// CHECK: requires #spirv.vce<v1.3, [GroupNonUniformClustered, GroupNonUniformBallot, GroupNonUniform, Shader, Matrix], []>
 spirv.module Logical GLSL450 attributes {
   spirv.target_env = #spirv.target_env<
     #spirv.vce<v1.3, [Shader, GroupNonUniformClustered, GroupNonUniformBallot], []>, #spirv.resource_limits<>>
@@ -120,7 +120,7 @@ spirv.module Logical GLSL450 attributes {
 // Test type required capabilities
 
 // Using 8-bit integers in non-interface storage class requires Int8.
-// CHECK: requires #spirv.vce<v1.0, [Int8, Shader], []>
+// CHECK: requires #spirv.vce<v1.0, [Int8, Shader, Matrix], []>
 spirv.module Logical GLSL450 attributes {
   spirv.target_env = #spirv.target_env<
     #spirv.vce<v1.3, [Shader, Int8], []>, #spirv.resource_limits<>>
@@ -132,7 +132,7 @@ spirv.module Logical GLSL450 attributes {
 }
 
 // Using 16-bit floats in non-interface storage class requires Float16.
-// CHECK: requires #spirv.vce<v1.0, [Float16, Shader], []>
+// CHECK: requires #spirv.vce<v1.0, [Float16, Shader, Matrix], []>
 spirv.module Logical GLSL450 attributes {
   spirv.target_env = #spirv.target_env<
     #spirv.vce<v1.3, [Shader, Float16], []>, #spirv.resource_limits<>>
@@ -144,7 +144,7 @@ spirv.module Logical GLSL450 attributes {
 }
 
 // Using 16-element vectors requires Vector16.
-// CHECK: requires #spirv.vce<v1.0, [Vector16, Shader], []>
+// CHECK: requires #spirv.vce<v1.0, [Vector16, Kernel, Shader, Matrix], []>
 spirv.module Logical GLSL450 attributes {
   spirv.target_env = #spirv.target_env<
     #spirv.vce<v1.3, [Shader, Vector16], []>, #spirv.resource_limits<>>
@@ -162,7 +162,7 @@ spirv.module Logical GLSL450 attributes {
 // Test deducing minimal extensions.
 // spirv.KHR.SubgroupBallot requires the SPV_KHR_shader_ballot extension.
 
-// CHECK: requires #spirv.vce<v1.0, [SubgroupBallotKHR, Shader], [SPV_KHR_shader_ballot]>
+// CHECK: requires #spirv.vce<v1.0, [SubgroupBallotKHR, Shader, Matrix], [SPV_KHR_shader_ballot]>
 spirv.module Logical GLSL450 attributes {
   spirv.target_env = #spirv.target_env<
     #spirv.vce<v1.0, [Shader, SubgroupBallotKHR],
@@ -178,7 +178,7 @@ spirv.module Logical GLSL450 attributes {
 // Vulkan memory model requires SPV_KHR_vulkan_memory_model, which is enabled
 // implicitly by v1.5.
 
-// CHECK: requires #spirv.vce<v1.0, [VulkanMemoryModel], [SPV_KHR_vulkan_memory_model]>
+// CHECK: requires #spirv.vce<v1.5, [VulkanMemoryModel], [SPV_KHR_vulkan_memory_model]>
 spirv.module Logical Vulkan attributes {
   spirv.target_env = #spirv.target_env<
     #spirv.vce<v1.5, [Shader, VulkanMemoryModel], []>, #spirv.resource_limits<>>
@@ -193,7 +193,7 @@ spirv.module Logical Vulkan attributes {
 
 // Using 8-bit integers in interface storage class requires additional
 // extensions and capabilities.
-// CHECK: requires #spirv.vce<v1.0, [StorageBuffer16BitAccess, Shader, Int16], [SPV_KHR_16bit_storage, SPV_KHR_storage_buffer_storage_class]>
+// CHECK: requires #spirv.vce<v1.0, [StorageBuffer16BitAccess, Shader, Int16, Matrix], [SPV_KHR_16bit_storage, SPV_KHR_storage_buffer_storage_class]>
 spirv.module Logical GLSL450 attributes {
   spirv.target_env = #spirv.target_env<
     #spirv.vce<v1.3, [Shader, StorageBuffer16BitAccess, Int16], []>, #spirv.resource_limits<>>
@@ -208,7 +208,7 @@ spirv.module Logical GLSL450 attributes {
 // Complicated nested types
 // * Buffer requires ImageBuffer or SampledBuffer.
 // * Rg32f requires StorageImageExtendedFormats.
-// CHECK: requires #spirv.vce<v1.0, [UniformAndStorageBuffer8BitAccess, StorageUniform16, Int64, Shader, ImageBuffer, StorageImageExtendedFormats], [SPV_KHR_8bit_storage, SPV_KHR_16bit_storage]>
+// CHECK: requires #spirv.vce<v1.0, [UniformAndStorageBuffer8BitAccess, StorageUniform16, Int64, Shader, StorageBuffer8BitAccess, StorageBuffer16BitAccess, Matrix, ImageBuffer, StorageImageExtendedFormats, SampledBuffer], [SPV_KHR_8bit_storage, SPV_KHR_16bit_storage]>
 spirv.module Logical GLSL450 attributes {
   spirv.target_env = #spirv.target_env<
     #spirv.vce<v1.5, [Shader, UniformAndStorageBuffer8BitAccess, StorageBuffer16BitAccess, StorageUniform16, Int16, Int64, ImageBuffer, StorageImageExtendedFormats], []>,
@@ -219,7 +219,7 @@ spirv.module Logical GLSL450 attributes {
 }
 
 // Using bfloat16 requires BFloat16TypeKHR capability and SPV_KHR_bfloat16 extension.
-// CHECK: requires #spirv.vce<v1.0, [StorageBuffer16BitAccess, Shader, BFloat16TypeKHR], [SPV_KHR_bfloat16, SPV_KHR_16bit_storage, SPV_KHR_storage_buffer_storage_class]>
+// CHECK: requires #spirv.vce<v1.0, [StorageBuffer16BitAccess, Shader, BFloat16TypeKHR, Matrix], [SPV_KHR_bfloat16, SPV_KHR_16bit_storage, SPV_KHR_storage_buffer_storage_class]>
 spirv.module Logical GLSL450 attributes {
   spirv.target_env = #spirv.target_env<
     #spirv.vce<v1.0, [Shader, StorageBuffer16BitAccess, BFloat16TypeKHR], [SPV_KHR_bfloat16, SPV_KHR_16bit_storage, SPV_KHR_storage_buffer_storage_class]>,
@@ -231,3 +231,14 @@ spirv.module Logical GLSL450 attributes {
     spirv.ReturnValue %val : bf16
   }
 }
+
+// CHECK: spirv.module Logical Vulkan requires #spirv.vce<v1.5, [GraphARM, TensorsARM, Int8, Shader, VulkanMemoryModel, Float16, Matrix], [SPV_ARM_graph, SPV_ARM_tensors, SPV_KHR_vulkan_memory_model]>
+spirv.module Logical Vulkan attributes {
+  spirv.target_env = #spirv.target_env<
+    #spirv.vce<v1.5, [GraphARM, TensorsARM, Float16], [SPV_ARM_tensors, SPV_ARM_graph, SPV_KHR_vulkan_memory_model]>,
+    #spirv.resource_limits<>>
+} {
+  spirv.ARM.Graph @argmax(%arg0 : !spirv.arm.tensor<14x19xi8>, %arg1 : !spirv.arm.tensor<1xf16>) -> !spirv.arm.tensor<14x19xi8> {
+      spirv.ARM.GraphOutputs %arg0 : !spirv.arm.tensor<14x19xi8>
+  }
+}
diff --git a/mlir/test/Target/SPIRV/constant.mlir b/mlir/test/Target/SPIRV/constant.mlir
index 76d34c2a96e6..6790cbd47db1 100644
--- a/mlir/test/Target/SPIRV/constant.mlir
+++ b/mlir/test/Target/SPIRV/constant.mlir
@@ -1,6 +1,7 @@
 // RUN: mlir-translate --no-implicit-module --split-input-file --test-spirv-roundtrip %s | FileCheck %s
+// RUN: %if spirv-tools %{ mlir-translate -no-implicit-module -serialize-spirv %s | spirv-val %}
 
-spirv.module Logical GLSL450 requires #spirv.vce<v1.0, [Shader], []> {
+spirv.module Logical Vulkan requires #spirv.vce<v1.3, [VulkanMemoryModel, Shader, Int64, Int16, Int8, Float64, Float16, CooperativeMatrixKHR], [SPV_KHR_vulkan_memory_model, SPV_KHR_cooperative_matrix]> {
   // CHECK-LABEL: @bool_const
   spirv.func @bool_const() -> () "None" {
     // CHECK: spirv.Constant true
@@ -305,6 +306,50 @@ spirv.module Logical GLSL450 requires #spirv.vce<v1.0, [Shader], []> {
     %coop = spirv.Constant dense<4> : !spirv.coopmatrix<16x16xi8, Subgroup, MatrixAcc>
     spirv.ReturnValue %coop : !spirv.coopmatrix<16x16xi8, Subgroup, MatrixAcc>
   }
+
+  // CHECK-LABEL: @arm_tensor_of_i32
+  spirv.func @arm_tensor_of_i32() -> (!spirv.arm.tensor<2x3xi32>) "None" {
+    // CHECK: {{%.*}} = spirv.Constant dense<{{\[}}[1, 2, 3], [4, 5, 6]]> : !spirv.arm.tensor<2x3xi32>
+    %0 = spirv.Constant dense<[[1, 2, 3], [4, 5, 6]]> : !spirv.arm.tensor<2x3xi32>
+    spirv.ReturnValue %0 : !spirv.arm.tensor<2x3xi32>
+  }
+
+  // CHECK-LABEL: @splat_arm_tensor_of_i32
+  spirv.func @splat_arm_tensor_of_i32() -> (!spirv.arm.tensor<2x3xi32>) "None" {
+    // CHECK: {{%.*}} = spirv.Constant dense<2> : !spirv.arm.tensor<2x3xi32>
+    %0 = spirv.Constant dense<2> : !spirv.arm.tensor<2x3xi32>
+    spirv.ReturnValue %0 : !spirv.arm.tensor<2x3xi32>
+  }
+
+  // CHECK-LABEL: @arm_tensor_of_f32
+  spirv.func @arm_tensor_of_f32() -> (!spirv.arm.tensor<2x3xf32>) "None" {
+    // CHECK: {{%.*}} = spirv.Constant dense<{{\[}}[1.000000e+00, 2.000000e+00, 3.000000e+00], [4.000000e+00, 5.000000e+00, 6.000000e+00]]> : !spirv.arm.tensor<2x3xf32>
+    %0 = spirv.Constant dense<[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]>: !spirv.arm.tensor<2x3xf32>
+    spirv.ReturnValue %0 : !spirv.arm.tensor<2x3xf32>
+  }
+
+  // CHECK-LABEL: @splat_arm_tensor_of_f32
+  spirv.func @splat_arm_tensor_of_f32() -> (!spirv.arm.tensor<2x3xf32>) "None" {
+    // CHECK: {{%.*}} = spirv.Constant dense<2.000000e+00> : !spirv.arm.tensor<2x3xf32>
+    %0 = spirv.Constant dense<2.0> : !spirv.arm.tensor<2x3xf32>
+    spirv.ReturnValue %0 : !spirv.arm.tensor<2x3xf32>
+  }
+
+  // CHECK-LABEL: @null_arm_tensor_of_i32
+  spirv.func @null_arm_tensor_of_i32() -> (!spirv.arm.tensor<2x3xi32>) "None" {
+    // CHECK: spirv.Constant dense<0> : !spirv.arm.tensor<2x3xi32>
+    %0 = spirv.Constant dense<0> : !spirv.arm.tensor<2x3xi32>
+    spirv.ReturnValue %0 : !spirv.arm.tensor<2x3xi32>
+  }
+
+  // CHECK-LABEL: @null_arm_tensor_of_f32
+  spirv.func @null_arm_tensor_of_f32() -> (!spirv.arm.tensor<2x3xf32>) "None" {
+    // CHECK: spirv.Constant dense<0.000000e+00> : !spirv.arm.tensor<2x3xf32>
+    %0 = spirv.Constant dense<0.0> : !spirv.arm.tensor<2x3xf32>
+    spirv.ReturnValue %0 : !spirv.arm.tensor<2x3xf32>
+  }
+
+  spirv.EntryPoint "GLCompute" @bool_const
 }
 
 // -----
@@ -360,6 +405,20 @@ spirv.module Logical GLSL450 requires #spirv.vce<v1.0, [Shader, ReplicatedCompos
     spirv.ReturnValue %0 : !spirv.arm.tensor<2x3xi32>
   }
 
+  // CHECK-LABEL: @array_of_splat_array_of_non_splat_arrays_of_i32
+  spirv.func @array_of_splat_array_of_non_splat_arrays_of_i32() -> !spirv.array<2 x !spirv.array<2 x !spirv.array<3 x i32>>> "None" {
+    // CHECK: spirv.EXT.ConstantCompositeReplicate {{\[}}{{\[}}[1 : i32, 2 : i32, 3 : i32], [4 : i32, 5 : i32, 6 : i32]]] : !spirv.array<2 x !spirv.array<2 x !spirv.array<3 x i32>>>
+    %0 = spirv.EXT.ConstantCompositeReplicate [[[1 : i32, 2 : i32, 3 : i32], [4 : i32, 5 : i32, 6 : i32]]] : !spirv.array<2 x !spirv.array<2 x !spirv.array<3 x i32>>>
+    spirv.ReturnValue %0 : !spirv.array<2 x !spirv.array<2 x !spirv.array<3 x i32>>>
+  }
+
+  // CHECK-LABEL: @null_cc_arm_tensor_of_i32
+  spirv.func @null_cc_arm_tensor_of_i32() -> (!spirv.arm.tensor<2x3xi32>) "None" {
+    // CHECK: spirv.Constant dense<0> : !spirv.arm.tensor<2x3xi32>
+    %0 = spirv.EXT.ConstantCompositeReplicate [0 : i32] : !spirv.arm.tensor<2x3xi32>
+    spirv.ReturnValue %0 : !spirv.arm.tensor<2x3xi32>
+  }
+
   // CHECK-LABEL: @splat_vector_f32
   spirv.func @splat_vector_f32() -> (vector<3xf32>) "None" {
     // CHECK: spirv.EXT.ConstantCompositeReplicate [1.000000e+00 : f32] : vector<3xf32>
@@ -408,4 +467,18 @@ spirv.module Logical GLSL450 requires #spirv.vce<v1.0, [Shader, ReplicatedCompos
     %0 = spirv.EXT.ConstantCompositeReplicate [2.0 : f32] : !spirv.arm.tensor<2x3xf32>
     spirv.ReturnValue %0 : !spirv.arm.tensor<2x3xf32>
   }
+
+  // CHECK-LABEL: @array_of_splat_array_of_non_splat_arrays_of_f32
+  spirv.func @array_of_splat_array_of_non_splat_arrays_of_f32() -> !spirv.array<2 x !spirv.array<2 x !spirv.array<3 x f32>>> "None" {
+    // CHECK: spirv.EXT.ConstantCompositeReplicate {{\[}}{{\[}}[1.000000e+00 : f32, 2.000000e+00 : f32, 3.000000e+00 : f32], [4.000000e+00 : f32, 5.000000e+00 : f32, 6.000000e+00 : f32]]] : !spirv.array<2 x !spirv.array<2 x !spirv.array<3 x f32>>>
+    %0 = spirv.EXT.ConstantCompositeReplicate [[[1.0 : f32, 2.0 : f32, 3.0 : f32], [4.0 : f32, 5.0 : f32, 6.0 : f32]]] : !spirv.array<2 x !spirv.array<2 x !spirv.array<3 x f32>>>
+    spirv.ReturnValue %0 : !spirv.array<2 x !spirv.array<2 x !spirv.array<3 x f32>>>
+  }
+
+  // CHECK-LABEL: @null_cc_arm_tensor_of_f32
+  spirv.func @null_cc_arm_tensor_of_f32() -> (!spirv.arm.tensor<2x3xf32>) "None" {
+    // CHECK: spirv.Constant dense<0.000000e+00> : !spirv.arm.tensor<2x3xf32>
+    %0 = spirv.EXT.ConstantCompositeReplicate [0.0 : f32] : !spirv.arm.tensor<2x3xf32>
+    spirv.ReturnValue %0 : !spirv.arm.tensor<2x3xf32>
+  }
 }
diff --git a/mlir/test/Target/SPIRV/lit.local.cfg b/mlir/test/Target/SPIRV/lit.local.cfg
new file mode 100644
index 000000000000..6d44394c8cd4
--- /dev/null
+++ b/mlir/test/Target/SPIRV/lit.local.cfg
@@ -0,0 +1,4 @@
+if config.spirv_tools_tests:
+    config.available_features.add("spirv-tools")
+    config.substitutions.append(("spirv-as", os.path.join(config.llvm_tools_dir, "spirv-as")))
+    config.substitutions.append(("spirv-val", os.path.join(config.llvm_tools_dir, "spirv-val")))
diff --git a/mlir/test/lib/Dialect/SPIRV/TestAvailability.cpp b/mlir/test/lib/Dialect/SPIRV/TestAvailability.cpp
index 2e5e591fe5f9..9efca825a663 100644
--- a/mlir/test/lib/Dialect/SPIRV/TestAvailability.cpp
+++ b/mlir/test/lib/Dialect/SPIRV/TestAvailability.cpp
@@ -21,7 +21,7 @@ using namespace mlir;
 namespace {
 /// A pass for testing SPIR-V op availability.
 struct PrintOpAvailability
-    : public PassWrapper<PrintOpAvailability, OperationPass<func::FuncOp>> {
+    : public PassWrapper<PrintOpAvailability, OperationPass<mlir::ModuleOp>> {
   MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(PrintOpAvailability)
 
   void runOnOperation() override;
@@ -33,12 +33,10 @@ struct PrintOpAvailability
 } // namespace
 
 void PrintOpAvailability::runOnOperation() {
-  auto f = getOperation();
-  llvm::outs() << f.getName() << "\n";
-
+  auto moduleOp = getOperation();
   Dialect *spirvDialect = getContext().getLoadedDialect("spirv");
 
-  f->walk([&](Operation *op) {
+  auto opCallback = [&](Operation *op) {
     if (op->getDialect() != spirvDialect)
       return WalkResult::advance();
 
@@ -89,6 +87,16 @@ void PrintOpAvailability::runOnOperation() {
     os.flush();
 
     return WalkResult::advance();
+  };
+
+  moduleOp.walk([&](func::FuncOp f) {
+    llvm::outs() << f.getName() << "\n";
+    f->walk(opCallback);
+  });
+
+  moduleOp.walk([&](spirv::GraphARMOp g) {
+    llvm::outs() << g.getName() << "\n";
+    g->walk(opCallback);
   });
 }
 
diff --git a/mlir/test/lit.cfg.py b/mlir/test/lit.cfg.py
index 233fef8ec429..feaf5fb852a1 100644
--- a/mlir/test/lit.cfg.py
+++ b/mlir/test/lit.cfg.py
@@ -343,7 +343,6 @@ if config.enable_assertions:
 else:
     config.available_features.add("noasserts")
 
-
 def have_host_jit_feature_support(feature_name):
     mlir_runner_exe = lit.util.which("mlir-runner", config.mlir_tools_dir)
 
diff --git a/mlir/test/lit.site.cfg.py.in b/mlir/test/lit.site.cfg.py.in
index 132aabe13594..b1185e19d86e 100644
--- a/mlir/test/lit.site.cfg.py.in
+++ b/mlir/test/lit.site.cfg.py.in
@@ -5,6 +5,7 @@ import sys
 config.target_triple = "@LLVM_TARGET_TRIPLE@"
 config.llvm_src_root = "@LLVM_SOURCE_DIR@"
 config.llvm_tools_dir = lit_config.substitute("@LLVM_TOOLS_DIR@")
+config.spirv_tools_tests = @LLVM_INCLUDE_SPIRV_TOOLS_TESTS@
 config.llvm_shlib_ext = "@SHLIBEXT@"
 config.llvm_shlib_dir = lit_config.substitute(path(r"@SHLIBDIR@"))
 config.python_executable = "@Python3_EXECUTABLE@"
@@ -41,7 +42,7 @@ config.mlir_run_amx_tests = @MLIR_RUN_AMX_TESTS@
 config.mlir_run_arm_sve_tests = @MLIR_RUN_ARM_SVE_TESTS@
 # This is a workaround for the fact that LIT's:
 #   %if <cond>
-# requires <cond> to be in the set of available features. 
+# requires <cond> to be in the set of available features.
 # TODO: Update LIT's TestRunner so that this is not required.
 if config.mlir_run_arm_sve_tests:
     config.available_features.add("mlir_arm_sve_tests")
diff --git a/utils/bazel/llvm-project-overlay/mlir/test/BUILD.bazel b/utils/bazel/llvm-project-overlay/mlir/test/BUILD.bazel
index 95e3ee4df7bc..54835b741800 100644
--- a/utils/bazel/llvm-project-overlay/mlir/test/BUILD.bazel
+++ b/utils/bazel/llvm-project-overlay/mlir/test/BUILD.bazel
@@ -37,6 +37,7 @@ expand_template(
         # All disabled, but required to substituted because they are not in quotes.
         "@LLVM_BUILD_EXAMPLES@": "0",
         "@LLVM_HAS_NVPTX_TARGET@": "0",
+        "@LLVM_INCLUDE_SPIRV_TOOLS_TESTS@": "0",
         "@MLIR_ENABLE_CUDA_RUNNER@": "0",
         "@MLIR_ENABLE_ROCM_CONVERSIONS@": "0",
         "@MLIR_ENABLE_ROCM_RUNNER@": "0",
